##교육
https://labs.msaez.io/#/courses/cna-full/be513ff0-e2d5-11ec-b71d-f90e603c5333

## 가입
https://hub.docker.com/

## 도커 싸이트 가입
hub.docker.com -> gorapadd@naver.com / keb213185###

## 도커로그인 (command로)
docker login -> gorapadd / keb213185###   <= 이메일아님.

root@labs--311032102:~# docker login
Login with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one.
Username: gorapadd
Password: 
Login Succeeded
root@labs--311032102:~# cd .docker
root@labs--311032102:~/.docker# ls
config.json
root@labs--311032102:~/.docker# cat *
{
        "auths": {
                "https://index.docker.io/v1/": {
                        "auth": "Z29yYXBhZGQ6a2ViMjEzMTg1IQ=="
                }
        }
}root@labs--311032102:~/.docker# echo "Z29yYXBhZGQ6a2ViMjEzMTg1IQ==" | base64 -d
gorapadd:keb213185!root@labs--31docker push  gorapadd/userapp:1^C
root@labs--311032102:~/.docker# cd home
bash: cd: home: No such file or directory
root@labs--311032102:~/.docker# ls
config.json
root@labs--311032102:~/.docker# cd /home


==> 조회 : nginx  
1. Tags :  일종의 버전관리

==> 조회 : mysql


1. 이미지관리 : docker 싸이트
root@labs--311032102:/home/project/monolith2misvc# docker pull nginx
=> 이미지 생성됨

2. 실행되는 순간
=> docker run 하는순간 우리가 관리함.
docker run --name my-nginx -d -p nginx

docker ps -a

docker rm d1e88aaa32ae

un 'docker COMMAND --help' for more information on a command.
root@labs--311032102:/home/project/monolith2misvc# docker image ls
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
gorapadd/order      2022061401          83818081755e        5 days ago          165 MB
jinyoung/order      2022061401          1d349b76aa1e        5 days ago          165 MB
openjdk             8u212-jdk-alpine    a3562aa0b991        3 years ago         105 MB
root@labs--311032102:/home/project/monolith2misvc# docker pull nginx
Using default tag: latest
latest: Pulling from library/nginx
42c077c10790: Pull complete 
62c70f376f6a: Pull complete 
915cc9bd79c2: Pull complete 
75a963e94de0: Pull complete 
7b1fab684d70: Pull complete 
db24d06d5af4: Pull complete 
Digest: sha256:2bcabc23b45489fb0885d69a06ba1d648aeda973fae7bb981bafbb884165e514
Status: Downloaded newer image for nginx:latest
root@labs--311032102:/home/project/monolith2misvc# docker image ls
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
gorapadd/order      2022061401          83818081755e        5 days ago          165 MB
jinyoung/order      2022061401          1d349b76aa1e        5 days ago          165 MB
nginx               latest              0e901e68141f        3 weeks ago         142 MB
openjdk             8u212-jdk-alpine    a3562aa0b991        3 years ago         105 MB
root@labs--311032102:/home/project/monolith2misvc# docker run --name my-nginx -d nginx
d1e88aaa32aef277055ee451d953c54d62ba5762605eafabcb7514018c0d189b
root@labs--311032102:/home/project/monolith2misvc# docker image ls
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
gorapadd/order      2022061401          83818081755e        5 days ago          165 MB
jinyoung/order      2022061401          1d349b76aa1e        5 days ago          165 MB
nginx               latest              0e901e68141f        3 weeks ago         142 MB
openjdk             8u212-jdk-alpine    a3562aa0b991        3 years ago         105 MB
root@labs--311032102:/home/project/monolith2misvc# docker ps -a
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
d1e88aaa32ae        nginx               "/docker-entrypoin..."   50 seconds ago      Up 49 seconds       80/tcp              my-nginx
root@labs--311032102:/home/project/monolith2misvc# docker rm docker ps -a
unknown shorthand flag: 'a' in -a
See 'docker rm --help'.
root@labs--311032102:/home/project/monolith2misvc# docker rm d1e88aaa32ae
Error response from daemon: You cannot remove a running container d1e88aaa32aef277055ee451d953c54d62ba5762605eafabcb7514018c0d189b. Stop the container before attempting removal or force remove
root@labs--311032102:/home/project/monolith2misvc# docker ps -a
CONTAINER ID        IMAGE               COMMAND                  CREATED              STATUS              PORTS               NAMES
d1e88aaa32ae        nginx               "/docker-entrypoin..."   About a minute ago   Up About a minute   80/tcp              my-nginx
root@labs--311032102:/home/project/monolith2misvc# docker rm d1e88aaa32ae
Error response from daemon: You cannot remove a running container d1e88aaa32aef277055ee451d953c54d62ba5762605eafabcb7514018c0d189b. Stop the container before attempting removal or force remove
root@labs--311032102:/home/project/monolith2misvc# docker ps -a
CONTAINER ID        IMAGE               COMMAND                  CREATED              STATUS              PORTS               NAMES
d1e88aaa32ae        nginx               "/docker-entrypoin..."   About a minute ago   Up About a minute   80/tcp              my-nginx
root@labs--311032102:/home/project/monolith2misvc# docker run --name my-nginx -d nginx
docker: Error response from daemon: Conflict. The container name "/my-nginx" is already in use by container "d1e88aaa32aef277055ee451d953c54d62ba5762605eafabcb7514018c0d189b". You have to remove (or rename) that container to be able to reuse that name..
See 'docker run --help'.
root@labs--311032102:/home/project/monolith2misvc# docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
ad02c8169ab4        bridge              bridge              local
d6a9fbb1b31f        host                host                local
a0bceb52ab69        none                null                local
root@labs--311032102:/home/project/monolith2misvc# ifconfig -a
root@labs--311032102:/home/project/monolith2misvc# docker inspect
"docker inspect" requires at least 1 argument(s).
See 'docker inspect --help'.

Usage:  docker inspect [OPTIONS] NAME|ID [NAME|ID...]

Return low-level information on Docker objects
root@labs--311032102:/home/project/monolith2misvc# docker inspect my-nginx
[
    {
        "Id": "d1e88aaa32aef277055ee451d953c54d62ba5762605eafabcb7514018c0d189b",
        "Created": "2022-06-20T01:14:38.711516537Z",
        "Path": "/docker-entrypoint.sh",
        "Args": [
            "nginx",
            "-g",
            "daemon off;"
        ],
        "State": {
            "Status": "running",
            "Running": true,
            "Paused": false,
            "Restarting": false,
            "OOMKilled": false,
            "Dead": false,
            "Pid": 1439,
            "ExitCode": 0,
            "Error": "",
            "StartedAt": "2022-06-20T01:14:39.151446144Z",
            "FinishedAt": "0001-01-01T00:00:00Z"
        },
        "Image": "sha256:0e901e68141fd02f237cf63eb842529f8a9500636a9419e3cf4fb986b8fe3d5d",
        "ResolvConfPath": "/var/lib/docker/containers/d1e88aaa32aef277055ee451d953c54d62ba5762605eafabcb7514018c0d189b/resolv.conf",
        "HostnamePath": "/var/lib/docker/containers/d1e88aaa32aef277055ee451d953c54d62ba5762605eafabcb7514018c0d189b/hostname",
        "HostsPath": "/var/lib/docker/containers/d1e88aaa32aef277055ee451d953c54d62ba5762605eafabcb7514018c0d189b/hosts",
        "LogPath": "/var/lib/docker/containers/d1e88aaa32aef277055ee451d953c54d62ba5762605eafabcb7514018c0d189b/d1e88aaa32aef277055ee451d953c54d62ba5762605eafabcb7514018c0d189b-json.log",
        "Name": "/my-nginx",
        "RestartCount": 0,
        "Driver": "overlay2",
        "Platform": "linux",
        "MountLabel": "",
        "ProcessLabel": "",
        "AppArmorProfile": "docker-default",
        "ExecIDs": null,
        "HostConfig": {
            "Binds": null,
            "ContainerIDFile": "",
            "LogConfig": {
                "Type": "json-file",
                "Config": {}
            },
            "NetworkMode": "default",
            "PortBindings": {},
            "RestartPolicy": {
                "Name": "no",
                "MaximumRetryCount": 0
            },
            "AutoRemove": false,
            "VolumeDriver": "",
            "VolumesFrom": null,
            "CapAdd": null,
            "CapDrop": null,
            "Dns": [],
            "DnsOptions": [],
            "DnsSearch": [],
            "ExtraHosts": null,
            "GroupAdd": null,
            "IpcMode": "shareable",
            "Cgroup": "",
            "Links": null,
            "OomScoreAdj": 0,
            "PidMode": "",
            "Privileged": false,
            "PublishAllPorts": false,
            "ReadonlyRootfs": false,
            "SecurityOpt": null,
            "UTSMode": "",
            "UsernsMode": "",
            "ShmSize": 67108864,
            "Runtime": "runc",
            "ConsoleSize": [
                0,
                0
            ],
            "Isolation": "",
            "CpuShares": 0,
            "Memory": 0,
            "NanoCpus": 0,
            "CgroupParent": "",
            "BlkioWeight": 0,
            "BlkioWeightDevice": null,
            "BlkioDeviceReadBps": null,
            "BlkioDeviceWriteBps": null,
            "BlkioDeviceReadIOps": null,
            "BlkioDeviceWriteIOps": null,
            "CpuPeriod": 0,
            "CpuQuota": 0,
            "CpuRealtimePeriod": 0,
            "CpuRealtimeRuntime": 0,
            "CpusetCpus": "",
            "CpusetMems": "",
            "Devices": [],
            "DeviceCgroupRules": null,
            "DiskQuota": 0,
            "KernelMemory": 0,
            "MemoryReservation": 0,
            "MemorySwap": 0,
            "MemorySwappiness": null,
            "OomKillDisable": false,
            "PidsLimit": 0,
            "Ulimits": null,
            "CpuCount": 0,
            "CpuPercent": 0,
            "IOMaximumIOps": 0,
            "IOMaximumBandwidth": 0,
            "MaskedPaths": [
                "/proc/acpi",
                "/proc/kcore",
                "/proc/keys",
                "/proc/latency_stats",
                "/proc/timer_list",
                "/proc/timer_stats",
                "/proc/sched_debug",
                "/proc/scsi",
                "/sys/firmware"
            ],
            "ReadonlyPaths": [
                "/proc/asound",
                "/proc/bus",
                "/proc/fs",
                "/proc/irq",
                "/proc/sys",
                "/proc/sysrq-trigger"
            ]
        },
        "GraphDriver": {
            "Data": {
                "LowerDir": "/var/lib/docker/overlay2/831c15cd999cca6cdb97a07ca33d313c815c3609518840f80f11d22e21367d05-init/diff:/var/lib/docker/overlay2/f7bb0a716335df9e4231cab56a2c14d28ad6580bf60d5f40e537af769d8f66e3/diff:/var/lib/docker/overlay2/e50dfd2d7a13abd6cc59f8913ac318fc0da9d57da0f0e82bfbd1f470e9f9102f/diff:/var/lib/docker/overlay2/b763a6047e01bc1668608cbf61eb0dd43d63d151ff770c1c37519c5f0b383991/diff:/var/lib/docker/overlay2/30af13ed418fa17dde4f876fca4a6e159e75d7ba1f1d3ff7e16f0e74e3f583b8/diff:/var/lib/docker/overlay2/ea70cae85822f88bc9fa88602a40b81b6f7c41a2a086c988723c9128ea6bcadf/diff:/var/lib/docker/overlay2/506d20b8d14718424d7dc8d573f1bb6d5a0e21552cf4f86fb6cb1b1fa6c1af60/diff",
                "MergedDir": "/var/lib/docker/overlay2/831c15cd999cca6cdb97a07ca33d313c815c3609518840f80f11d22e21367d05/merged",
                "UpperDir": "/var/lib/docker/overlay2/831c15cd999cca6cdb97a07ca33d313c815c3609518840f80f11d22e21367d05/diff",
                "WorkDir": "/var/lib/docker/overlay2/831c15cd999cca6cdb97a07ca33d313c815c3609518840f80f11d22e21367d05/work"
            },
            "Name": "overlay2"
        },
        "Mounts": [],
        "Config": {
            "Hostname": "d1e88aaa32ae",
            "Domainname": "",
            "User": "",
            "AttachStdin": false,
            "AttachStdout": false,
            "AttachStderr": false,
            "ExposedPorts": {
                "80/tcp": {}
            },
            "Tty": false,
            "OpenStdin": false,
            "StdinOnce": false,
            "Env": [
                "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin",
                "NGINX_VERSION=1.21.6",
                "NJS_VERSION=0.7.3",
                "PKG_RELEASE=1~bullseye"
            ],
            "Cmd": [
                "nginx",
                "-g",
                "daemon off;"
            ],
            "Image": "nginx",
            "Volumes": null,
            "WorkingDir": "",
            "Entrypoint": [
                "/docker-entrypoint.sh"
            ],
            "OnBuild": null,
            "Labels": {
                "maintainer": "NGINX Docker Maintainers <docker-maint@nginx.com>"
            },
            "StopSignal": "SIGQUIT"
        },
        "NetworkSettings": {
            "Bridge": "",
            "SandboxID": "9afefdfc1ed341e598f3abec5415b28057a5fa90818889d274353e6184eae704",
            "HairpinMode": false,
            "LinkLocalIPv6Address": "",
            "LinkLocalIPv6PrefixLen": 0,
            "Ports": {
                "80/tcp": null
            },
            "SandboxKey": "/var/run/docker/netns/9afefdfc1ed3",
            "SecondaryIPAddresses": null,
            "SecondaryIPv6Addresses": null,
            "EndpointID": "a24765e94922e393a6b3e90a5c6c0d5c5641953af9da3c162fe434a1112ba767",
            "Gateway": "172.17.0.1",
            "GlobalIPv6Address": "",
            "GlobalIPv6PrefixLen": 0,
            "IPAddress": "172.17.0.2",
            "IPPrefixLen": 16,
            "IPv6Gateway": "",
            "MacAddress": "02:42:ac:11:00:02",
            "Networks": {
                "bridge": {
                    "IPAMConfig": null,
                    "Links": null,
                    "Aliases": null,
                    "NetworkID": "ad02c8169ab437410a6660992c1e983a212bebe380f0ce1ba871a45a9c1b1140",
                    "EndpointID": "a24765e94922e393a6b3e90a5c6c0d5c5641953af9da3c162fe434a1112ba767",
                    "Gateway": "172.17.0.1",
                    "IPAddress": "172.17.0.2",
                    "IPPrefixLen": 16,
                    "IPv6Gateway": "",
                    "GlobalIPv6Address": "",
                    "GlobalIPv6PrefixLen": 0,
                    "MacAddress": "02:42:ac:11:00:02",
                    "DriverOpts": null
                }
            }
        }
    }
]

## 엔진X 동작여부 
root@labs--311032102:/home/project/monolith2misvc# curl 172.17.0.2
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>

## image 파일안에 접속함 으로 접속함
root@labs--311032102:/home/project/monolith2misvc# docker exec -it my-nginx bash
root@d1e88aaa32ae:/# ls
bin   dev                  docker-entrypoint.sh  home  lib64  mnt  proc  run   srv  tmp  var
boot  docker-entrypoint.d  etc                   lib   media  opt  root  sbin  sys  usr
root@d1e88aaa32ae:/

##
docker run --name my-nginx -d -p 8080:80 nginx

==> 8080 -> 80 으로 nginx 처리함.  (-d 백그라운드)

root@labs--311032102:/home/project/monolith2misvc# docker run --name my-new-nginx -d -p 8081:80 nginx
656bcd4b4d7d74ba51fa7f2af3c45cd64b5d1fed888d78e85842bde974d29d6e
root@labs--311032102:/home/project/monolith2misvc# 
root@labs--311032102:/home/project/monolith2misvc# 
root@labs--311032102:/home/project/monolith2misvc# docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
gorapadd/order      2022061401          83818081755e        5 days ago          165 MB
jinyoung/order      2022061401          1d349b76aa1e        5 days ago          165 MB
nginx               latest              0e901e68141f        3 weeks ago         142 MB
openjdk             8u212-jdk-alpine    a3562aa0b991        3 years ago         105 MB
root@labs--311032102:/home/project/monolith2misvc# docker image ls
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
gorapadd/order      2022061401          83818081755e        5 days ago          165 MB
jinyoung/order      2022061401          1d349b76aa1e        5 days ago          165 MB
nginx               latest              0e901e68141f        3 weeks ago         142 MB
openjdk             8u212-jdk-alpine    a3562aa0b991        3 years ago         105 MB
root@labs--311032102:/home/project/monolith2misvc# docker exec -it my-nginx bash^C
root@labs--311032102:/home/project/monolith2misvc# docker image ls
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
gorapadd/order      2022061401          83818081755e        5 days ago          165 MB
jinyoung/order      2022061401          1d349b76aa1e        5 days ago          165 MB
nginx               latest              0e901e68141f        3 weeks ago         142 MB
openjdk             8u212-jdk-alpine    a3562aa0b991        3 years ago         105 MB
root@labs--311032102:/home/project/monolith2misvc# docker container ls
CONTAINER ID        IMAGE               COMMAND                  CREATED              STATUS              PORTS                  NAMES
656bcd4b4d7d        nginx               "/docker-entrypoin..."   About a minute ago   Up 59 seconds       0.0.0.0:8081->80/tcp   my-new-nginx
d1e88aaa32ae        nginx               "/docker-entrypoin..."   20 minutes ago       Up 20 minutes       80/tcp                 my-nginx
root@labs--311032102:/home/project/monolith2misvc# curl :8081
curl: (3) Bad URL, colon is first character
root@labs--311032102:/home/project/monolith2misvc# http :8081
HTTP/1.1 200 OK
Accept-Ranges: bytes
Connection: keep-alive
Content-Length: 615
Content-Type: text/html
Date: Mon, 20 Jun 2022 01:35:48 GMT
ETag: "61f01158-267"
Last-Modified: Tue, 25 Jan 2022 15:03:52 GMT
Server: nginx/1.21.6

<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>

root@labs--311032102:/home/project/monolith2misvc# docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                  NAMES
656bcd4b4d7d        nginx               "/docker-entrypoin..."   2 minutes ago       Up 2 minutes        0.0.0.0:8081->80/tcp   my-new-nginx
d1e88aaa32ae        nginx               "/docker-entrypoin..."   21 minutes ago      Up 21 minutes       80/tcp                 my-nginx
root@labs--311032102:/home/project/monolith2misvc# 

==> 로컬인터넷에서 container 확인하기 (8081 포트로 접속시 --> 80에서 처리함.)
https://8081-hanagcp-labs--311032102.kuberez.io/ 

root@labs--311032102:/home/project/monolith2misvc# docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                  NAMES
656bcd4b4d7d        nginx               "/docker-entrypoin..."   2 minutes ago       Up 2 minutes        0.0.0.0:8081->80/tcp   my-new-nginx
d1e88aaa32ae        nginx               "/docker-entrypoin..."   21 minutes ago      Up 21 minutes       80/tcp                 my-nginx
root@labs--311032102:/home/project/monolith2misvc# 
root@labs--311032102:/home/project/monolith2misvc# 
root@labs--311032102:/home/project/monolith2misvc# docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                  NAMES
656bcd4b4d7d        nginx               "/docker-entrypoin..."   9 minutes ago       Up 9 minutes        0.0.0.0:8081->80/tcp   my-new-nginx
d1e88aaa32ae        nginx               "/docker-entrypoin..."   29 minutes ago      Up 29 minutes       80/tcp                 my-nginx
root@labs--311032102:/home/project/monolith2misvc# docker container ls
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                  NAMES
656bcd4b4d7d        nginx               "/docker-entrypoin..."   9 minutes ago       Up 9 minutes        0.0.0.0:8081->80/tcp   my-new-nginx
d1e88aaa32ae        nginx               "/docker-entrypoin..."   29 minutes ago      Up 29 minutes       80/tcp                 my-nginx
root@labs--311032102:/home/project/monolith2misvc# 
root@labs--311032102:/home/project/monolith2misvc# 
root@labs--311032102:/home/project/monolith2misvc# docker container stop my-nginx
my-nginx
root@labs--311032102:/home/project/monolith2misvc# docker container ls
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                  NAMES
656bcd4b4d7d        nginx               "/docker-entrypoin..."   9 minutes ago       Up 9 minutes        0.0.0.0:8081->80/tcp   my-new-nginx
root@labs--311032102:/home/project/monolith2misvc# docker container stop my-new-nginx
my-new-nginx
root@labs--311032102:/home/project/monolith2misvc# 
root@labs--311032102:/home/project/monolith2misvc# docker container ls
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
root@labs--311032102:/home/project/monolith2misvc# docker container rm my-nginx
my-nginx
root@labs--311032102:/home/project/monolith2misvc# docker container ls
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
root@labs--311032102:/home/project/monolith2misvc# docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
gorapadd/order      2022061401          83818081755e        5 days ago          165 MB
jinyoung/order      2022061401          1d349b76aa1e        5 days ago          165 MB
nginx               latest              0e901e68141f        3 weeks ago         142 MB
openjdk             8u212-jdk-alpine    a3562aa0b991        3 years ago         105 MB
root@labs--311032102:/home/project/monolith2misvc# docker container rm my-new-nginx
my-new-nginx
root@labs--311032102:/home/project/monolith2misvc# docker image rm nginx
Untagged: nginx:latest
Untagged: nginx@sha256:2bcabc23b45489fb0885d69a06ba1d648aeda973fae7bb981bafbb884165e514
Deleted: sha256:0e901e68141fd02f237cf63eb842529f8a9500636a9419e3cf4fb986b8fe3d5d
Deleted: sha256:1e877fb1acf761377390ab38bbad050a1d5296f1b4f51878c2695d4ecdb98c62
Deleted: sha256:834e54d50f731515065370d1c15f0ed47d2f7b6a7b0452646db80f14ace9b8de
Deleted: sha256:d28ca7ee17ff94497071d5c075b4099a4f2c950a3471fc49bdf9876227970b24
Deleted: sha256:096f97ba95539883af393732efac02acdd0e2ae587a5479d97065b64b4eded8c
Deleted: sha256:de7e3b2a7430261fde88313fbf784a63c2229ce369b9116053786845c39058d5
Deleted: sha256:ad6562704f3759fb50f0d3de5f80a38f65a85e709b77fd24491253990f30b6be
root@labs--311032102:/home/project/monolith2misvc# 
root@labs--311032102:/home/project/monolith2misvc# 
root@labs--311032102:/home/project/monolith2misvc# docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
gorapadd/order      2022061401          83818081755e        5 days ago          165 MB
jinyoung/order      2022061401          1d349b76aa1e        5 days ago          165 MB
openjdk             8u212-jdk-alpine    a3562aa0b991        3 years ago         105 MB
root@labs--311032102:/home/project/monolith2misvc# docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
gorapadd/order      2022061401          83818081755e        5 days ago          165 MB
jinyoung/order      2022061401          1d349b76aa1e        5 days ago          165 MB
openjdk             8u212-jdk-alpine    a3562aa0b991        3 years ago         105 MB
root@labs--311032102:/home/project/monolith2misvc# 
root@labs--311032102:/home/project/monolith2misvc# 
root@labs--311032102:/home/project/monolith2misvc# docker ps -a
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
root@labs--311032102:/home/project/monolith2misvc# 

## PING 설치 (우분투계역 apt-get 사용)
- linux install ping command
- apt update 
- PING 설치 : apt-get install iputils-ping -y

## 
docker stop $(docker ps -a -q) && docker rm $(docker ps -a -q)

## httpd 설치 

docker run -dit --name my-running-app -p 8080:80 httpd


##

root@labs--311032102:~# ls -al
total 84
drwx------ 1 root root 4096 Jun 14 05:34 .
drwxr-xr-x 1 root root 4096 Jun 12 15:20 ..
-rw------- 1 root root 2185 Jun 14 05:50 .bash_history
-rw-r--r-- 1 root root 3148 May 17 06:07 .bashrc
drwxr-xr-x 1 root root 4096 Jun 13 02:07 .cache
drwxr-xr-x 1 root root 4096 Jun 14 02:19 .config
drwx------ 2 root root 4096 Jun 14 05:33 .docker
-rw-r--r-- 1 root root  298 May 17 06:07 .fzf.bash
drwx------ 1 root root 4096 May  4 19:53 .gnupg
drwx------ 2 root root 4096 Jun 13 00:30 .httpie
drwxr-xr-x 4 root root 4096 Jun 14 05:19 .kube
drwx------ 4 root root 4096 May  4 19:51 .local
drwxr-xr-x 3 root root 4096 Jun 13 00:10 .m2
drwxr-xr-x 4 root root 4096 Jun 14 02:19 .npm
-rw-r--r-- 1 root root  148 Aug 17  2015 .profile
drwxr-x--- 2 root root 4096 Jun 13 00:15 .siege
drwxr-xr-x 6 root root 4096 Jun 13 00:06 .theia
-rw------- 1 root root 4441 Jun 14 05:34 .viminfo
-rw-r--r-- 1 root root  203 May 17 06:07 .wget-hsts
root@labs--311032102:~# cd .docker
root@labs--311032102:~/.docker# ls
config.json
root@labs--311032102:~/.docker# 
root@labs--311032102:~/.docker# 
root@labs--311032102:~/.docker# ls
config.json
root@labs--311032102:~/.docker# vi *
root@labs--311032102:~/.docker# 
root@labs--311032102:~/.docker# cat config.json
{
        "auths": {
                "https://index.docker.io/v1/": {
                        "auth": "Z29yYXBhZGQ6a2ViMjEzMTg1IQ=="    <== base64 풀면 됨..
                }
        }
}root@labs--311032102:~/.docker# 
root@labs--311032102:~/.docker# 


root@labs--311032102:~# . ~/.bashrc
root@labs--311032102:~# 
root@labs--311032102:~# 
root@labs--311032102:~# . ~/.bashr^C
root@labs--311032102:~# 
root@labs--311032102:~# source ~/.bashrc

====================================


root@labs--311032102:/home/project# 
root@labs--311032102:/home/project# 
root@labs--311032102:/home/project# ls -lrt
total 84
drwxr-xr-x 2 root root 4096 Jun  9 00:14 default-env
drwxr-xr-x 7 root root 4096 Jun  9 00:17 shopping
drwxr-xr-x 6 root root 4096 Jun 10 02:18 cna-start
drwxr-xr-x 3 root root 4096 Jun 10 04:26 gateway
drwxr-xr-x 3 root root 4096 Jun 10 05:06 oauth2
drwxr-xr-x 5 root root 4096 Jun 10 05:41 Oauth2withKeycloak
drwxr-xr-x 5 root root 4096 Jun 10 06:06 monolith2misvc
drwxr-xr-x 3 root root 4096 Jun 10 07:17 circuitbreaker
drwxr-xr-x 2 root root 4096 Jun 13 01:19 kafka-base
drwxr-xr-x 5 root root 4096 Jun 13 01:58 Keycloak-OAuth2-1
drwxr-xr-x 3 root root 4096 Jun 13 02:05 cna-pubsub
drwxr-xr-x 4 root root 4096 Jun 13 02:53 cna-pubsub2
drwxr-xr-x 8 root root 4096 Jun 13 05:34 Kafka-Scaling
drwxr-xr-x 7 root root 4096 Jun 13 05:34 Kafka-Retry-DLQ
drwxr-xr-x 7 root root 4096 Jun 13 06:51 Kafka-Manual-Commit
drwxr-xr-x 7 root root 4096 Jun 14 00:21 advanced-connect
drwxr-xr-x 6 root root 4096 Jun 14 01:08 dp-composite-svc
drwxr-xr-x 4 root root 4096 Jun 14 02:12 dp-graphql
drwxr-xr-x 3 root root 4096 Jun 14 02:27 dp-cqrs
drwxr-xr-x 4 root root 4096 Jun 14 04:24 contract-test
drwxr-xr-x 3 root root 4096 Jun 14 05:08 ops-deploy-my-app
root@labs--311032102:/home/project# cp /index.html .
cp: cannot stat '/index.html': No such file or directory
root@labs--311032102:/home/project# 
root@labs--311032102:/home/project# cd ^C
root@labs--311032102:/home/project# ls /root/index.html
/root/index.html
root@labs--311032102:/home/project# 
root@labs--311032102:/home/project# 
root@labs--311032102:/home/project# set -o vi
root@labs--311032102:/home/project# cp /root/index.html /home/project
root@labs--311032102:/home/project# 
root@labs--311032102:/home/project# 
root@labs--311032102:/home/project# cd /home/project*
root@labs--311032102:/home/project# ls
Kafka-Manual-Commit  Keycloak-OAuth2-1   circuitbreaker  cna-start      dp-composite-svc  gateway     monolith2misvc     shopping
Kafka-Retry-DLQ      Oauth2withKeycloak  cna-pubsub      contract-test  dp-cqrs           index.html  oauth2
Kafka-Scaling        advanced-connect    cna-pubsub2     default-env    dp-graphql        kafka-base  ops-deploy-my-app
root@labs--311032102:/home/project# ls -lrt
total 88
drwxr-xr-x 2 root root 4096 Jun  9 00:14 default-env
drwxr-xr-x 7 root root 4096 Jun  9 00:17 shopping
drwxr-xr-x 6 root root 4096 Jun 10 02:18 cna-start
drwxr-xr-x 3 root root 4096 Jun 10 04:26 gateway
drwxr-xr-x 3 root root 4096 Jun 10 05:06 oauth2
drwxr-xr-x 5 root root 4096 Jun 10 05:41 Oauth2withKeycloak
drwxr-xr-x 5 root root 4096 Jun 10 06:06 monolith2misvc
drwxr-xr-x 3 root root 4096 Jun 10 07:17 circuitbreaker
drwxr-xr-x 2 root root 4096 Jun 13 01:19 kafka-base
drwxr-xr-x 5 root root 4096 Jun 13 01:58 Keycloak-OAuth2-1
drwxr-xr-x 3 root root 4096 Jun 13 02:05 cna-pubsub
drwxr-xr-x 4 root root 4096 Jun 13 02:53 cna-pubsub2
drwxr-xr-x 8 root root 4096 Jun 13 05:34 Kafka-Scaling
drwxr-xr-x 7 root root 4096 Jun 13 05:34 Kafka-Retry-DLQ
drwxr-xr-x 7 root root 4096 Jun 13 06:51 Kafka-Manual-Commit
drwxr-xr-x 7 root root 4096 Jun 14 00:21 advanced-connect
drwxr-xr-x 6 root root 4096 Jun 14 01:08 dp-composite-svc
drwxr-xr-x 4 root root 4096 Jun 14 02:12 dp-graphql
drwxr-xr-x 3 root root 4096 Jun 14 02:27 dp-cqrs
drwxr-xr-x 4 root root 4096 Jun 14 04:24 contract-test
drwxr-xr-x 3 root root 4096 Jun 14 05:08 ops-deploy-my-app
-rw-r--r-- 1 root root   46 Jun 20 02:21 index.html
root@labs--311032102:/home/project# ls
Kafka-Manual-Commit  Keycloak-OAuth2-1   circuitbreaker  cna-start      dp-composite-svc  gateway     monolith2misvc     shopping
Kafka-Retry-DLQ      Oauth2withKeycloak  cna-pubsub      contract-test  dp-cqrs           index.html  oauth2
Kafka-Scaling        advanced-connect    cna-pubsub2     default-env    dp-graphql        kafka-base  ops-deploy-my-app
root@labs--311032102:/home/project# vi Dockerfile
root@labs--311032102:/home/project# 
root@labs--311032102:/home/project# 
root@labs--311032102:/home/project# 
root@labs--311032102:/home/project# ls -lrt
total 92
drwxr-xr-x 2 root root 4096 Jun  9 00:14 default-env
drwxr-xr-x 7 root root 4096 Jun  9 00:17 shopping
drwxr-xr-x 6 root root 4096 Jun 10 02:18 cna-start
drwxr-xr-x 3 root root 4096 Jun 10 04:26 gateway
drwxr-xr-x 3 root root 4096 Jun 10 05:06 oauth2
drwxr-xr-x 5 root root 4096 Jun 10 05:41 Oauth2withKeycloak
drwxr-xr-x 5 root root 4096 Jun 10 06:06 monolith2misvc
drwxr-xr-x 3 root root 4096 Jun 10 07:17 circuitbreaker
drwxr-xr-x 2 root root 4096 Jun 13 01:19 kafka-base
drwxr-xr-x 5 root root 4096 Jun 13 01:58 Keycloak-OAuth2-1
drwxr-xr-x 3 root root 4096 Jun 13 02:05 cna-pubsub
drwxr-xr-x 4 root root 4096 Jun 13 02:53 cna-pubsub2
drwxr-xr-x 8 root root 4096 Jun 13 05:34 Kafka-Scaling
drwxr-xr-x 7 root root 4096 Jun 13 05:34 Kafka-Retry-DLQ
drwxr-xr-x 7 root root 4096 Jun 13 06:51 Kafka-Manual-Commit
drwxr-xr-x 7 root root 4096 Jun 14 00:21 advanced-connect
drwxr-xr-x 6 root root 4096 Jun 14 01:08 dp-composite-svc
drwxr-xr-x 4 root root 4096 Jun 14 02:12 dp-graphql
drwxr-xr-x 3 root root 4096 Jun 14 02:27 dp-cqrs
drwxr-xr-x 4 root root 4096 Jun 14 04:24 contract-test
drwxr-xr-x 3 root root 4096 Jun 14 05:08 ops-deploy-my-app
-rw-r--r-- 1 root root   46 Jun 20 02:21 index.html
-rw-r--r-- 1 root root   50 Jun 20 02:22 Dockerfile
root@labs--311032102:/home/project# vi Dockerfile
root@labs--311032102:/home/project# 
root@labs--311032102:/home/project# 
root@labs--311032102:/home/project# 
root@labs--311032102:/home/project# 
root@labs--311032102:/home/project# cat Dockerfile
FROM nginx
COPY index.html /usr/share/nginx/html/
root@labs--311032102:/home/project# 
root@labs--311032102:/home/project# ls -lrt
total 92
drwxr-xr-x 2 root root 4096 Jun  9 00:14 default-env
drwxr-xr-x 7 root root 4096 Jun  9 00:17 shopping
drwxr-xr-x 6 root root 4096 Jun 10 02:18 cna-start
drwxr-xr-x 3 root root 4096 Jun 10 04:26 gateway
drwxr-xr-x 3 root root 4096 Jun 10 05:06 oauth2
drwxr-xr-x 5 root root 4096 Jun 10 05:41 Oauth2withKeycloak
drwxr-xr-x 5 root root 4096 Jun 10 06:06 monolith2misvc
drwxr-xr-x 3 root root 4096 Jun 10 07:17 circuitbreaker
drwxr-xr-x 2 root root 4096 Jun 13 01:19 kafka-base
drwxr-xr-x 5 root root 4096 Jun 13 01:58 Keycloak-OAuth2-1
drwxr-xr-x 3 root root 4096 Jun 13 02:05 cna-pubsub
drwxr-xr-x 4 root root 4096 Jun 13 02:53 cna-pubsub2
drwxr-xr-x 8 root root 4096 Jun 13 05:34 Kafka-Scaling
drwxr-xr-x 7 root root 4096 Jun 13 05:34 Kafka-Retry-DLQ
drwxr-xr-x 7 root root 4096 Jun 13 06:51 Kafka-Manual-Commit
drwxr-xr-x 7 root root 4096 Jun 14 00:21 advanced-connect
drwxr-xr-x 6 root root 4096 Jun 14 01:08 dp-composite-svc
drwxr-xr-x 4 root root 4096 Jun 14 02:12 dp-graphql
drwxr-xr-x 3 root root 4096 Jun 14 02:27 dp-cqrs
drwxr-xr-x 4 root root 4096 Jun 14 04:24 contract-test
drwxr-xr-x 3 root root 4096 Jun 14 05:08 ops-deploy-my-app
-rw-r--r-- 1 root root   46 Jun 20 02:21 index.html
-rw-r--r-- 1 root root   50 Jun 20 02:22 Dockerfile
root@labs--311032102:/home/project# docker build -t gorapadd/welcome:v1 -f Dockerfile
"docker build" requires exactly 1 argument(s).
See 'docker build --help'.

Usage:  docker build [OPTIONS] PATH | URL | -

Build an image from a Dockerfile
root@labs--311032102:/home/project# docker build -t gorapadd/welcome:v1 -f .
"docker build" requires exactly 1 argument(s).
See 'docker build --help'.

Usage:  docker build [OPTIONS] PATH | URL | -

Build an image from a Dockerfile
root@labs--311032102:/home/project# docker build -t gorapadd/welcome:v1  .
Sending build context to Docker daemon 688.4 MB
Step 1/2 : FROM nginx
latest: Pulling from library/nginx
42c077c10790: Already exists 
62c70f376f6a: Pull complete 
915cc9bd79c2: Pull complete 
75a963e94de0: Pull complete 
7b1fab684d70: Pull complete 
db24d06d5af4: Pull complete 
Digest: sha256:2bcabc23b45489fb0885d69a06ba1d648aeda973fae7bb981bafbb884165e514
Status: Downloaded newer image for nginx:latest
 ---> 0e901e68141f
Step 2/2 : COPY index.html /usr/share/nginx/html/
 ---> a6f8064d8aeb
Successfully built a6f8064d8aeb
Successfully tagged gorapadd/welcome:v1
root@labs--311032102:/home/project# docker push -t gorapadd/welcome:v1  .
unknown shorthand flag: 't' in -t
See 'docker push --help'.
root@labs--311032102:/home/project# docker push  gorapadd/welcome:v1  .
"docker push" requires exactly 1 argument(s).
See 'docker push --help'.

Usage:  docker push [OPTIONS] NAME[:TAG]

Push an image or a repository to a registry
root@labs--311032102:/home/project# docker push  gorapadd/welcome:v1  
The push refers to repository [docker.io/gorapadd/welcome]
ba08f6ab43af: Pushed 
33e3df466e11: Mounted from library/nginx 
747b7a567071: Mounted from library/nginx 
57d3fc88cb3f: Mounted from library/nginx 
53ae81198b64: Mounted from library/nginx 
58354abe5f0e: Mounted from library/nginx 
ad6562704f37: Mounted from library/nginx 
v1: digest: sha256:2174131f6d63f155d3b9d6b0c8aab976f4ee56ed40e7e041b627b58ecba0ec61 size: 1777
root@labs--311032102:/home/project# docker run -p 8091:80 gorapadd/welcome:v1


root@labs--311032102:/home/project# netstat -lntp | grep :808 
tcp6       0      0 :::8080                 :::*                    LISTEN      -                   
root@labs--311032102:/home/project# docker run nginx -p 8090:80 gorapadd/welcome:v1
/docker-entrypoint.sh: 38: exec: -p: not found
root@labs--311032102:/home/project# docker run  -p 8090:80 gorapadd/welcome:v1
docker: Error response from daemon: driver failed programming external connectivity on endpoint nifty_raman (e7529d8788e60f57c3cdc1b2fb352507cf07e90fd7378fd332fa6f9dca7703fc): Error starting userland proxy: listen tcp 0.0.0.0:8090: bind: address already in use.
root@labs--311032102:/home/project# docker run -d  -p 8090:80 gorapadd/welcome:v1
e2cd835b1b2196506d2e176a0832b30fed3cbd77a3417e982770898cd228b718
docker: Error response from daemon: driver failed programming external connectivity on endpoint silly_pascal (197da9b43cba30f1e87f29116aad537584b145b095f293b46ba51b8caf3a77d7): Error starting userland proxy: listen tcp 0.0.0.0:8090: bind: address already in use.
root@labs--311032102:/home/project# docker run -d  -p 8091:80 gorapadd/welcome:v1
0d806e5b83f6017e03b533a54c7a1730f9dfff9025bf5c4e4b976887114c541d
root@labs--311032102:/home/project# docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
gorapadd/welcome    v1                  a6f8064d8aeb        6 minutes ago       142 MB
gorapadd/order      2022061401          83818081755e        5 days ago          165 MB
jinyoung/order      2022061401          1d349b76aa1e        5 days ago          165 MB
httpd               latest              b260a49eebf9        6 days ago          145 MB
nginx               latest              0e901e68141f        3 weeks ago         142 MB
openjdk             8u212-jdk-alpine    a3562aa0b991        3 years ago         105 MB
root@labs--311032102:/home/project# docker container ls
CONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS              PORTS                  NAMES
0d806e5b83f6        gorapadd/welcome:v1   "/docker-entrypoin..."   19 seconds ago      Up 18 seconds       0.0.0.0:8091->80/tcp   naughty_mahavira
8b55e1bad221        httpd                 "httpd-foreground"       42 minutes ago      Up 42 minutes       0.0.0.0:8080->80/tcp   my-running-app


root@labs--311032102:/home/project# echo "dd"| base64 -d 
ubase64: invalid input
root@labs--311032102:/home/project# which base64
/usr/bin/base64
root@labs--311032102:/home/project# echo "dklsajflds" | base64 -d
vIlj7�vbase64: invalid input
root@labs--311032102:/home/project# echo "dklsajflds" | base64
ZGtsc2FqZmxkcwo=
root@labs--311032102:/home/project# echo "ZGtsc2FqZmxkcwo=" | base64 -d
dklsajflds

root@labs--311032102:/home/project# http :8091
HTTP/1.1 200 OK
Accept-Ranges: bytes
Connection: keep-alive
Content-Length: 46
Content-Type: text/html
Date: Mon, 20 Jun 2022 02:38:06 GMT
ETag: "62afd9a4-2e"
Last-Modified: Mon, 20 Jun 2022 02:21:24 GMT
Server: nginx/1.21.6

<h1> Hi~ My name is Hong Gil-Dong...~~~ </h1>

## AWS

	접속 URL		https://uengine2.signin.aws.amazon.com/console			
No,	이름	ID	PW	액세스 키 ID	비밀 액세스 키	리전
1	서준성	user01	hana!@12	AKIA6H47RAWMWZN6CVHT	RgsIq9d/L4bd9bs2ot8V6I/uIkzPMijxAqd2Af4y	(도쿄)ap-northeast-1
2	정연욱	user02	hana!@12	AKIA6H47RAWMZAAJRW7C	GrwZSP53C+0ocKODo1hF6/w44JmKoZmMtKKU0v7I	(도쿄)ap-northeast-1

## uengine2 , user02 , hana!@12 ==> 로그인 

root@labs--311032102:/home/project# cc/cccc/cccc^C
root@labs--311032102:/home/project# aws configure
AWS Access Key ID [None]: user02
AWS Secret Access Key [None]: AKIA6H47RAWMZAAJRW7C
Default region name [None]: ap-northeast-1
Default output format [None]: json
root@labs--311032102:/home/project# eksctl create cluster --name user02-eks --version 1.19 --spot --managed --nodegroup-name standard-workers --node-type t3.medium --nodes 5 --nodes-min 1 --nodes-max 3
Error: cannot use --nodes-max=3 and --nodes=5 at the same time
root@labs--311032102:/home/project# eksctl create cluster --name user02-eks --version 1.19 --spot --managed --nodegroup-name standard-workers --node-type t3.medium --nodes 3 --nodes-min 1 --nodes-max 3
Error: checking AWS STS access – cannot get role ARN for current session: operation error STS: GetCallerIdentity, https response error StatusCode: 403, RequestID: 7b6db17f-5a1b-4c04-8f54-9beb12b40f0f, api error InvalidClientTokenId: The security token included in the request is invalid.
root@labs--311032102:/home/project# eksctl create cluster --name user02-eks --version 1.19 --spot --managed --nodegroup-name standard-workers --node-type t3.medium --nodes 3 --nodes-min 1 --nodes-max 3^C
root@labs--311032102:/home/project# ls
Dockerfile           Oauth2withKeycloak  cna-start         dp-graphql      oauth2
Kafka-Manual-Commit  advanced-connect    contract-test     gateway         ops-deploy-my-app
Kafka-Retry-DLQ      circuitbreaker      default-env       index.html      shopping
Kafka-Scaling        cna-pubsub          dp-composite-svc  kafka-base
Keycloak-OAuth2-1    cna-pubsub2         dp-cqrs           monolith2misvc
root@labs--311032102:/home/project# ks -lrt^C
root@labs--311032102:/home/project# ls -lrt
total 92
drwxr-xr-x 2 root root 4096 Jun  9 00:14 default-env
drwxr-xr-x 7 root root 4096 Jun  9 00:17 shopping
drwxr-xr-x 6 root root 4096 Jun 10 02:18 cna-start
drwxr-xr-x 3 root root 4096 Jun 10 04:26 gateway
drwxr-xr-x 3 root root 4096 Jun 10 05:06 oauth2
drwxr-xr-x 5 root root 4096 Jun 10 05:41 Oauth2withKeycloak
drwxr-xr-x 5 root root 4096 Jun 10 06:06 monolith2misvc
drwxr-xr-x 3 root root 4096 Jun 10 07:17 circuitbreaker
drwxr-xr-x 2 root root 4096 Jun 13 01:19 kafka-base
drwxr-xr-x 5 root root 4096 Jun 13 01:58 Keycloak-OAuth2-1
drwxr-xr-x 3 root root 4096 Jun 13 02:05 cna-pubsub
drwxr-xr-x 4 root root 4096 Jun 13 02:53 cna-pubsub2
drwxr-xr-x 8 root root 4096 Jun 13 05:34 Kafka-Scaling
drwxr-xr-x 7 root root 4096 Jun 13 05:34 Kafka-Retry-DLQ
drwxr-xr-x 7 root root 4096 Jun 13 06:51 Kafka-Manual-Commit
drwxr-xr-x 7 root root 4096 Jun 14 00:21 advanced-connect
drwxr-xr-x 6 root root 4096 Jun 14 01:08 dp-composite-svc
drwxr-xr-x 4 root root 4096 Jun 14 02:12 dp-graphql
drwxr-xr-x 3 root root 4096 Jun 14 02:27 dp-cqrs
drwxr-xr-x 4 root root 4096 Jun 14 04:24 contract-test
drwxr-xr-x 3 root root 4096 Jun 14 05:08 ops-deploy-my-app
-rw-r--r-- 1 root root   46 Jun 20 02:21 index.html
-rw-r--r-- 1 root root   50 Jun 20 02:22 Dockerfile
root@labs--311032102:/home/project# cd 
root@labs--311032102:~# ls -lrt
total 4
-rw-r--r-- 1 root root 46 Jun 20 02:20 index.html
root@labs--311032102:~# ls -lr
total 4
-rw-r--r-- 1 root root 46 Jun 20 02:20 index.html
root@labs--311032102:~# ls -lrt
total 4
-rw-r--r-- 1 root root 46 Jun 20 02:20 index.html
root@labs--311032102:~# ls -la
total 108
drwx------ 1 root root  4096 Jun 20 04:20 .
drwxr-xr-x 1 root root  4096 Jun 12 15:20 ..
drwxr-xr-x 2 root root  4096 Jun 20 04:20 .aws
-rw------- 1 root root  2185 Jun 14 05:50 .bash_history
-rw-r--r-- 1 root root  3141 Jun 20 02:17 .bashrc
drwxr-xr-x 1 root root  4096 Jun 13 02:07 .cache
drwxr-xr-x 1 root root  4096 Jun 14 02:19 .config
drwxr-xr-x 2 root root  4096 Jun 20 02:16 .dart
drwxr-xr-x 4 root root  4096 Jun 20 02:16 .dartServer
drwx------ 2 root root  4096 Jun 20 02:53 .docker
-rw-r--r-- 1 root root   298 May 17 06:07 .fzf.bash
drwx------ 1 root root  4096 May  4 19:53 .gnupg
drwx------ 2 root root  4096 Jun 13 00:30 .httpie
drwxr-xr-x 4 root root  4096 Jun 14 05:19 .kube
drwx------ 4 root root  4096 May  4 19:51 .local
drwxr-xr-x 3 root root  4096 Jun 13 00:10 .m2
drwxr-xr-x 4 root root  4096 Jun 14 02:19 .npm
-rw-r--r-- 1 root root   148 Aug 17  2015 .profile
drwxr-x--- 2 root root  4096 Jun 13 00:15 .siege
drwxr-xr-x 6 root root  4096 Jun 13 00:06 .theia
-rw------- 1 root root 13597 Jun 20 02:53 .viminfo
-rw-r--r-- 1 root root   203 May 17 06:07 .wget-hsts
-rw-r--r-- 1 root root    46 Jun 20 02:20 index.html
root@labs--311032102:~# cd .asw
bash: cd: .asw: No such file or directory
root@labs--311032102:~# cd .aws
root@labs--311032102:~/.aws# ls
config  credentials
root@labs--311032102:~/.aws# ls -lrt
total 8
-rw------- 1 root root 82 Jun 20 04:20 credentials
-rw------- 1 root root 48 Jun 20 04:20 config
root@labs--311032102:~/.aws# cd config
bash: cd: config: Not a directory
root@labs--311032102:~/.aws# ls
config  credentials
root@labs--311032102:~/.aws# aws configuration

usage: aws [options] <command> <subcommand> [<subcommand> ...] [parameters]
To see help text, you can run:

  aws help
  aws <command> help
  aws <command> <subcommand> help

aws: error: argument command: Invalid choice, valid choices are:

accessanalyzer                           | account                                 
acm                                      | acm-pca                                 
alexaforbusiness                         | amp                                     
amplify                                  | amplifybackend                          
amplifyuibuilder                         | apigateway                              
apigatewaymanagementapi                  | apigatewayv2                            
appconfig                                | appconfigdata                           
appflow                                  | appintegrations                         
application-autoscaling                  | application-insights                    
applicationcostprofiler                  | appmesh                                 
apprunner                                | appstream                               
appsync                                  | athena                                  
auditmanager                             | autoscaling                             
autoscaling-plans                        | backup                                  
backup-gateway                           | batch                                   
billingconductor                         | braket                                  
budgets                                  | ce                                      
chime                                    | chime-sdk-identity                      
chime-sdk-media-pipelines                | chime-sdk-meetings                      
chime-sdk-messaging                      | cloud9                                  
cloudcontrol                             | clouddirectory                          
cloudformation                           | cloudfront                              
cloudhsm                                 | cloudhsmv2                              
cloudsearch                              | cloudsearchdomain                       
cloudtrail                               | cloudwatch                              
codeartifact                             | codebuild                               
codecommit                               | codeguru-reviewer                       
codeguruprofiler                         | codepipeline                            
codestar                                 | codestar-connections                    
codestar-notifications                   | cognito-identity                        
cognito-idp                              | cognito-sync                            
comprehend                               | comprehendmedical                       
compute-optimizer                        | connect                                 
connect-contact-lens                     | connectparticipant                      
cur                                      | customer-profiles                       
databrew                                 | dataexchange                            
datapipeline                             | datasync                                
dax                                      | detective                               
devicefarm                               | devops-guru                             
directconnect                            | discovery                               
dlm                                      | dms                                     
docdb                                    | drs                                     
ds                                       | dynamodb                                
dynamodbstreams                          | ebs                                     
ec2                                      | ec2-instance-connect                    
ecr                                      | ecr-public                              
ecs                                      | efs                                     
eks                                      | elastic-inference                       
elasticache                              | elasticbeanstalk                        
elastictranscoder                        | elb                                     
elbv2                                    | emr                                     
emr-containers                           | es                                      
events                                   | evidently                               
finspace                                 | finspace-data                           
firehose                                 | fis                                     
fms                                      | forecast                                
forecastquery                            | frauddetector                           
fsx                                      | gamelift                                
gamesparks                               | glacier                                 
globalaccelerator                        | glue                                    
grafana                                  | greengrass                              
greengrassv2                             | groundstation                           
guardduty                                | health                                  
healthlake                               | honeycode                               
iam                                      | identitystore                           
imagebuilder                             | importexport                            
inspector                                | inspector2                              
iot                                      | iot-data                                
iot-jobs-data                            | iot1click-devices                       
iot1click-projects                       | iotanalytics                            
iotdeviceadvisor                         | iotevents                               
iotevents-data                           | iotfleethub                             
iotsecuretunneling                       | iotsitewise                             
iotthingsgraph                           | iottwinmaker                            
iotwireless                              | ivs                                     
ivschat                                  | kafka                                   
kafkaconnect                             | kendra                                  
keyspaces                                | kinesis                                 
kinesis-video-archived-media             | kinesis-video-media                     
kinesis-video-signaling                  | kinesisanalytics                        
kinesisanalyticsv2                       | kinesisvideo                            
kms                                      | lakeformation                           
lambda                                   | lex-models                              
lex-runtime                              | lexv2-models                            
lexv2-runtime                            | license-manager                         
lightsail                                | location                                
logs                                     | lookoutequipment                        
lookoutmetrics                           | lookoutvision                           
machinelearning                          | macie                                   
macie2                                   | managedblockchain                       
marketplace-catalog                      | marketplace-entitlement                 
marketplacecommerceanalytics             | mediaconnect                            
mediaconvert                             | medialive                               
mediapackage                             | mediapackage-vod                        
mediastore                               | mediastore-data                         
mediatailor                              | memorydb                                
meteringmarketplace                      | mgh                                     
mgn                                      | migration-hub-refactor-spaces           
migrationhub-config                      | migrationhubstrategy                    
mobile                                   | mq                                      
mturk                                    | mwaa                                    
neptune                                  | network-firewall                        
networkmanager                           | nimble                                  
opensearch                               | opsworks                                
opsworkscm                               | organizations                           
outposts                                 | panorama                                
personalize                              | personalize-events                      
personalize-runtime                      | pi                                      
pinpoint                                 | pinpoint-email                          
pinpoint-sms-voice                       | pinpoint-sms-voice-v2                   
polly                                    | pricing                                 
proton                                   | qldb                                    
qldb-session                             | quicksight                              
ram                                      | rbin                                    
rds                                      | rds-data                                
redshift                                 | redshift-data                           
rekognition                              | resiliencehub                           
resource-groups                          | resourcegroupstaggingapi                
robomaker                                | route53                                 
route53-recovery-cluster                 | route53-recovery-control-config         
route53-recovery-readiness               | route53domains                          
route53resolver                          | rum                                     
s3control                                | s3outposts                              
sagemaker                                | sagemaker-a2i-runtime                   
sagemaker-edge                           | sagemaker-featurestore-runtime          
sagemaker-runtime                        | savingsplans                            
schemas                                  | sdb                                     
secretsmanager                           | securityhub                             
serverlessrepo                           | service-quotas                          
servicecatalog                           | servicecatalog-appregistry              
servicediscovery                         | ses                                     
sesv2                                    | shield                                  
signer                                   | sms                                     
snow-device-management                   | snowball                                
sns                                      | sqs                                     
ssm                                      | ssm-contacts                            
ssm-incidents                            | sso                                     
sso-admin                                | sso-oidc                                
stepfunctions                            | storagegateway                          
sts                                      | support                                 
swf                                      | synthetics                              
textract                                 | timestream-query                        
timestream-write                         | transcribe                              
transfer                                 | translate                               
voice-id                                 | waf                                     
waf-regional                             | wafv2                                   
wellarchitected                          | wisdom                                  
workdocs                                 | worklink                                
workmail                                 | workmailmessageflow                     
workspaces                               | workspaces-web                          
xray                                     | s3api                                   
s3                                       | ddb                                     
configure                                | deploy                                  
configservice                            | opsworks-cm                             
history                                  | cli-dev                                 
help                                    

root@labs--311032102:~/.aws# aws configure
AWS Access Key ID [****************er02]: AKIA6H47RAWMZAAJRW7C
AWS Secret Access Key [****************RW7C]: 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
Default region name [ap-northeast-1]: ap-northeast-1
Default output format [json]: 
root@labs--311032102:~/.aws# 
root@labs--311032102:~/.aws# eksctl create cluster --name user02-eks --version 1.19 --spot --managed --nodegroup-name standard-workers --node-type t3.medium --nodes 5 --nodes-min 1 --nodes-max 3
Error: cannot use --nodes-max=3 and --nodes=5 at the same time
root@labs--311032102:~/.aws# eksctl create cluster --name user02-eks --version 1.19 --spot --managed --nodegroup-name standard-workers --node-type t3.medium --nodes 3 --nodes-min 1 --nodes-max 3
2022-06-20 04:26:00 [ℹ]  eksctl version 0.97.0
2022-06-20 04:26:00 [ℹ]  using region ap-northeast-1
2022-06-20 04:26:00 [ℹ]  setting availability zones to [ap-northeast-1a ap-northeast-1c ap-northeast-1d]
2022-06-20 04:26:00 [ℹ]  subnets for ap-northeast-1a - public:192.168.0.0/19 private:192.168.96.0/19
2022-06-20 04:26:00 [ℹ]  subnets for ap-northeast-1c - public:192.168.32.0/19 private:192.168.128.0/19
2022-06-20 04:26:00 [ℹ]  subnets for ap-northeast-1d - public:192.168.64.0/19 private:192.168.160.0/19
2022-06-20 04:26:00 [ℹ]  nodegroup "standard-workers" will use "" [AmazonLinux2/1.19]
2022-06-20 04:26:00 [ℹ]  using Kubernetes version 1.19
2022-06-20 04:26:00 [ℹ]  creating EKS cluster "user02-eks" in "ap-northeast-1" region with managed nodes
2022-06-20 04:26:00 [ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial managed nodegroup
2022-06-20 04:26:00 [ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=ap-northeast-1 --cluster=user02-eks'
2022-06-20 04:26:00 [ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster "user02-eks" in "ap-northeast-1"
2022-06-20 04:26:00 [ℹ]  CloudWatch logging will not be enabled for cluster "user02-eks" in "ap-northeast-1"
2022-06-20 04:26:00 [ℹ]  you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=ap-northeast-1 --cluster=user02-eks'
2022-06-20 04:26:00 [ℹ]  
2 sequential tasks: { create cluster control plane "user02-eks", 
    2 sequential sub-tasks: { 
        wait for control plane to become ready,
        create managed nodegroup "standard-workers",
    } 
}
2022-06-20 04:26:00 [ℹ]  building cluster stack "eksctl-user02-eks-cluster"
2022-06-20 04:26:01 [ℹ]  deploying stack "eksctl-user02-eks-cluster"
2022-06-20 04:26:31 [ℹ]  waiting for CloudFormation stack "eksctl-user02-eks-cluster"
2022-06-20 04:27:01 [ℹ]  waiting for CloudFormation stack "eksctl-user02-eks-cluster"

eksctl create cluster --name [mycluster-userid] --version 1.19 --spot --managed --nodegroup-name standard-workers --node-type t3.medium --nodes 5 --nodes-min 1 --nodes-max 3
eksctl create cluster --name user02-eks --version 1.19 --spot --managed --nodegroup-name standard-workers --node-type t3.medium --nodes 5 --nodes-min 1 --nodes-max 3

root@labs--311032102:/home/project/monolith2misvc# kubectl get nodes
Kubeconfig user entry is using deprecated API version client.authentication.k8s.io/v1alpha1. Run 'aws eks update-kubeconfig' to update.
NAME                                               STATUS   ROLES    AGE   VERSION
ip-192-168-29-79.ap-northeast-1.compute.internal   Ready    <none>   15m   v1.19.15-eks-9c63c4
ip-192-168-45-77.ap-northeast-1.compute.internal   Ready    <none>   15m   v1.19.15-eks-9c63c4
ip-192-168-91-26.ap-northeast-1.compute.internal   Ready    <none>   15m   v1.19.15-eks-9c63c4

--> REGION 변경 
root@labs--311032102:/home/project/monolith2misvc# aws eks --region ap-northeast-1 update-kubeconfig --name user02-eks
Added new context arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks to /root/.kube/config
root@labs--311032102:/home/project/monolith2misvc# 

root@labs--311032102:/home/project/monolith2misvc# cat /root/.kube/config
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://35.189.156.127
  name: kcb-test2.k8s.local
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1EWXlNREEwTXpFMU9Gb1hEVE15TURZeE56QTBNekUxT0Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHNECkhWUHNhMUVuUGhNdjR3NkZXa01KR2oyd1V0UHNGWmxKRGhGK3ZBNkwvS293OUJEVW5KTG5IdXFrWXl1ZlBtSDIKNSs0aTNQZVUzeWIzckFCTGdZRlNpU2k2VERCbVFXejVxbGNOeUxlYU5CWExvNFlFdFNNWTBhcFlYMVF4RFBlKwprM2l5VXdjMkpNa0xEVTM2clpFR3pyYi8zZFZ3c05BUmJHUjZmd3gxMEZpYXR3cVRnUVpmK2JOZ05NdWplR0p2CmJIRER2eHEzaFVSa3kvUjNlNUxPNDlWNTBLbzFPR21DSitDaVliY0tFY0tWanU2SmptYVhwNjU5VU83LzNMTmwKVDl2bTVlTHhLYkZkYTA1SWtzeno2TUZ1SFhmdTdCZm05Y3NzYzkrZFRJZlVrR05YVTFsV3pkMFVnbS9Vb0RTbgpHRGoyeXFwOERSY1RQMzI2bkFFQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZKT0VRbFpERWVYN2Nnc1BXOG9meEdQcUdodjZNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFCQ01vTWtKazQxV1l6elU2OEg3aml6SFByWkpaYTg2UGJ0T203YXlwNllrbnJjeVMvTQpPaStQLyt4NjM2N3dHYTc0bjFsSkFTRHpFS3dFV013U2NKRmxKUU5qWExoRW5obUhWa3FLRXZ4SUNEa3YyZVA2CmdRZ1p4OFcwT3QyeWxnR1lvdU1yb09zL1RlSXJDR3dtcmVZR2pEUHh3RnJSVFg3b1NNZTVxSktqa2xyakFwa08KaFFmdUhLVjMzZnlMaXF2SzB5dnNsS2pETzQ3dlFIWFZkQWR0WkZQOXZ0eWluNHRBdkc2SVRTN05lcDY2UEZOWgpWMElwK20wQm0wdVBLTHVUMC81VEt5MHlSUGhEd2lQL1BKbnRNRnFBVlRubUUwWlJqcUUzK2l2VjdaQjNiY0NwCjlmT09TWmtWMXZCdXIrU0xJbUpFRTBaczBqeFdtelpYSmxCTQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://77E4F9C82F02ADF5802F48EC25844915.gr7.ap-northeast-1.eks.amazonaws.com
  name: user02-eks.ap-northeast-1.eksctl.io
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1EWXlNREEwTXpFMU9Gb1hEVE15TURZeE56QTBNekUxT0Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHNECkhWUHNhMUVuUGhNdjR3NkZXa01KR2oyd1V0UHNGWmxKRGhGK3ZBNkwvS293OUJEVW5KTG5IdXFrWXl1ZlBtSDIKNSs0aTNQZVUzeWIzckFCTGdZRlNpU2k2VERCbVFXejVxbGNOeUxlYU5CWExvNFlFdFNNWTBhcFlYMVF4RFBlKwprM2l5VXdjMkpNa0xEVTM2clpFR3pyYi8zZFZ3c05BUmJHUjZmd3gxMEZpYXR3cVRnUVpmK2JOZ05NdWplR0p2CmJIRER2eHEzaFVSa3kvUjNlNUxPNDlWNTBLbzFPR21DSitDaVliY0tFY0tWanU2SmptYVhwNjU5VU83LzNMTmwKVDl2bTVlTHhLYkZkYTA1SWtzeno2TUZ1SFhmdTdCZm05Y3NzYzkrZFRJZlVrR05YVTFsV3pkMFVnbS9Vb0RTbgpHRGoyeXFwOERSY1RQMzI2bkFFQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZKT0VRbFpERWVYN2Nnc1BXOG9meEdQcUdodjZNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFCQ01vTWtKazQxV1l6elU2OEg3aml6SFByWkpaYTg2UGJ0T203YXlwNllrbnJjeVMvTQpPaStQLyt4NjM2N3dHYTc0bjFsSkFTRHpFS3dFV013U2NKRmxKUU5qWExoRW5obUhWa3FLRXZ4SUNEa3YyZVA2CmdRZ1p4OFcwT3QyeWxnR1lvdU1yb09zL1RlSXJDR3dtcmVZR2pEUHh3RnJSVFg3b1NNZTVxSktqa2xyakFwa08KaFFmdUhLVjMzZnlMaXF2SzB5dnNsS2pETzQ3dlFIWFZkQWR0WkZQOXZ0eWluNHRBdkc2SVRTN05lcDY2UEZOWgpWMElwK20wQm0wdVBLTHVUMC81VEt5MHlSUGhEd2lQL1BKbnRNRnFBVlRubUUwWlJqcUUzK2l2VjdaQjNiY0NwCjlmT09TWmtWMXZCdXIrU0xJbUpFRTBaczBqeFdtelpYSmxCTQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://77E4F9C82F02ADF5802F48EC25844915.gr7.ap-northeast-1.eks.amazonaws.com
  name: arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks
contexts:
- context:
    cluster: kcb-test2.k8s.local
    namespace: labs--311032102
    user: labs--311032102
  name: kcb-test2.k8s.local
- context:
    cluster: user02-eks.ap-northeast-1.eksctl.io
    user: user02@user02-eks.ap-northeast-1.eksctl.io
  name: user02@user02-eks.ap-northeast-1.eksctl.io
- context:
    cluster: arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks
    user: arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks
  name: arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks
current-context: arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks
kind: Config
preferences: {}
users:
- name: labs--311032102
  user:
    token: eyJhbGciOiJSUzI1NiIsImtpZCI6InFQaE1menpURGlISnprRm44Tmt6UGRNeEVBS3ZoMHQ0MGRrR04zSmZwUjQifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJsYWJzLS0zMTEwMzIxMDIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoibGFicy0tMzExMDMyMTAyLXRva2VuLXg3NXFzIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImxhYnMtLTMxMTAzMjEwMiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImFkYjRkMjUzLTQyMGItNGNhOS1iNjM0LWMzODc2ODUwZjk3YyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpsYWJzLS0zMTEwMzIxMDI6bGFicy0tMzExMDMyMTAyIn0.emrq_HwF-jxjPJuICYl_7mxbuR5-Re8HErPNZriqPHLj9U5ZQnPD27y5WCjx7IrAirvWvBZg-rCfn0TbOhB31V8Iahug0QftMEKcXBSayB8nQk_cXrWdkRic0k0_sU3JfyLQ4OSDyqfDhuPJOua5rtbMYvwLgFA-PeasZU4Hsxjztc0f1ier_ecR4C_ZBtcl_t1ojh-eVqrChIv6Wje9z3M-juhnMVV75fP4JHKUk_8ZxMXrDOUY6OY5fImLEO1MyZeGNE4IW817D_GPL00LhNs4Wr-LZm-ZNm4bsKtAmziir_SqS02w8E3KPZagIrHCT-u9_ErlFyXul2-8bP5leg
- name: user02@user02-eks.ap-northeast-1.eksctl.io
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1alpha1
      args:
      - eks
      - get-token
      - --cluster-name
      - user02-eks
      - --region
      - ap-northeast-1
      command: aws
      env:
      - name: AWS_STS_REGIONAL_ENDPOINTS
        value: regional
      provideClusterInfo: false
- name: arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - --region
      - ap-northeast-1
      - eks
      - get-token
      - --cluster-name
      - user02-eks
      command: aws

## [2번째 실행했을경우] aws configure + region 설정후 정보를 조회할수 있음.
root@labs--311032102:/# kubectl get all
NAME                                  READY   STATUS    RESTARTS   AGE
pod/nginx-6799fc88d8-4b9wl            1/1     Running   0          17h
pod/nginx-6799fc88d8-bh6d7            1/1     Running   0          17h
pod/nginx-6799fc88d8-xhj5j            1/1     Running   0          17h
pod/nginx-gorapa-2-8559b5bc76-8xgc8   1/1     Running   0          17h
pod/nginx-gorapa-2-8559b5bc76-mzblw   1/1     Running   0          17h
pod/nginx-gorapa-2-8559b5bc76-sbwq9   1/1     Running   0          17h
pod/order-6d7c686964-s5vfm            1/1     Running   0          18h
pod/siege                             1/1     Running   0          16h
pod/user02-order-5fd6ff986d-4q9q6     1/1     Running   0          18h

NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/kubernetes       ClusterIP   10.100.0.1      <none>        443/TCP    20h
service/nginx-gorapa-2   ClusterIP   10.100.91.162   <none>        8080/TCP   16h

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx            3/3     3            3           17h
deployment.apps/nginx-gorapa-2   3/3     3            3           17h
deployment.apps/order            1/1     1            1           18h
deployment.apps/user02-order     1/1     1            1           18h

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-6799fc88d8            3         3         3       17h
replicaset.apps/nginx-gorapa-2-8559b5bc76   3         3         3       17h
replicaset.apps/order-6d7c686964            1         1         1       18h
replicaset.apps/user02-order-5fd6ff986d     1         1         1       18h
root@labs--311032102:/# 

root@labs--311032102:/# kubectl get nodes
NAME                                               STATUS   ROLES    AGE   VERSION
ip-192-168-29-79.ap-northeast-1.compute.internal   Ready    <none>   19h   v1.19.15-eks-9c63c4
ip-192-168-45-77.ap-northeast-1.compute.internal   Ready    <none>   19h   v1.19.15-eks-9c63c4
ip-192-168-91-26.ap-northeast-1.compute.internal   Ready    <none>   19h   v1.19.15-eks-9c63c4

## 서버 떠 있는 것을 확인가능함.
root@labs--311032102:/# kubectl get pods -o wide   
NAME                              READY   STATUS    RESTARTS   AGE   IP               NODE                                               NOMINATED NODE   READINESS GATES
nginx-6799fc88d8-4b9wl            1/1     Running   0          17h   192.168.43.80    ip-192-168-45-77.ap-northeast-1.compute.internal   <none>           <none>
nginx-6799fc88d8-bh6d7            1/1     Running   0          17h   192.168.3.78     ip-192-168-29-79.ap-northeast-1.compute.internal   <none>           <none>
nginx-6799fc88d8-xhj5j            1/1     Running   0          17h   192.168.88.157   ip-192-168-91-26.ap-northeast-1.compute.internal   <none>           <none>
nginx-gorapa-2-8559b5bc76-8xgc8   1/1     Running   0          17h   192.168.58.153   ip-192-168-45-77.ap-northeast-1.compute.internal   <none>           <none>
nginx-gorapa-2-8559b5bc76-mzblw   1/1     Running   0          17h   192.168.65.154   ip-192-168-91-26.ap-northeast-1.compute.internal   <none>           <none>
nginx-gorapa-2-8559b5bc76-sbwq9   1/1     Running   0          17h   192.168.30.9     ip-192-168-29-79.ap-northeast-1.compute.internal   <none>           <none>
order-6d7c686964-s5vfm            1/1     Running   0          18h   192.168.9.201    ip-192-168-29-79.ap-northeast-1.compute.internal   <none>           <none>
siege                             1/1     Running   0          16h   192.168.51.156   ip-192-168-45-77.ap-northeast-1.compute.internal   <none>           <none>
user02-order-5fd6ff986d-4q9q6     1/1     Running   0          18h   192.168.75.188   ip-192-168-91-26.ap-northeast-1.compute.internal   <none>           <none>


### ECR (클러스터 스토리지)
https://uengine2.signin.aws.amazon.com/console

uengine2
user02/hana!@12

리전선택 : ap-northeast-1
계정클릭
ID: 9790-5023-5289
IAM 사용자: user02

1. ECR 스토리지 생성 :user02_ecr  
   : 싸이트접속해서... (Elastic Container Registry)
     ==> user02-order 로 생성해야함 ( _ 는 적용되지 않음)


## 매우주의
Elastic Container Registry 클릭 --> 리포지토리생성 (user02_ecr)

==> user02-order 로 생성해야함 ( _ 는 적용되지 않음)


#
사용자 이름
user02 (에 생성됨)
사용자 ARN
arn:aws:iam::979050235289:user/user02 
AWS 계정 ID
979050235289 
계정 정규 사용자 ID 
65d222c82d1bc1e19fc27fae6fa5ba22300911f12e6507fddf2bfa93670b2611 


##
aws --region ap-northeast-1 ecr get-login-password 

root@labs--311032102:/home/project/monolith2misvc# aws --region ap-northeast-1 ecr get-login-password 
eyJwYXlsb2FkIjoiMk9BWjRZZldJaTBZZDBDVUU0NG1jbkFKcW9UTXhUUzBHdStnNFRvc25reEZFY1ZvR3RRYWVnMVUwSUkrcGxvcXJmUWZtTmlNQjlUMjdUckJIc3lJN3V0d21pWVBvUS9ybmlnK2o4VE5TdWU3TndKN2xTS0dzQzdDSkJPWkxhYkErOTRSLzQ3UUIvdnBIbEZyeWdhOTlZdTV0bHlPUlBwTFo0MmxGem9jYW9ESWxtVjlGVnFxTTgwNHZaZmR0MlE2UFpnRjFPVmtKZ3d2b05HckY1cklaN3ErcEJEMjZEcHdXN3ozQ1d6ZW4rdzN5UEN0TlpPSWNjdFprY3ZtUVZ3WERZWWRTanpnT1ZNaEgrbU9lb1JLdjlVN1V3K2FEN0o1OXc2NHdMQVRRWCtkUDB0YUZpZGwwVmtMZTdXMjZGNmM3VW14WS9GVUIzeXhHVFB3dlVFU0NjTGl0MzQrekdob3NoaVhGNXF6Y3UxWWYrOExEak1ROE1LUDVVelAzcUMyYjUzcEp3cXZTcks1QUNobUhGQTlwUWZQUE9nbXJiRGh4R0hTNStoVWxmc294WTBjUW10TU92RGZOZnMwZ3dzSzhYTE02N2tFaW05bkZjbHNIeUc0QjJiM0lFcXVxSnc1YU1Sa3V1NXpacXM0OEs0MlczQ3B2WXI1MjdiTExDWE1rYzFaLzZyNWJkcmJqeDNUYWhkcHNrcW9WQ0xoUWpTb3RSQnF0ZmFkVXdPbnRpdjJVUTU0SGZjc2ZhNE50ZmQzSGlxc0lPdHpvOVdLNGpNcS8zRDk0dFNVUTRxcHNQUytTM3RkM2dOcHBFL2xOTVRoSzMxeUdUdW01c1FlNlF6c21GRndVanBYeWdTdzV0U2szM2FadWU0WFF1amc0SG40UFpPYnJ2SEF1QjVieXBMRVlRQnNmakhoRkpmNDFmTjZrZnhUTEFRbDBrKzR2QmRVMzU3OTZlK2JFbVRMZkV4SGZPSkI5MHVaVTBIclB2RWNZSHUrZzQ3T0NleEdoVmt5My94MFJ0bkJYbVJXc3RRRGRMRFMzL1lmU1JDUE5CMVdwMGowK3VkN3k2WFlndVowRGpkODQwSUdBb2ZXTStPWWd6bnZTR0k4VW5YcXloVHF2aXZtOUJEVElPWWxpUXdVdDNBNGErVUZ6bDBzc1N4NHh1ODRQaUlWaE52L0RJTW55Y3hMb1ZCbERuZGN1dnBHWmo1dm9ydzFUYS81UzlVNDdnbGhHaTFWMkVERVFhY2paMWVTTFc1NWVDOTBuWGxWKzI1MlZNQlVmWG11ZlV6NXNpL1lvWWRXbVBsM3VnV2R4M0cxYjdmVE5kQmUxSWVaWnY0UVU2TlBjMmlxZGgxcGhKNGF2WUoyVk16T3AvQmd5N1ZpZDRUYTMwajFqRjQyOHRTazY3ST0iLCJkYXRha2V5IjoiQVFFQkFIZ0FNZktEbElvcEM2enMwYk1kUnJZU0hhL0MzOWtDcmNQOGtWcHJFOWYra1FBQUFINHdmQVlKS29aSWh2Y05BUWNHb0c4d2JRSUJBREJvQmdrcWhraUc5dzBCQndFd0hnWUpZSVpJQVdVREJBRXVNQkVFREloVVFIZXFjOHFwc1c1SFdnSUJFSUE3VmNMNjV6Zjl2TU05S1Z3bm5sbnpjWTRiQVMrYS9rdlozUFNqUHRoN3NmUE55K1V4R1QyeXdPOGVOTFJZMk5Tc2hhMzJSTVlzZjh6WWhwVT0iLCJ2ZXJzaW9uIjoiMiIsInR5cGUiOiJEQVRBX0tFWSIsImV4cGlyYXRpb24iOjE2NTU3NDUwNDV9

## https://jwt.io/ 접속후 넣으면 아래 "expiration" 안에 사용해야함.

{
  "payload": "2OAZ4YfWIi0Yd0CUE44mcnAJqoTMxTS0Gu+g4TosnkxFEcVoGtQaeg1U0II+ploqrfQfmNiMB9T27TrBHsyI7utwmiYPoQ/rnig+j8TNSue7NwJ7lSKGsC7CJBOZLabA+94R/47QB/vpHlFryga99Yu5tlyORPpLZ42lFzocaoDIlmV9FVqqM804vZfdt2Q6PZgF1OVkJgwvoNGrF5rIZ7q+pBD26DpwW7z3CWzen+w3yPCtNZOIcctZkcvmQVwXDYYdSjzgOVMhH+mOeoRKv9U7Uw+aD7J59w64wLATQX+dP0taFidl0VkLe7W26F6c7UmxY/FUB3yxGTPwvUESCcLit34+zGhoshiXF5qzcu1Yf+8LDjMQ8MKP5UzP3qC2b53pJwqvSrK5AChmHFA9pQfPPOgmrbDhxGHS5+hUlfsoxY0cQmtMOvDfNfs0gwsK8XLM67kEim9nFclsHyG4B2b3IEquqJw5aMRkuu5zZqs48K42W3CpvYr527bLLCXMkc1Z/6r5bdrbjx3TahdpskqoVCLhQjSotRBqtfadUwOntiv2UQ54Hfcsfa4Ntfd3HiqsIOtzo9WK4jMq/3D94tSUQ4qpsPS+S3td3gNppE/lNMThK31yGTum5sQe6QzsmFFwUjpXygSw5tSk33aZue4XQujg4Hn4PZObrvHAuB5bypLEYQBsfjHhFJf41fN6kfxTLAQl0k+4vBdU35796e+bEmTLfExHfOJB90uZU0HrPvEcYHu+g47OCexGhVky3/x0RtnBXmRWstQDdLDS3/YfSRCPNB1Wp0j0+ud7y6XYguZ0Djd840IGAofWM+OYgznvSGI8UnXqyhTqvivm9BDTIOYliQwUt3A4a+UFzl0ssSx4xu84PiIVhNv/DIMnycxLoVBlDndcuvpGZj5vorw1Ta/5S9U47glhGi1V2EDEQacjZ1eSLW55eC90nXlV+252VMBUfXmufUz5si/YoYdWmPl3ugWdx3G1b7fTNdBe1IeZZv4QU6NPc2iqdh1phJ4avYJ2VMzOp/Bgy7Vid4Ta30j1jF428tSk67I=",
  "datakey": "AQEBAHgAMfKDlIopC6zs0bMdRrYSHa/C39kCrcP8kVprE9f+kQAAAH4wfAYJKoZIhvcNAQcGoG8wbQIBADBoBgkqhkiG9w0BBwEwHgYJYIZIAWUDBAEuMBEEDIhUQHeqc8qpsW5HWgIBEIA7VcL65zf9vMM9KVwnnlnzcY4bAS+a/kvZ3PSjPth7sfPNy+UxGT2ywO8eNLRY2NSsha32RMYsf8zYhpU=",
  "version": "2",
  "type": "DATA_KEY",
  "expiration": 1655745045
}

docker login --username AWS -p 아주긴도커패스워드 [AWS유저아이디-숫자로만된].dkr.ecr.[리전명].amazonaws.com
docker login --username AWS -p eyJwYXlsb2FkIjoiMk9BWjRZZldJaTBZZDBDVUU0NG1jbkFKcW9UTXhUUzBHdStnNFRvc25reEZFY1ZvR3RRYWVnMVUwSUkrcGxvcXJmUWZtTmlNQjlUMjdUckJIc3lJN3V0d21pWVBvUS9ybmlnK2o4VE5TdWU3TndKN2xTS0dzQzdDSkJPWkxhYkErOTRSLzQ3UUIvdnBIbEZyeWdhOTlZdTV0bHlPUlBwTFo0MmxGem9jYW9ESWxtVjlGVnFxTTgwNHZaZmR0MlE2UFpnRjFPVmtKZ3d2b05HckY1cklaN3ErcEJEMjZEcHdXN3ozQ1d6ZW4rdzN5UEN0TlpPSWNjdFprY3ZtUVZ3WERZWWRTanpnT1ZNaEgrbU9lb1JLdjlVN1V3K2FEN0o1OXc2NHdMQVRRWCtkUDB0YUZpZGwwVmtMZTdXMjZGNmM3VW14WS9GVUIzeXhHVFB3dlVFU0NjTGl0MzQrekdob3NoaVhGNXF6Y3UxWWYrOExEak1ROE1LUDVVelAzcUMyYjUzcEp3cXZTcks1QUNobUhGQTlwUWZQUE9nbXJiRGh4R0hTNStoVWxmc294WTBjUW10TU92RGZOZnMwZ3dzSzhYTE02N2tFaW05bkZjbHNIeUc0QjJiM0lFcXVxSnc1YU1Sa3V1NXpacXM0OEs0MlczQ3B2WXI1MjdiTExDWE1rYzFaLzZyNWJkcmJqeDNUYWhkcHNrcW9WQ0xoUWpTb3RSQnF0ZmFkVXdPbnRpdjJVUTU0SGZjc2ZhNE50ZmQzSGlxc0lPdHpvOVdLNGpNcS8zRDk0dFNVUTRxcHNQUytTM3RkM2dOcHBFL2xOTVRoSzMxeUdUdW01c1FlNlF6c21GRndVanBYeWdTdzV0U2szM2FadWU0WFF1amc0SG40UFpPYnJ2SEF1QjVieXBMRVlRQnNmakhoRkpmNDFmTjZrZnhUTEFRbDBrKzR2QmRVMzU3OTZlK2JFbVRMZkV4SGZPSkI5MHVaVTBIclB2RWNZSHUrZzQ3T0NleEdoVmt5My94MFJ0bkJYbVJXc3RRRGRMRFMzL1lmU1JDUE5CMVdwMGowK3VkN3k2WFlndVowRGpkODQwSUdBb2ZXTStPWWd6bnZTR0k4VW5YcXloVHF2aXZtOUJEVElPWWxpUXdVdDNBNGErVUZ6bDBzc1N4NHh1ODRQaUlWaE52L0RJTW55Y3hMb1ZCbERuZGN1dnBHWmo1dm9ydzFUYS81UzlVNDdnbGhHaTFWMkVERVFhY2paMWVTTFc1NWVDOTBuWGxWKzI1MlZNQlVmWG11ZlV6NXNpL1lvWWRXbVBsM3VnV2R4M0cxYjdmVE5kQmUxSWVaWnY0UVU2TlBjMmlxZGgxcGhKNGF2WUoyVk16T3AvQmd5N1ZpZDRUYTMwajFqRjQyOHRTazY3ST0iLCJkYXRha2V5IjoiQVFFQkFIZ0FNZktEbElvcEM2enMwYk1kUnJZU0hhL0MzOWtDcmNQOGtWcHJFOWYra1FBQUFINHdmQVlKS29aSWh2Y05BUWNHb0c4d2JRSUJBREJvQmdrcWhraUc5dzBCQndFd0hnWUpZSVpJQVdVREJBRXVNQkVFREloVVFIZXFjOHFwc1c1SFdnSUJFSUE3VmNMNjV6Zjl2TU05S1Z3bm5sbnpjWTRiQVMrYS9rdlozUFNqUHRoN3NmUE55K1V4R1QyeXdPOGVOTFJZMk5Tc2hhMzJSTVlzZjh6WWhwVT0iLCJ2ZXJzaW9uIjoiMiIsInR5cGUiOiJEQVRBX0tFWSIsImV4cGlyYXRpb24iOjE2NTU3NDUwNDV9 979050235289.dkr.ecr.ap-northeast-1.amazonaws.com

root@labs--311032102:/home/project/monolith2misvc# docker login --username AWS -p eyJwYXlsb2FkIjoiMk9BWjRZZldJaTBZZDBDVUU0NG1jbkFKcW9UTXhUUzBHdStnNFRvc25reEZFY1ZvR3RRYWVnMVUwSUkrcGxvcXJmUWZtTmlNQjlUMjdUckJIc3lJN3V0d21pWVBvUS9ybmlnK2o4VE5TdWU3TndKN2xTS0dzQzdDSkJPWkxhYkErOTRSLzQ3UUIvdnBIbEZyeWdhOTlZdTV0bHlPUlBwTFo0MmxGem9jYW9ESWxtVjlGVnFxTTgwNHZaZmR0MlE2UFpnRjFPVmtKZ3d2b05HckY1cklaN3ErcEJEMjZEcHdXN3ozQ1d6ZW4rdzN5UEN0TlpPSWNjdFprY3ZtUVZ3WERZWWRTanpnT1ZNaEgrbU9lb1JLdjlVN1V3K2FEN0o1OXc2NHdMQVRRWCtkUDB0YUZpZGwwVmtMZTdXMjZGNmM3VW14WS9GVUIzeXhHVFB3dlVFU0NjTGl0MzQrekdob3NoaVhGNXF6Y3UxWWYrOExEak1ROE1LUDVVelAzcUMyYjUzcEp3cXZTcks1QUNobUhGQTlwUWZQUE9nbXJiRGh4R0hTNStoVWxmc294WTBjUW10TU92RGZOZnMwZ3dzSzhYTE02N2tFaW05bkZjbHNIeUc0QjJiM0lFcXVxSnc1YU1Sa3V1NXpacXM0OEs0MlczQ3B2WXI1MjdiTExDWE1rYzFaLzZyNWJkcmJqeDNUYWhkcHNrcW9WQ0xoUWpTb3RSQnF0ZmFkVXdPbnRpdjJVUTU0SGZjc2ZhNE50ZmQzSGlxc0lPdHpvOVdLNGpNcS8zRDk0dFNVUTRxcHNQUytTM3RkM2dOcHBFL2xOTVRoSzMxeUdUdW01c1FlNlF6c21GRndVanBYeWdTdzV0U2szM2FadWU0WFF1amc0SG40UFpPYnJ2SEF1QjVieXBMRVlRQnNmakhoRkpmNDFmTjZrZnhUTEFRbDBrKzR2QmRVMzU3OTZlK2JFbVRMZkV4SGZPSkI5MHVaVTBIclB2RWNZSHUrZzQ3T0NleEdoVmt5My94MFJ0bkJYbVJXc3RRRGRMRFMzL1lmU1JDUE5CMVdwMGowK3VkN3k2WFlndVowRGpkODQwSUdBb2ZXTStPWWd6bnZTR0k4VW5YcXloVHF2aXZtOUJEVElPWWxpUXdVdDNBNGErVUZ6bDBzc1N4NHh1ODRQaUlWaE52L0RJTW55Y3hMb1ZCbERuZGN1dnBHWmo1dm9ydzFUYS81UzlVNDdnbGhHaTFWMkVERVFhY2paMWVTTFc1NWVDOTBuWGxWKzI1MlZNQlVmWG11ZlV6NXNpL1lvWWRXbVBsM3VnV2R4M0cxYjdmVE5kQmUxSWVaWnY0UVU2TlBjMmlxZGgxcGhKNGF2WUoyVk16T3AvQmd5N1ZpZDRUYTMwajFqRjQyOHRTazY3ST0iLCJkYXRha2V5IjoiQVFFQkFIZ0FNZktEbElvcEM2enMwYk1kUnJZU0hhL0MzOWtDcmNQOGtWcHJFOWYra1FBQUFINHdmQVlKS29aSWh2Y05BUWNHb0c4d2JRSUJBREJvQmdrcWhraUc5dzBCQndFd0hnWUpZSVpJQVdVREJBRXVNQkVFREloVVFIZXFjOHFwc1c1SFdnSUJFSUE3VmNMNjV6Zjl2TU05S1Z3bm5sbnpjWTRiQVMrYS9rdlozUFNqUHRoN3NmUE55K1V4R1QyeXdPOGVOTFJZMk5Tc2hhMzJSTVlzZjh6WWhwVT0iLCJ2ZXJzaW9uIjoiMiIsInR5cGUiOiJEQVRBX0tFWSIsImV4cGlyYXRpb24iOjE2NTU3NDUwNDV9 979050235289.dkr.ecr.ap-northeast-1.amazonaws.com
Login Succeeded

============> docker --> ecr 로 이동함.

## 
cd /home/project 에서 
실행 git clone https://github.com/event-storming/monolith   ==> monolith 소스를 현재위치에 다운
cd monolith
mvn package -B

docker build -t [AWS유저아이디-숫자로만된].dkr.ecr.ap-northeast-2.amazonaws.com/order:v1 .

docker build -t 979050235289.dkr.ecr.ap-northeast-1.amazonaws.com/user02_ecr:v1 .

-- 다시 
docker build -t 979050235289.dkr.ecr.ap-northeast-1.amazonaws.com/user02-order:v1 .

oot@labs--311032102:/home/project/monolith# ls -lrt
total 52
-rw-r--r-- 1 root root  215 Jun 20 05:25 Dockerfile
-rw-r--r-- 1 root root  320 Jun 20 05:25 Dockerfile-prod
-rw-r--r-- 1 root root  343 Jun 20 05:25 DockerfileAutoBuild
-rw-r--r-- 1 root root  633 Jun 20 05:25 README.md
drwxr-xr-x 2 root root 4096 Jun 20 05:25 azure
-rw-r--r-- 1 root root 2239 Jun 20 05:25 cloudbuild.yaml
drwxr-xr-x 2 root root 4096 Jun 20 05:25 kubernetes
-rw-r--r-- 1 root root 9260 Jun 20 05:25 monolith.iml
-rw-r--r-- 1 root root 2650 Jun 20 05:25 pom.xml
drwxr-xr-x 3 root root 4096 Jun 20 05:25 src
drwxr-xr-x 6 root root 4096 Jun 20 05:28 target
root@labs--311032102:/home/project/monolith# 
root@labs--311032102:/home/project/monolith# 
root@labs--311032102:/home/project/monolith# cd tartge^C
root@labs--311032102:/home/project/monolith# date
Mon Jun 20 05:30:01 UTC 2022
root@labs--311032102:/home/project/monolith# cd target
root@labs--311032102:/home/project/monolith/target# ls
classes  generated-sources  maven-archiver  maven-status  monolith-0.0.1.BUILD-SNAPSHOT.jar  monolith-0.0.1.BUILD-SNAPSHOT.jar.original
root@labs--311032102:/home/project/monolith/target# ls -lrt
total 40508
drwxr-xr-x 3 root root     4096 Jun 20 05:28 maven-status
drwxr-xr-x 3 root root     4096 Jun 20 05:28 generated-sources
drwxr-xr-x 4 root root     4096 Jun 20 05:28 classes
drwxr-xr-x 2 root root     4096 Jun 20 05:28 maven-archiver
-rw-r--r-- 1 root root   356342 Jun 20 05:28 monolith-0.0.1.BUILD-SNAPSHOT.jar.original
-rw-r--r-- 1 root root 41103784 Jun 20 05:28 monolith-0.0.1.BUILD-SNAPSHOT.jar
root@labs--311032102:/home/project/monolith/target# 
root@labs--311032102:/home/project/monolith/target# 
root@labs--311032102:/home/project/monolith/target# 
root@labs--311032102:/home/project/monolith/target# 
root@labs--311032102:/home/project/monolith/target# cd ..
root@labs--311032102:/home/project/monolith# ls
Dockerfile  Dockerfile-prod  DockerfileAutoBuild  README.md  azure  cloudbuild.yaml  kubernetes  monolith.iml  pom.xml  src  target
root@labs--311032102:/home/project/monolith# docker build -t 979050235289.dkr.ecr.ap-northeast-1.amazonaws.com/user02_ecr:v1 .
Sending build context to Docker daemon 42.77 MB
Step 1/4 : FROM ghcr.io/gkedu/openjdk:8u212-jdk-alpine
8u212-jdk-alpine: Pulling from gkedu/openjdk
Digest: sha256:44b3cea369c947527e266275cee85c71a81f20fc5076f6ebb5a13f19015dce71
Status: Downloaded newer image for ghcr.io/gkedu/openjdk:8u212-jdk-alpine
 ---> a3562aa0b991
Step 2/4 : COPY target/*SNAPSHOT.jar app.jar
 ---> 6369d0f7b8bc
Step 3/4 : EXPOSE 8080
 ---> Running in 823269eb2dbf
Removing intermediate container 823269eb2dbf
 ---> f3ab877945ff
Step 4/4 : ENTRYPOINT ["java","-Xmx400M","-Djava.security.egd=file:/dev/./urandom","-jar","/app.jar","--spring.profiles.active=docker"]
 ---> Running in 47dbc1b15da7
Removing intermediate container 47dbc1b15da7
 ---> 089f99198c1c
Successfully built 089f99198c1c
Successfully tagged 979050235289.dkr.ecr.ap-northeast-1.amazonaws.com/user02_ecr:v1



docker build -t 979050235289.dkr.ecr.ap-northeast-1.amazonaws.com/user02_ecr:v1 .

docker push 979050235289.dkr.ecr.ap-northeast-1.amazonaws.com/user02_ecr:v1

-->
docker push 979050235289.dkr.ecr.ap-northeast-1.amazonaws.com/user02-order:v1

root@labs--311032102:/home/project/monolith# docker push 979050235289.dkr.ecr.ap-northeast-1.amazonaws.com/user02_ecr:v1
The push refers to repository [979050235289.dkr.ecr.ap-northeast-1.amazonaws.com/user02_ecr]
39dc5e3d9b32: Pushed 
ceaf9e1ebef5: Pushed 
9b9b7f3d56a0: Pushed 
f1b5933fe4b5: Pushed 
v1: digest: sha256:933d853fce37adb7f48f19409f99fd02284ef9a73b9ce6fd62cd47a61589750c size: 1159

==> 이미지에서 user02_ecr 에서 v1 버전을 확인함 (아마존에 v1 이미지 올림)
==> 이미지에서 user02-order 에서 v1 버전을 확인함 (아마존에 v1 이미지 올림)


source <(kubectl completion bash)
complete -F __start_kubectl kubectl

root@labs--311032102:/home/project/monolith# source<(kubectl completion bash)
bash: source/dev/fd/63: No such file or directory
root@labs--311032102:/home/project/monolith# source <(kubectl completion bash)
root@labs--311032102:/home/project/monolith# complete -F __start_kubectl kubectl

## 설정파일 적용하기 
source ~/.bashrc

root@labs--311032102:/home/project/monolith# source<(kubectl completion bash)
bash: source/dev/fd/63: No such file or directory
root@labs--311032102:/home/project/monolith# source <(kubectl completion bash)
root@labs--311032102:/home/project/monolith# complete -F __start_kubectl kubectl
root@labs--311032102:/home/project/monolith# kubectl get all
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.100.0.1   <none>        443/TCP   98m
root@labs--311032102:/home/project/monolith# 

## 
--979050235289.dkr.ecr.ap-northeast-1.amazonaws.com/user02_ecr:v1

-- 오류발생 (user02_ecr 안되서 싸이트에서 user02-order 로 생성하고 리빌드 다시해야함.)
kubectl create deploy order --image=979050235289.dkr.ecr.ap-northeast-1.amazonaws.com/user02_ecr:v1
--
kubectl create deploy order --image=979050235289.dkr.ecr.ap-northeast-1.amazonaws.com/user02-order:v1
kubectl create deploy user02-order --image=979050235289.dkr.ecr.ap-northeast-1.amazonaws.com/user02-order:v1

v1ot@labs--311032102:/home/project/monolith# kubectl create deploy order --image=979050235289.dkr.ecr.ap-northeast-1.amazonaws.com/user02-order:v
deployment.apps/order created

deployment.apps/user02-order created
root@labs--311032102:/home/project/monolith# kubectl get all^C
root@labs--311032102:/home/project/monolith# kubectl get all
NAME                                READY   STATUS    RESTARTS   AGE
pod/order-6d7c686964-s5vfm          1/1     Running   0          82s
pod/user02-order-5fd6ff986d-4q9q6   1/1     Running   0          10s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.100.0.1   <none>        443/TCP   108m

NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/order          1/1     1            1           82s
deployment.apps/user02-order   1/1     1            1           10s

NAME                                      DESIRED   CURRENT   READY   AGE
replicaset.apps/order-6d7c686964          1         1         1       82s
replicaset.apps/user02-order-5fd6ff986d   1         1         1       10s
root@labs--311032102:/home/project/monolith# kubectlget po
bash: kubectlget: command not found
root@labs--311032102:/home/project/monolith# kubectl get po
NAME                            READY   STATUS    RESTARTS   AGE
order-6d7c686964-s5vfm          1/1     Running   0          112s
user02-order-5fd6ff986d-4q9q6   1/1     Running   0          40s

root@labs--311032102:/home/project/monolith# kubectl get po
NAME                            READY   STATUS    RESTARTS   AGE
order-6d7c686964-s5vfm          1/1     Running   0          4m1s
user02-order-5fd6ff986d-4q9q6   1/1     Running   0          2m49s
root@labs--311032102:/home/project/monolith# kubectl get pod
NAME                            READY   STATUS    RESTARTS   AGE
order-6d7c686964-s5vfm          1/1     Running   0          4m5s
user02-order-5fd6ff986d-4q9q6   1/1     Running   0          2m53s
root@labs--311032102:/home/project/monolith# kubectl get pod -o wide
NAME                            READY   STATUS    RESTARTS   AGE     IP               NODE                                               NOMINATED NODE   READINESS GATES
order-6d7c686964-s5vfm          1/1     Running   0          4m15s   192.168.9.201    ip-192-168-29-79.ap-northeast-1.compute.internal   <none>           <none>
user02-order-5fd6ff986d-4q9q6   1/1     Running   0          3m3s    192.168.75.188   ip-192-168-91-26.ap-northeast-1.compute.internal   <none>           <none>
root@labs--311032102:/home/project/monolith#

## 로그를 확인한다.
kubectl logs -f user02-order-5fd6ff986d-4q9q6

## 가상환경에 접속해서 확인한다.
root@labs--311032102:/home/project/monolith# kubectl exec -it user02-order-5fd6ff986d-4q9q6 -- /bin/sh

root@labs--311032102:/home/project/monolith# kubectl exec -it user02-order-5fd6ff986d-4q9q6 -- /bin/sh
/ # ls
app.jar  dev      home     media    opt      root     sbin     sys      usr
bin      etc      lib      mnt      proc     run      srv      tmp      var
/ # ls -lrt
total 40152
drwxr-xr-x    1 root     root            19 May  9  2019 var
drwxr-xr-x    2 root     root             6 May  9  2019 srv
drwxr-xr-x    2 root     root             6 May  9  2019 opt
drwxr-xr-x    2 root     root             6 May  9  2019 mnt
drwxr-xr-x    5 root     root            44 May  9  2019 media
drwxr-xr-x    2 root     root             6 May  9  2019 home
drwxr-xr-x    2 root     root          4096 May  9  2019 sbin
drwxr-xr-x    2 root     root          4096 May  9  2019 bin
drwxr-xr-x    1 root     root            81 May 11  2019 usr
drwxr-xr-x    1 root     root           132 May 11  2019 lib
-rw-r--r--    1 root     root      41103784 Jun 20 05:28 app.jar
dr-xr-xr-x   13 root     root             0 Jun 20 06:20 sys
drwxr-xr-x    1 root     root            21 Jun 20 06:20 run
dr-xr-xr-x  167 root     root             0 Jun 20 06:20 proc
drwxr-xr-x    1 root     root            66 Jun 20 06:20 etc
drwxr-xr-x    5 root     root           360 Jun 20 06:20 dev
drwxrwxrwt    1 root     root           114 Jun 20 06:21 tmp
drwx------    1 root     root            26 Jun 20 06:32 root
/ # tar -tvf all^C
/ # tar -tvf app.tar
tar: can't open 'app.tar': No such file or directory
/ # ^C
/ # ls
app.jar  dev      home     media    opt      root     sbin     sys      usr
bin      etc      lib      mnt      proc     run      srv      tmp      var
/ # ls^C
/ # tar -tvf app.jar
tar: invalid tar magic

exit

##
root@labs--311032102:/home/project/monolith# kubectl version --short
Client Version: v1.17.3
Server Version: v1.19.16-eks-de875a99

## kubectl Client  버전 맞추는 방법
kubectl client download 조회 
curl -LO https://dl.k8s.io/release/v1.19.0/bin/linux/amd64/kubectl

root@labs--311032102:/home/project/monolith# kubectl version --short
Client Version: v1.17.3
Server Version: v1.19.16-eks-de875a99
root@labs--311032102:/home/project/monolith# cd curl -LO https://dl.k8s.io/release/v1.19.0/bin/linux/amd64/kubectl
bash: cd: too many arguments
root@labs--311032102:/home/project/monolith#  curl -LO https://dl.k8s.io/release/v1.19.0/bin/linux/amd64/kubectl
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   154  100   154    0     0    865      0 --:--:-- --:--:-- --:--:--   865
100 41.0M  100 41.0M    0     0  59.0M      0 --:--:-- --:--:-- --:--:-- 59.0M
root@labs--311032102:/home/project/monolith# 
root@labs--311032102:/home/project/monolith# 
root@labs--311032102:/home/project/monolith# cd ..
root@labs--311032102:/home/project# ls
Dockerfile           Kafka-Scaling       advanced-connect  cna-pubsub2    default-env       dp-graphql  kafka-base      oauth2
Kafka-Manual-Commit  Keycloak-OAuth2-1   circuitbreaker    cna-start      dp-composite-svc  gateway     monolith        ops-deploy-my-app
Kafka-Retry-DLQ      Oauth2withKeycloak  cna-pubsub        contract-test  dp-cqrs           index.html  monolith2misvc  shopping
root@labs--311032102:/home/project# ls -la kubectl
ls: cannot access 'kubectl': No such file or directory
root@labs--311032102:/home/project# ^C
root@labs--311032102:/home/project# cd ..
root@labs--311032102:/home# cd project
root@labs--311032102:/home/project# ls
Dockerfile           Kafka-Scaling       advanced-connect  cna-pubsub2    default-env       dp-graphql  kafka-base      oauth2
Kafka-Manual-Commit  Keycloak-OAuth2-1   circuitbreaker    cna-start      dp-composite-svc  gateway     monolith        ops-deploy-my-app
Kafka-Retry-DLQ      Oauth2withKeycloak  cna-pubsub        contract-test  dp-cqrs           index.html  monolith2misvc  shopping
root@labs--311032102:/home/project# cd monolith
root@labs--311032102:/home/project/monolith# ls
Dockerfile  Dockerfile-prod  DockerfileAutoBuild  README.md  azure  cloudbuild.yaml  kubectl  kubernetes  monolith.iml  pom.xml  src  target
root@labs--311032102:/home/project/monolith# ls -lrt
total 42048
-rw-r--r-- 1 root root      215 Jun 20 05:25 Dockerfile
-rw-r--r-- 1 root root      320 Jun 20 05:25 Dockerfile-prod
-rw-r--r-- 1 root root      343 Jun 20 05:25 DockerfileAutoBuild
-rw-r--r-- 1 root root      633 Jun 20 05:25 README.md
drwxr-xr-x 2 root root     4096 Jun 20 05:25 azure
-rw-r--r-- 1 root root     2239 Jun 20 05:25 cloudbuild.yaml
drwxr-xr-x 2 root root     4096 Jun 20 05:25 kubernetes
-rw-r--r-- 1 root root     9260 Jun 20 05:25 monolith.iml
-rw-r--r-- 1 root root     2650 Jun 20 05:25 pom.xml
drwxr-xr-x 3 root root     4096 Jun 20 05:25 src
drwxr-xr-x 7 root root     4096 Jun 20 05:48 target
-rw-r--r-- 1 root root 43003904 Jun 20 06:45 kubectl
root@labs--311032102:/home/project/monolith# cp kubectl ^C
root@labs--311032102:/home/project/monolith# which kubectl
/usr/bin/kubectl
root@labs--311032102:/home/project/monolith# cp ./curl -LO https://dl.k8s.io/release/v1.19.0/bin/linux/amd64/kubectl^C
root@labs--311032102:/home/project/monolith# co ./kubectl /usr/bin/kubectl
bash: co: command not found
root@labs--311032102:/home/project/monolith# cp ./kubectl /usr/bin/kubectl
root@labs--311032102:/home/project/monolith# 
root@labs--311032102:/home/project/monolith# 
root@labs--311032102:/home/project/monolith# 
root@labs--311032102:/home/project/monolith# kubectl version --short
Client Version: v1.19.0
Server Version: v1.19.16-eks-de875a99

#
root@labs--311032102:/home/project/monolith# kubectl -h
kubectl controls the Kubernetes cluster manager.

 Find more information at: https://kubernetes.io/docs/reference/kubectl/overview/

Basic Commands (Beginner):
  create        Create a resource from a file or from stdin.
  expose        Take a replication controller, service, deployment or pod and expose it as a new Kubernetes Service
  run           Run a particular image on the cluster
  set           Set specific features on objects

Basic Commands (Intermediate):
  explain       Documentation of resources
  get           Display one or many resources
  edit          Edit a resource on the server
  delete        Delete resources by filenames, stdin, resources and names, or by resources and label selector

Deploy Commands:
  rollout       Manage the rollout of a resource
  scale         Set a new size for a Deployment, ReplicaSet or Replication Controller
  autoscale     Auto-scale a Deployment, ReplicaSet, or ReplicationController

Cluster Management Commands:
  certificate   Modify certificate resources.
  cluster-info  Display cluster info
  top           Display Resource (CPU/Memory/Storage) usage.
  cordon        Mark node as unschedulable
  uncordon      Mark node as schedulable
  drain         Drain node in preparation for maintenance
  taint         Update the taints on one or more nodes

Troubleshooting and Debugging Commands:
  describe      Show details of a specific resource or group of resources
  logs          Print the logs for a container in a pod
  attach        Attach to a running container
  exec          Execute a command in a container
  port-forward  Forward one or more local ports to a pod
  proxy         Run a proxy to the Kubernetes API server
  cp            Copy files and directories to and from containers.
  auth          Inspect authorization

Advanced Commands:
  diff          Diff live version against would-be applied version
  apply         Apply a configuration to a resource by filename or stdin
  patch         Update field(s) of a resource using strategic merge patch
  replace       Replace a resource by filename or stdin
  wait          Experimental: Wait for a specific condition on one or many resources.
  convert       Convert config files between different API versions
  kustomize     Build a kustomization target from a directory or a remote url.

Settings Commands:
  label         Update the labels on a resource
  annotate      Update the annotations on a resource
  completion    Output shell completion code for the specified shell (bash or zsh)

Other Commands:
  alpha         Commands for features in alpha
  api-resources Print the supported API resources on the server
  api-versions  Print the supported API versions on the server, in the form of "group/version"
  config        Modify kubeconfig files
  plugin        Provides utilities for interacting with plugins.
  version       Print the client and server version information

Usage:
  kubectl [flags] [options]

Use "kubectl <command> --help" for more information about a given command.
Use "kubectl options" for a list of global command-line options (applies to all commands).

root@labs--311032102:/home/project/monolith# kubectl create -h
Create a resource from a file or from stdin.

 JSON and YAML formats are accepted.

Examples:
  # Create a pod using the data in pod.json.
  kubectl create -f ./pod.json
  
  # Create a pod based on the JSON passed into stdin.
  cat pod.json | kubectl create -f -
  
  # Edit the data in docker-registry.yaml in JSON then create the resource using the edited data.
  kubectl create -f docker-registry.yaml --edit -o json

Available Commands:
  clusterrole         Create a ClusterRole.
  clusterrolebinding  Create a ClusterRoleBinding for a particular ClusterRole
  configmap           Create a configmap from a local file, directory or literal value
  cronjob             Create a cronjob with the specified name.
  deployment          Create a deployment with the specified name.
  job                 Create a job with the specified name.
  namespace           Create a namespace with the specified name
  poddisruptionbudget Create a pod disruption budget with the specified name.
  priorityclass       Create a priorityclass with the specified name.
  quota               Create a quota with the specified name.
  role                Create a role with single rule.
  rolebinding         Create a RoleBinding for a particular Role or ClusterRole
  secret              Create a secret using specified subcommand
  service             Create a service using specified subcommand.
  serviceaccount      Create a service account with the specified name

Options:
      --allow-missing-template-keys=true: If true, ignore any errors in templates when a field or map key is missing in
the template. Only applies to golang and jsonpath output formats.
      --dry-run='none': Must be "none", "server", or "client". If client strategy, only print the object that would be
sent, without sending it. If server strategy, submit server-side request without persisting the resource.
      --edit=false: Edit the API resource before creating
      --field-manager='kubectl-create': Name of the manager used to track field ownership.
  -f, --filename=[]: Filename, directory, or URL to files to use to create the resource
  -k, --kustomize='': Process the kustomization directory. This flag can't be used together with -f or -R.
  -o, --output='': Output format. One of:
json|yaml|name|go-template|go-template-file|template|templatefile|jsonpath|jsonpath-as-json|jsonpath-file.
      --raw='': Raw URI to POST to the server.  Uses the transport specified by the kubeconfig file.
      --record=false: Record current kubectl command in the resource annotation. If set to false, do not record the
command. If set to true, record the command. If not set, default to updating the existing annotation value only if one
already exists.
  -R, --recursive=false: Process the directory used in -f, --filename recursively. Useful when you want to manage
related manifests organized within the same directory.
      --save-config=false: If true, the configuration of current object will be saved in its annotation. Otherwise, the
annotation will be unchanged. This flag is useful when you want to perform kubectl apply on this object in the future.
  -l, --selector='': Selector (label query) to filter on, supports '=', '==', and '!='.(e.g. -l key1=value1,key2=value2)
      --template='': Template string or path to template file to use when -o=go-template, -o=go-template-file. The
template format is golang templates [http://golang.org/pkg/text/template/#pkg-overview].
      --validate=true: If true, use a schema to validate the input before sending it
      --windows-line-endings=false: Only relevant if --edit=true. Defaults to the line ending native to your platform.

Usage:
  kubectl create -f FILENAME [options]

Use "kubectl <command> --help" for more information about a given command.
Use "kubectl options" for a list of global command-line options (applies to all commands).

root@labs--311032102:/home/project/monolith# kubectl create deployment -h
Create a deployment with the specified name.

Aliases:
deployment, deploy

Examples:
  # Create a deployment named my-dep that runs the busybox image.
  kubectl create deployment my-dep --image=busybox
  
  # Create a deployment with command
  kubectl create deployment my-dep --image=busybox -- date
  
  # Create a deployment named my-dep that runs the nginx image with 3 replicas.
  kubectl create deployment my-dep --image=nginx --replicas=3
  
  # Create a deployment named my-dep that runs the busybox image and expose port 5701.
  kubectl create deployment my-dep --image=busybox --port=5701

Options:
      --allow-missing-template-keys=true: If true, ignore any errors in templates when a field or map key is missing in
the template. Only applies to golang and jsonpath output formats.
      --dry-run='none': Must be "none", "server", or "client". If client strategy, only print the object that would be
sent, without sending it. If server strategy, submit server-side request without persisting the resource.
      --field-manager='kubectl-create': Name of the manager used to track field ownership.
      --image=[]: Image names to run.
  -o, --output='': Output format. One of:
json|yaml|name|go-template|go-template-file|template|templatefile|jsonpath|jsonpath-as-json|jsonpath-file.
      --port=-1: The port that this container exposes.
  -r, --replicas=1: Number of replicas to create. Default is 1.
      --save-config=false: If true, the configuration of current object will be saved in its annotation. Otherwise, the
annotation will be unchanged. This flag is useful when you want to perform kubectl apply on this object in the future.
      --template='': Template string or path to template file to use when -o=go-template, -o=go-template-file. The
template format is golang templates [http://golang.org/pkg/text/template/#pkg-overview].
      --validate=true: If true, use a schema to validate the input before sending it

Usage:
  kubectl create deployment NAME --image=image -- [COMMAND] [args...] [options]

Use "kubectl options" for a list of global command-line options (applies to all commands).

##
root@labs--311032102:/home/project/monolith# kubectl create deployment nginx-gorapa --image=nginx
  : 마아존 등록된 싸이트에서 다운,  버전은 latest 

root@labs--311032102:/home/project/monolith# watch kubectl get all

root@labs--311032102:/home/project/monolith# kubectl create deployment nginx-gorapa-2 --image=nginx --replicas 3
deployment.apps/nginx-gorapa-2 created
root@labs--311032102:/home/project/monolith# kubectl get all
NAME                                  READY   STATUS              RESTARTS   AGE
pod/nginx-gorapa-2-8559b5bc76-lqh7x   1/1     Running             0          8s
pod/nginx-gorapa-2-8559b5bc76-mz5hn   0/1     ContainerCreating   0          8s
pod/nginx-gorapa-2-8559b5bc76-sbwq9   0/1     ContainerCreating   0          8s
pod/nginx-gorapa-7878965fcc-x2lmj     1/1     Running             0          2m51s
pod/order-6d7c686964-s5vfm            1/1     Running             0          44m
pod/user02-order-5fd6ff986d-4q9q6     1/1     Running             0          43m

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.100.0.1   <none>        443/TCP   151m

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx-gorapa     1/1     1            1           2m52s
deployment.apps/nginx-gorapa-2   1/3     3            1           8s
deployment.apps/order            1/1     1            1           44m
deployment.apps/user02-order     1/1     1            1           43m

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-gorapa-2-8559b5bc76   3         3         1       8s
replicaset.apps/nginx-gorapa-7878965fcc     1         1         1       2m52s
replicaset.apps/order-6d7c686964            1         1         1       44m
replicaset.apps/user02-order-5fd6ff986d     1         1         1       43m



root@labs--311032102:/home/project/monolith# kubectl delete pod  nginx-gorapa-2-8559b5bc76-lqh7x
pod "nginx-gorapa-2-8559b5bc76-lqh7x" deleted
root@labs--311032102:/home/project/monolith# 
root@labs--311032102:/home/project/monolith# kubectl get all
NAME                                  READY   STATUS    RESTARTS   AGE
pod/nginx-gorapa-2-8559b5bc76-8xgc8   1/1     Running   0          6s
pod/nginx-gorapa-2-8559b5bc76-mz5hn   1/1     Running   0          2m54s
pod/nginx-gorapa-2-8559b5bc76-sbwq9   1/1     Running   0          2m54s
pod/nginx-gorapa-7878965fcc-x2lmj     1/1     Running   0          5m37s
pod/order-6d7c686964-s5vfm            1/1     Running   0          47m
pod/user02-order-5fd6ff986d-4q9q6     1/1     Running   0          46m

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.100.0.1   <none>        443/TCP   154m

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx-gorapa     1/1     1            1           5m38s
deployment.apps/nginx-gorapa-2   3/3     3            3           2m54s
deployment.apps/order            1/1     1            1           47m
deployment.apps/user02-order     1/1     1            1           46m

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-gorapa-2-8559b5bc76   3         3         3       2m54s
replicaset.apps/nginx-gorapa-7878965fcc     1         1         1       5m38s
replicaset.apps/order-6d7c686964            1         1         1       47m
replicaset.apps/user02-order-5fd6ff986d     1         1         1       46m
root@labs--311032102:/home/project/monolith# kubectl delete pod  nginx-gorapa-2-8559b5bc76-lqh7x
Error from server (NotFound): pods "nginx-gorapa-2-8559b5bc76-lqh7x" not found
root@labs--311032102:/home/project/monolith# kubectl delete pod  nginx-gorapa-2-8559b5bc76-mz5hn
pod "nginx-gorapa-2-8559b5bc76-mz5hn" deleted
^[[A^[[A
root@labs--311032102:/home/project/monolith# kubectl delete pod  nginx-gorapa-2-8559b5bc76-lqh7x
^[[A^[[AError from server (NotFound): pods "nginx-gorapa-2-8559b5bc76-lqh7x" not found
root@labs--311032102:/home/project/monolith# kubectl get all
NAME                                  READY   STATUS    RESTARTS   AGE
pod/nginx-gorapa-2-8559b5bc76-8xgc8   1/1     Running   0          45s
pod/nginx-gorapa-2-8559b5bc76-mzblw   1/1     Running   0          19s
pod/nginx-gorapa-2-8559b5bc76-sbwq9   1/1     Running   0          3m33s
pod/nginx-gorapa-7878965fcc-x2lmj     1/1     Running   0          6m16s
pod/order-6d7c686964-s5vfm            1/1     Running   0          47m
pod/user02-order-5fd6ff986d-4q9q6     1/1     Running   0          46m

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.100.0.1   <none>        443/TCP   155m

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx-gorapa     1/1     1            1           6m17s
deployment.apps/nginx-gorapa-2   3/3     3            3           3m33s
deployment.apps/order            1/1     1            1           47m
deployment.apps/user02-order     1/1     1            1           46m

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-gorapa-2-8559b5bc76   3         3         3       3m33s
replicaset.apps/nginx-gorapa-7878965fcc     1         1         1       6m17s
replicaset.apps/order-6d7c686964            1         1         1       47m
replicaset.apps/user02-order-5fd6ff986d     1         1         1       46m
root@labs--311032102:/home/project/monolith# kubectl delete deployment.apps nginx-gorapa2
Error from server (NotFound): deployments.apps "nginx-gorapa2" not found
root@labs--311032102:/home/project/monolith# kubectl delete deployment.apps nginx-gorapa
deployment.apps "nginx-gorapa" deleted
root@labs--311032102:/home/project/monolith# kubectl get all
NAME                                  READY   STATUS    RESTARTS   AGE
pod/nginx-gorapa-2-8559b5bc76-8xgc8   1/1     Running   0          99s
pod/nginx-gorapa-2-8559b5bc76-mzblw   1/1     Running   0          73s
pod/nginx-gorapa-2-8559b5bc76-sbwq9   1/1     Running   0          4m27s
pod/order-6d7c686964-s5vfm            1/1     Running   0          48m
pod/user02-order-5fd6ff986d-4q9q6     1/1     Running   0          47m

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.100.0.1   <none>        443/TCP   156m

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx-gorapa-2   3/3     3            3           4m28s
deployment.apps/order            1/1     1            1           48m
deployment.apps/user02-order     1/1     1            1           47m

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-gorapa-2-8559b5bc76   3         3         3       4m28s
replicaset.apps/order-6d7c686964            1         1         1       48m
replicaset.apps/user02-order-5fd6ff986d     1         1         1       47m
root@labs--311032102:/home/project/monolith# kubectl get all

##

root@labs--311032102:/home/project/monolith# kubectl create deployment nginx --image=nginx --replicas=3 --dry-run=client -o yaml > nginxdep.yaml 
root@labs--311032102:/home/project/monolith# ls -lrt
total 42052
-rw-r--r-- 1 root root      215 Jun 20 05:25 Dockerfile
-rw-r--r-- 1 root root      320 Jun 20 05:25 Dockerfile-prod
-rw-r--r-- 1 root root      343 Jun 20 05:25 DockerfileAutoBuild
-rw-r--r-- 1 root root      633 Jun 20 05:25 README.md
drwxr-xr-x 2 root root     4096 Jun 20 05:25 azure
-rw-r--r-- 1 root root     2239 Jun 20 05:25 cloudbuild.yaml
drwxr-xr-x 2 root root     4096 Jun 20 05:25 kubernetes
-rw-r--r-- 1 root root     9260 Jun 20 05:25 monolith.iml
-rw-r--r-- 1 root root     2650 Jun 20 05:25 pom.xml
drwxr-xr-x 3 root root     4096 Jun 20 05:25 src
drwxr-xr-x 7 root root     4096 Jun 20 05:48 target
-rw-r--r-- 1 root root 43003904 Jun 20 06:45 kubectl
-rw-r--r-- 1 root root      384 Jun 20 07:10 nginxdep.yaml
root@labs--311032102:/home/project/monolith# vi n*.y*l
root@labs--311032102:/home/project/monolith# 
root@labs--311032102:/home/project/monolith# 
root@labs--311032102:/home/project/monolith# set -o vi
root@labs--311032102:/home/project/monolith# kubectl create deployment nginx --image=nginx --replicas=3 --dry-run=client -o yaml > nginxdep.yaml

root@labs--311032102:/home/project/monolith# cat nginxdep.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: nginx
  name: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}

#
root@labs--311032102:/home/project/monolith# kubectl create -f nginxdep.yaml
deployment.apps/nginx created
root@labs--311032102:/home/project/monolith# kubectl get all
NAME                                  READY   STATUS    RESTARTS   AGE
pod/nginx-6799fc88d8-4b9wl            1/1     Running   0          16s
pod/nginx-6799fc88d8-bh6d7            1/1     Running   0          16s
pod/nginx-6799fc88d8-xhj5j            1/1     Running   0          16s
pod/nginx-gorapa-2-8559b5bc76-8xgc8   1/1     Running   0          5m30s
pod/nginx-gorapa-2-8559b5bc76-mzblw   1/1     Running   0          5m4s
pod/nginx-gorapa-2-8559b5bc76-sbwq9   1/1     Running   0          8m18s
pod/order-6d7c686964-s5vfm            1/1     Running   0          52m
pod/user02-order-5fd6ff986d-4q9q6     1/1     Running   0          51m

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.100.0.1   <none>        443/TCP   159m

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx            3/3     3            3           17s
deployment.apps/nginx-gorapa-2   3/3     3            3           8m18s
deployment.apps/order            1/1     1            1           52m
deployment.apps/user02-order     1/1     1            1           51m

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-6799fc88d8            3         3         3       16s
replicaset.apps/nginx-gorapa-2-8559b5bc76   3         3         3       8m18s
replicaset.apps/order-6d7c686964            1         1         1       52m
replicaset.apps/user02-order-5fd6ff986d     1         1         1       51m
root@labs--311032102:/home/project/monolith# 

## 

replicaset.apps/user02-order-5fd6ff986d     1         1         1       51m
root@labs--311032102:/home/project/monolith# kubectl explain deployment.spec
KIND:     Deployment
VERSION:  apps/v1

RESOURCE: spec <Object>

DESCRIPTION:
     Specification of the desired behavior of the Deployment.

     DeploymentSpec is the specification of the desired behavior of the
     Deployment.

FIELDS:
   minReadySeconds      <integer>
     Minimum number of seconds for which a newly created pod should be ready
     without any of its container crashing, for it to be considered available.
     Defaults to 0 (pod will be considered available as soon as it is ready)

   paused       <boolean>
     Indicates that the deployment is paused.

   progressDeadlineSeconds      <integer>
     The maximum time in seconds for a deployment to make progress before it is
     considered to be failed. The deployment controller will continue to process
     failed deployments and a condition with a ProgressDeadlineExceeded reason
     will be surfaced in the deployment status. Note that progress will not be
     estimated during the time a deployment is paused. Defaults to 600s.

   replicas     <integer>
     Number of desired pods. This is a pointer to distinguish between explicit
     zero and not specified. Defaults to 1.

   revisionHistoryLimit <integer>
     The number of old ReplicaSets to retain to allow rollback. This is a
     pointer to distinguish between explicit zero and not specified. Defaults to
     10.

   selector     <Object> -required-
     Label selector for pods. Existing ReplicaSets whose pods are selected by
     this will be the ones affected by this deployment. It must match the pod
     template's labels.

   strategy     <Object>
     The deployment strategy to use to replace existing pods with new ones.

   template     <Object> -required-
     Template describes the pods that will be created

##
root@labs--311032102:/home/project/monolith# kubectl get all
^[[ANAME                                  READY   STATUS    RESTARTS   AGE
pod/nginx-6799fc88d8-4b9wl            1/1     Running   0          30m
pod/nginx-6799fc88d8-bh6d7            1/1     Running   0          30m
pod/nginx-6799fc88d8-xhj5j            1/1     Running   0          30m
pod/nginx-gorapa-2-8559b5bc76-8xgc8   1/1     Running   0          35m
pod/nginx-gorapa-2-8559b5bc76-mzblw   1/1     Running   0          35m
pod/nginx-gorapa-2-8559b5bc76-sbwq9   1/1     Running   0          38m
pod/order-6d7c686964-s5vfm            1/1     Running   0          82m
pod/user02-order-5fd6ff986d-4q9q6     1/1     Running   0          81m

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.100.0.1   <none>        443/TCP   3h9m

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx            3/3     3            3           30m
deployment.apps/nginx-gorapa-2   3/3     3            3           38m
deployment.apps/order            1/1     1            1           82m
deployment.apps/user02-order     1/1     1            1           81m

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-6799fc88d8            3         3         3       30m
replicaset.apps/nginx-gorapa-2-8559b5bc76   3         3         3       38m
replicaset.apps/order-6d7c686964            1         1         1       82m
replicaset.apps/user02-order-5fd6ff986d     1         1         1       81m
root@labs--311032102:/home/project/monolith# kubectl describe pod/nginx-6799fc88d8-bh6d7
Name:         nginx-6799fc88d8-bh6d7
Namespace:    default
Priority:     0
Node:         ip-192-168-29-79.ap-northeast-1.compute.internal/192.168.29.79
Start Time:   Mon, 20 Jun 2022 07:12:06 +0000
Labels:       app=nginx
              pod-template-hash=6799fc88d8
Annotations:  kubernetes.io/psp: eks.privileged
Status:       Running
IP:           192.168.3.78
IPs:
  IP:           192.168.3.78
Controlled By:  ReplicaSet/nginx-6799fc88d8
Containers:
  nginx:
    Container ID:   docker://c15555385c79af96d377c1a3f8ddbfebc5d5c9182a00fb116e1f5a75405fc566
    Image:          nginx
    Image ID:       docker-pullable://nginx@sha256:2bcabc23b45489fb0885d69a06ba1d648aeda973fae7bb981bafbb884165e514
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Mon, 20 Jun 2022 07:12:09 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zkb74 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-zkb74:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-zkb74
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From                                                       Message
  ----    ------     ----  ----                                                       -------
  Normal  Scheduled  30m   default-scheduler                                          Successfully assigned default/nginx-6799fc88d8-bh6d7 to ip-192-168-29-79.ap-northeast-1.compute.internal
  Normal  Pulling    30m   kubelet, ip-192-168-29-79.ap-northeast-1.compute.internal  Pulling image "nginx"
  Normal  Pulled     30m   kubelet, ip-192-168-29-79.ap-northeast-1.compute.internal  Successfully pulled image "nginx" in 1.842087721s
  Normal  Created    30m   kubelet, ip-192-168-29-79.ap-northeast-1.compute.internal  Created container nginx
  Normal  Started    30m   kubelet, ip-192-168-29-79.ap-northeast-1.compute.internal  Started container nginx
root@labs--311032102:/home/project/monolith# kubectl -f logs pod/nginx-6799fc88d8-bh6d7
Error: unknown command "pod/nginx-6799fc88d8-bh6d7" for "kubectl"
Run 'kubectl --help' for usage.
root@labs--311032102:/home/project/monolith# kubectl logs pod/nginx-6799fc88d8-bh6d7
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
/docker-entrypoint.sh: Configuration complete; ready for start up
2022/06/20 07:12:09 [notice] 1#1: using the "epoll" event method
2022/06/20 07:12:09 [notice] 1#1: nginx/1.21.6
2022/06/20 07:12:09 [notice] 1#1: built by gcc 10.2.1 20210110 (Debian 10.2.1-6) 
2022/06/20 07:12:09 [notice] 1#1: OS: Linux 5.4.196-108.356.amzn2.x86_64
2022/06/20 07:12:09 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576
2022/06/20 07:12:09 [notice] 1#1: start worker processes
2022/06/20 07:12:09 [notice] 1#1: start worker process 31


root@labs--311032102:/home/project/monolith# kubectl expose deployment.apps/nginx-gorapa-2 --port=8080
service/nginx-gorapa-2 exposed
root@labs--311032102:/home/project/monolith# kubectl get all
NAME                                  READY   STATUS    RESTARTS   AGE
pod/nginx-6799fc88d8-4b9wl            1/1     Running   0          31m
pod/nginx-6799fc88d8-bh6d7            1/1     Running   0          31m
pod/nginx-6799fc88d8-xhj5j            1/1     Running   0          31m
pod/nginx-gorapa-2-8559b5bc76-8xgc8   1/1     Running   0          37m
pod/nginx-gorapa-2-8559b5bc76-mzblw   1/1     Running   0          36m
pod/nginx-gorapa-2-8559b5bc76-sbwq9   1/1     Running   0          39m
pod/order-6d7c686964-s5vfm            1/1     Running   0          84m
pod/user02-order-5fd6ff986d-4q9q6     1/1     Running   0          83m

NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/kubernetes       ClusterIP   10.100.0.1      <none>        443/TCP    3h11m
service/nginx-gorapa-2   ClusterIP   10.100.91.162   <none>        8080/TCP   8s

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx            3/3     3            3           31m
deployment.apps/nginx-gorapa-2   3/3     3            3           39m
deployment.apps/order            1/1     1            1           84m
deployment.apps/user02-order     1/1     1            1           83m

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-6799fc88d8            3         3         3       31m
replicaset.apps/nginx-gorapa-2-8559b5bc76   3         3         3       39m
replicaset.apps/order-6d7c686964            1         1         1       84m
replicaset.apps/user02-order-5fd6ff986d     1         1         1       83m

root@labs--311032102:/home/project/monolith# kubectl run siege --image=zasmin/siege:1.0
pod/siege created
root@labs--311032102:/home/project/monolith# kubectl get all
NAME                                  READY   STATUS              RESTARTS   AGE
pod/nginx-6799fc88d8-4b9wl            1/1     Running             0          37m
pod/nginx-6799fc88d8-bh6d7            1/1     Running             0          37m
pod/nginx-6799fc88d8-xhj5j            1/1     Running             0          37m
pod/nginx-gorapa-2-8559b5bc76-8xgc8   1/1     Running             0          42m
pod/nginx-gorapa-2-8559b5bc76-mzblw   1/1     Running             0          42m
pod/nginx-gorapa-2-8559b5bc76-sbwq9   1/1     Running             0          45m
pod/order-6d7c686964-s5vfm            1/1     Running             0          89m
pod/siege                             0/1     ContainerCreating   0          4s
pod/user02-order-5fd6ff986d-4q9q6     1/1     Running             0          88m

NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/kubernetes       ClusterIP   10.100.0.1      <none>        443/TCP    3h16m
service/nginx-gorapa-2   ClusterIP   10.100.91.162   <none>        8080/TCP   5m28s

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx            3/3     3            3           37m
deployment.apps/nginx-gorapa-2   3/3     3            3           45m
deployment.apps/order            1/1     1            1           89m
deployment.apps/user02-order     1/1     1            1           88m

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-6799fc88d8            3         3         3       37m
replicaset.apps/nginx-gorapa-2-8559b5bc76   3         3         3       45m
replicaset.apps/order-6d7c686964            1         1         1       89m
replicaset.apps/user02-order-5fd6ff986d     1         1         1       88m
root@labs--311032102:/home/project/monolith# kubectl get all
NAME                                  READY   STATUS              RESTARTS   AGE
pod/nginx-6799fc88d8-4b9wl            1/1     Running             0          37m
pod/nginx-6799fc88d8-bh6d7            1/1     Running             0          37m
pod/nginx-6799fc88d8-xhj5j            1/1     Running             0          37m
pod/nginx-gorapa-2-8559b5bc76-8xgc8   1/1     Running             0          42m
pod/nginx-gorapa-2-8559b5bc76-mzblw   1/1     Running             0          42m
pod/nginx-gorapa-2-8559b5bc76-sbwq9   1/1     Running             0          45m
pod/order-6d7c686964-s5vfm            1/1     Running             0          89m
pod/siege                             0/1     ContainerCreating   0          10s
pod/user02-order-5fd6ff986d-4q9q6     1/1     Running             0          88m

NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/kubernetes       ClusterIP   10.100.0.1      <none>        443/TCP    3h16m
service/nginx-gorapa-2   ClusterIP   10.100.91.162   <none>        8080/TCP   5m34s

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx            3/3     3            3           37m
deployment.apps/nginx-gorapa-2   3/3     3            3           45m
deployment.apps/order            1/1     1            1           89m
deployment.apps/user02-order     1/1     1            1           88m

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-6799fc88d8            3         3         3       37m
replicaset.apps/nginx-gorapa-2-8559b5bc76   3         3         3       45m
replicaset.apps/order-6d7c686964            1         1         1       89m
replicaset.apps/user02-order-5fd6ff986d     1         1         1       88m
root@labs--311032102:/home/project/monolith# kubectl get all
NAME                                  READY   STATUS    RESTARTS   AGE
pod/nginx-6799fc88d8-4b9wl            1/1     Running   0          37m
pod/nginx-6799fc88d8-bh6d7            1/1     Running   0          37m
pod/nginx-6799fc88d8-xhj5j            1/1     Running   0          37m
pod/nginx-gorapa-2-8559b5bc76-8xgc8   1/1     Running   0          42m
pod/nginx-gorapa-2-8559b5bc76-mzblw   1/1     Running   0          42m
pod/nginx-gorapa-2-8559b5bc76-sbwq9   1/1     Running   0          45m
pod/order-6d7c686964-s5vfm            1/1     Running   0          89m
pod/siege                             1/1     Running   0          14s
pod/user02-order-5fd6ff986d-4q9q6     1/1     Running   0          88m

NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/kubernetes       ClusterIP   10.100.0.1      <none>        443/TCP    3h17m
service/nginx-gorapa-2   ClusterIP   10.100.91.162   <none>        8080/TCP   5m38s

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx            3/3     3            3           37m
deployment.apps/nginx-gorapa-2   3/3     3            3           45m
deployment.apps/order            1/1     1            1           89m
deployment.apps/user02-order     1/1     1            1           88m

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-6799fc88d8            3         3         3       37m
replicaset.apps/nginx-gorapa-2-8559b5bc76   3         3         3       45m
replicaset.apps/order-6d7c686964            1         1         1       89m
replicaset.apps/user02-order-5fd6ff986d     1         1         1       88m
root@labs--311032102:/home/project/monolith# kubectl get all
NAME                                  READY   STATUS    RESTARTS   AGE
pod/nginx-6799fc88d8-4b9wl            1/1     Running   0          37m
pod/nginx-6799fc88d8-bh6d7            1/1     Running   0          37m
pod/nginx-6799fc88d8-xhj5j            1/1     Running   0          37m
pod/nginx-gorapa-2-8559b5bc76-8xgc8   1/1     Running   0          42m
pod/nginx-gorapa-2-8559b5bc76-mzblw   1/1     Running   0          42m
pod/nginx-gorapa-2-8559b5bc76-sbwq9   1/1     Running   0          45m
pod/order-6d7c686964-s5vfm            1/1     Running   0          89m
pod/siege                             1/1     Running   0          18s
pod/user02-order-5fd6ff986d-4q9q6     1/1     Running   0          88m

NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/kubernetes       ClusterIP   10.100.0.1      <none>        443/TCP    3h17m
service/nginx-gorapa-2   ClusterIP   10.100.91.162   <none>        8080/TCP   5m42s

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx            3/3     3            3           37m
deployment.apps/nginx-gorapa-2   3/3     3            3           45m
deployment.apps/order            1/1     1            1           89m
deployment.apps/user02-order     1/1     1            1           88m

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-6799fc88d8            3         3         3       37m
replicaset.apps/nginx-gorapa-2-8559b5bc76   3         3         3       45m
replicaset.apps/order-6d7c686964            1         1         1       89m
replicaset.apps/user02-order-5fd6ff986d     1         1         1       88m
root@labs--311032102:/home/project/monolith# kubectl exec -it siege --bash
Error: unknown flag: --bash
See 'kubectl exec --help' for usage.
root@labs--311032102:/home/project/monolith# kubectl exec -it siege -- bash
root@siege:/# 
root@siege:/# 
root@siege:/# http http://localhost:8081

http: error: ConnectionError: HTTPConnectionPool(host='localhost', port=8081): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2cd2678eb8>: Failed to establish a new connection: [Errno 111] Connection refused')) while doing GET request to URL: http://localhost:8081/
root@siege:/# http http://localhost:8081



## 아래싸이트 참조 

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/


## 2일차...  

## 문서참조 
- https://kubernetes.io/docs/home/
- 조회 deployments

1.비지니스앱을 생성함 
2.POD 생성을 위함.

1.  DDD 작성 -> 소스 다운
2. 아래 복사 
 root@labs--311032102:/home/project/user# ls -lrt
total 16
drwxr-xr-x 5 root root 4096 Jun 21 00:45 frontend
drwxr-xr-x 4 root root 4096 Jun 21 00:45 gateway
drwxr-xr-x 2 root root 4096 Jun 21 00:45 kubernetes
drwxr-xr-x 4 root root 4096 Jun 21 00:45 user02
3. cd user02
4. mvn spring-boot:run 
5. 데이타 생성
   /home/project# http :8081/user02s name=kim email="gorapadd@naver.com" 

root@labs--311032102:/home/project# http :8081/user02s name=kim email="gorapadd@naver.com" 
HTTP/1.1 201 
Connection: keep-alive
Content-Type: application/json
Date: Tue, 21 Jun 2022 00:54:38 GMT
Keep-Alive: timeout=60
Location: http://localhost:8081/user02s/1
Transfer-Encoding: chunked
Vary: Origin
Vary: Access-Control-Request-Method
Vary: Access-Control-Request-Headers

{
    "_links": {
        "self": {
            "href": "http://localhost:8081/user02s/1"
        },
        "user02": {
            "href": "http://localhost:8081/user02s/1"
        }
    },
    "email": "gorapadd@naver.com",
    "name": "kim",
    "phone": null
}

6. mvn clean
6. mvn package -Dskiptest    <===  


root@labs--311032102:/home/project/user/user02# ls
Dockerfile  README.md  azure-pipelines.yml  cloudbuild.yaml  kubernetes  pom.xml  src  target



root@labs--311032102:/home/project/user/user02# ls -lrt
total 36
-rw-r--r-- 1 root root 4548 Jun 21 00:45 azure-pipelines.yml
-rw-r--r-- 1 root root 2838 Jun 21 00:45 cloudbuild.yaml
-rw-r--r-- 1 root root  295 Jun 21 00:45 Dockerfile
drwxr-xr-x 2 root root 4096 Jun 21 00:45 kubernetes
-rw-r--r-- 1 root root 3549 Jun 21 00:45 pom.xml
-rw-r--r-- 1 root root 1617 Jun 21 00:45 README.md
drwxr-xr-x 4 root root 4096 Jun 21 00:45 src
drwxr-xr-x 9 root root 4096 Jun 21 00:57 target    <== 생성됨.

root@labs--311032102:/home/project/user/user02# cat Dockerfile
FROM openjdk:15-jdk-alpine
COPY target/*SNAPSHOT.jar app.jar
EXPOSE 8080
ENV TZ=Asia/Seoul
RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone
ENTRYPOINT ["java","-Xmx400M","-Djava.security.egd=file:/dev/./urandom","-jar","/app.jar","--spring.profiles.active=docker"]


root@labs--311032102:/home/project/user/user02# docker build -t gorapadd/userapp:1.0 .
Sending build context to Docker daemon 74.19 MB
Step 1/6 : FROM openjdk:15-jdk-alpine
15-jdk-alpine: Pulling from library/openjdk
df20fa9351a1: Pull complete 
65a2e4aad8c9: Pull complete 
fc5eabcede36: Pull complete 
Digest: sha256:fb60cc0750e6a3e90d2c853413f07dfde53ba3dc3c020b2fa20df0027ca0023a
Status: Downloaded newer image for openjdk:15-jdk-alpine
 ---> f02adfce91a2
Step 2/6 : COPY target/*SNAPSHOT.jar app.jar
 ---> 816631cc5062
Step 3/6 : EXPOSE 8080
 ---> Running in 6ed59040c47c
Removing intermediate container 6ed59040c47c
 ---> 0c3c910c2405
Step 4/6 : ENV TZ=Asia/Seoul
 ---> Running in 396fc8ce3cd7
Removing intermediate container 396fc8ce3cd7
 ---> 51fa6e81ffba
Step 5/6 : RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone
 ---> Running in 931ef2bab9c2
Removing intermediate container 931ef2bab9c2
 ---> 54d703b789fe
Step 6/6 : ENTRYPOINT ["java","-Xmx400M","-Djava.security.egd=file:/dev/./urandom","-jar","/app.jar","--spring.profiles.active=docker"]   --> application.ymd 에서 docker profile을 찾는다.
 ---> Running in 9496a11e7672
Removing intermediate container 9496a11e7672
 ---> f13057de45d7
Successfully built f13057de45d7
Successfully tagged gorapadd/userapp:1.0
root@labs--311032102:/home/project/user/user02# 

## 한번더 
root@labs--311032102:/home/project/user/user02# docker build -t gorapadd/userapp:1.0 .

## push 실행
 - docker push gorapadd/userapp:1.0


## docker push 안되는 경우 도커 로그인 한후 (keb213185!)



root@labs--311032102:~# docker login
Login with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one.
Username: gorapadd
Password: 
Login Succeeded

##상기 로그인해야 .docker 폴더가 생김.

root@labs--311032102:~# cd .docker
root@labs--311032102:~/.docker# ls
config.json
root@labs--311032102:~/.docker# cat *
{
        "auths": {
                "https://index.docker.io/v1/": {
                        "auth": "Z29yYXBhZGQ6a2ViMjEzMTg1IQ=="
                }
        }
}root@labs--311032102:~/.docker# echo "Z29yYXBhZGQ6a2ViMjEzMTg1IQ==" | base64 -d
gorapadd:keb213185!root@labs--311032102:~/.docker# 


## yml 생성 
1. https://kubernetes.io/docs/concepts/workloads/pods/ 싸이트 pod 조회후 처음내용 카피
2. user02_pod.yml 생성후 내용 카피

root@labs--311032102:/home/project/user/user02# vi user02_pod.yml
root@labs--311032102:/home/project/user/user02# cat user02_pod.yml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80

==> 수정
apiVersion: v1
kind: Pod
metadata:
  name: userapp
spec:
  containers:
  - name: userapp
    image: gorapadd:userapp/1
    ports:
    - containerPort: 8080

==> 수정 2 ( labels 가 꼭이어야함.)
root@labs--311032102:/home/project/user/user02# cat user02_pod.yml
cat user02_pod.yml
apiVersion: v1
kind: Pod
metadata:
  name: userapp
  labels:
    app: userapp 
spec:
  containers:
  - name: userapp
    image: gorapadd/userapp:1.0
    ports:
    - containerPort: 8080

root@labs--311032102:/home/project/user/user02# kubectl create -f user02_pod.yml
pod/userapp created

root@labs--311032102:/home/project/user/user02# watch kubectl get pod
Every 2.0s: kubectl get pod                                                                            labs--311032102: Tue Jun 21 02:01:37 2022

NAME                              READY   STATUS             RESTARTS   AGE
nginx-6799fc88d8-4b9wl            1/1     Running            0          18h
nginx-6799fc88d8-bh6d7            1/1     Running            0          18h
nginx-6799fc88d8-xhj5j            1/1     Running            0          18h
nginx-gorapa-2-8559b5bc76-8xgc8   1/1     Running            0          18h
nginx-gorapa-2-8559b5bc76-mzblw   1/1     Running            0          18h
nginx-gorapa-2-8559b5bc76-sbwq9   1/1     Running            0          18h
order-6d7c686964-s5vfm            1/1     Running            0          19h
siege                             1/1     Running            0          18h
user02-order-5fd6ff986d-4q9q6     1/1     Running            0          19h
userapp                           0/1     InvalidImageName   0          44s


root@labs--311032102:/home/project/user/user02# kubectl describe pod userapp     <--확인하는 방법 
Name:         userapp
Namespace:    default
Priority:     0
Node:         ip-192-168-91-26.ap-northeast-1.compute.internal/192.168.91.26
Start Time:   Tue, 21 Jun 2022 02:00:55 +0000
Labels:       <none>
Annotations:  kubernetes.io/psp: eks.privileged
Status:       Pending
IP:           192.168.70.67
IPs:
  IP:  192.168.70.67
Containers:
  userapp:
    Container ID:   
    Image:          gorapadd:userapp/1
    Image ID:       
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       InvalidImageName
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zkb74 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  default-token-zkb74:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-zkb74
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason         Age                From                                                       Message
  ----     ------         ----               ----                                                       -------
  Normal   Scheduled      96s                default-scheduler                                          Successfully assigned default/userapp to ip-192-168-91-26.ap-northeast-1.compute.internal
  Warning  InspectFailed  4s (x10 over 95s)  kubelet, ip-192-168-91-26.ap-northeast-1.compute.internal  Failed to apply default image tag "gorapadd:userapp/1": couldn't parse image reference "gorapadd:userapp/1": invalid reference format
  Warning  Failed         4s (x10 over 95s)  kubelet, ip-192-168-91-26.ap-northeast-1.compute.internal  Error: InvalidImageName



root@labs--311032102:/home/project/user/user02# docker build -t gorapadd/userapp:1.0 .
Sending build context to Docker daemon 74.19 MB
Step 1/6 : FROM openjdk:15-jdk-alpine
 ---> f02adfce91a2
Step 2/6 : COPY target/*SNAPSHOT.jar app.jar
 ---> Using cache
 ---> 816631cc5062
Step 3/6 : EXPOSE 8080
 ---> Using cache
 ---> 0c3c910c2405
Step 4/6 : ENV TZ=Asia/Seoul
 ---> Using cache
 ---> 51fa6e81ffba
Step 5/6 : RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone
 ---> Using cache
 ---> 54d703b789fe
Step 6/6 : ENTRYPOINT ["java","-Xmx400M","-Djava.security.egd=file:/dev/./urandom","-jar","/app.jar","--spring.profiles.active=docker"]
 ---> Using cache
 ---> f13057de45d7
Successfully built f13057de45d7
Successfully tagged gorapadd/userapp:1.0
root@labs--311032102:/home/project/user/user02# docker push gorapadd/userapp:1.0
The push refers to repository [docker.io/gorapadd/userapp]
01ca05d7decf: Layer already exists 
b42b712aa1a5: Layer already exists 
ca35920ce48a: Layer already exists 
a9711b2e31f2: Layer already exists 
50644c29ef5a: Layer already exists 
1.0: digest: sha256:c1c4a78c128d25a92da8101fea78ea0e17b2455c3d0d24b8af2abe4fb7dab267 size: 1370
root@labs--311032102:/home/project/user/user02# kubectl describe pod userapp^C
root@labs--311032102:/home/project/user/user02# ls -lrt
total 40
-rw-r--r-- 1 root root 4548 Jun 21 00:45 azure-pipelines.yml
-rw-r--r-- 1 root root 2838 Jun 21 00:45 cloudbuild.yaml
-rw-r--r-- 1 root root  295 Jun 21 00:45 Dockerfile
drwxr-xr-x 2 root root 4096 Jun 21 00:45 kubernetes
-rw-r--r-- 1 root root 3549 Jun 21 00:45 pom.xml
-rw-r--r-- 1 root root 1617 Jun 21 00:45 README.md
drwxr-xr-x 4 root root 4096 Jun 21 00:45 src
drwxr-xr-x 9 root root 4096 Jun 21 00:57 target
-rw-r--r-- 1 root root  158 Jun 21 02:04 user02_pod.yml
root@labs--311032102:/home/project/user/user02# vi user02_pod.yml
root@labs--311032102:/home/project/user/user02# 
root@labs--311032102:/home/project/user/user02# kubectl create -f user02_pod.yml
Error from server (AlreadyExists): error when creating "user02_pod.yml": pods "userapp" already exists
root@labs--311032102:/home/project/user/user02# docker push gorapadd/userapp:1.0^C
root@labs--311032102:/home/project/user/user02# kubectl delete pod/userapp
pod "userapp" deleted
root@labs--311032102:/home/project/user/user02# kubectl create -f user02_pod.yml
pod/userapp created
root@labs--311032102:/home/project/user/user02# kubectl describe pod userapp   <=== 정상으로 생성된것을 반드시 확인해야함.
Name:         userapp
Namespace:    default
Priority:     0
Node:         ip-192-168-91-26.ap-northeast-1.compute.internal/192.168.91.26
Start Time:   Tue, 21 Jun 2022 02:09:25 +0000
Labels:       <none>
Annotations:  kubernetes.io/psp: eks.privileged
Status:       Pending
IP:           
IPs:          <none>
Containers:
  userapp:
    Container ID:   
    Image:          gorapadd/userapp:1.0
    Image ID:       
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zkb74 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  default-token-zkb74:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-zkb74
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age   From                                                       Message
  ----    ------     ----  ----                                                       -------
  Normal  Scheduled  6s    default-scheduler                                          Successfully assigned default/userapp to ip-192-168-91-26.ap-northeast-1.compute.internal
  Normal  Pulling    5s    kubelet, ip-192-168-91-26.ap-northeast-1.compute.internal  Pulling image "gorapadd/userapp:1.0"
root@labs--311032102:/home/project/user/user02# 

root@labs--311032102:/home/project/user/user02# vi user02_pod.yml
vi user02_pod.yml
root@labs--311032102:/home/project/user/user02# kubectl create -f user02_pod.yml
kubectl create -f user02_pod.yml
pod/userapp created
root@labs--311032102:/home/project/user/user02# vi user02_pod.yml
vi user02_pod.yml
root@labs--311032102:/home/project/user/user02# kubectl create -f user02_pod.yml
kubectl create -f user02_pod.yml
error: error validating "user02_pod.yml": error validating data: ValidationError(Pod.spec.containers[0]): unknown field "labels" in io.k8s.api.core.v1.Container; if you choose to ignore these errors, turn validation off with --validate=false
root@labs--311032102:/home/project/user/user02# vi user02_pod.yml
vi user02_pod.yml
root@labs--311032102:/home/project/user/user02# 

root@labs--311032102:/home/project/user/user02# vi user02_pod.yml
vi user02_pod.yml
root@labs--311032102:/home/project/user/user02# kubectl create -f user02_pod.yml
kubectl create -f user02_pod.yml
Error from server (AlreadyExists): error when creating "user02_pod.yml": pods "userapp" already exists
root@labs--311032102:/home/project/user/user02# kubectl delete pod userapp
kubectl delete pod userapp
pod "userapp" deleted
root@labs--311032102:/home/project/user/user02# kubectl create -f user02_pod.yml
kubectl create -f user02_pod.yml
pod/userapp created
root@labs--311032102:/home/project/user/user02# kubectl expore pod userapp --port=8080
kubectl expore pod userapp --port=8080
Error: unknown command "expore" for "kubectl"

Did you mean this?
        expose

Run 'kubectl --help' for usage.
root@labs--311032102:/home/project/user/user02# kubectl expose pod userapp --port=8080
kubectl expose pod userapp --port=8080
service/userapp exposed
root@labs--311032102:/home/project/user/user02# kubectl get service
kubectl get service
NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
kubernetes       ClusterIP   10.100.0.1       <none>        443/TCP    21h
nginx-gorapa-2   ClusterIP   10.100.91.162    <none>        8080/TCP   18h
userapp          ClusterIP   10.100.170.131   <none>        8080/TCP   15s
root@labs--311032102:/home/project/user/user02# 


root@labs--311032102:/home/project/user/user02# kubectl logs userapp
--> 로그 에러 

##[1] 거래실행해서   ==> 왜안되지???

## seige  로접속해서 거래를 보내야함.

root@labs--311032102:/home/project/user/user02# kubectl run siege --image=zasmin/siege:1.0
kubectl run siege --image=zasmin/siege:1.0
kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
deployment.apps/siege created
root@labs--311032102:/home/project/user/user02# kubectl get all
kubectl get all
NAME                                  READY   STATUS    RESTARTS   AGE
pod/nginx-6799fc88d8-4b9wl            1/1     Running   0          19h
pod/nginx-6799fc88d8-bh6d7            1/1     Running   0          19h
pod/nginx-6799fc88d8-xhj5j            1/1     Running   0          19h
pod/nginx-gorapa-2-8559b5bc76-8xgc8   1/1     Running   0          19h
pod/nginx-gorapa-2-8559b5bc76-mzblw   1/1     Running   0          19h
pod/nginx-gorapa-2-8559b5bc76-sbwq9   1/1     Running   0          19h
pod/order-6d7c686964-s5vfm            1/1     Running   0          20h
pod/siege                             1/1     Running   0          18h
pod/siege-55b7f7d5fc-vwqk6            1/1     Running   0          10s
pod/user02-order-5fd6ff986d-4q9q6     1/1     Running   0          20h

NAME                     TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/kubernetes       ClusterIP   10.100.0.1       <none>        443/TCP    22h
service/nginx-gorapa-2   ClusterIP   10.100.91.162    <none>        8080/TCP   19h
service/userapp          ClusterIP   10.100.170.131   <none>        8080/TCP   24m

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx            3/3     3            3           19h
deployment.apps/nginx-gorapa-2   3/3     3            3           19h
deployment.apps/order            1/1     1            1           20h
deployment.apps/siege            1/1     1            1           10s
deployment.apps/user02-order     1/1     1            1           20h

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-6799fc88d8            3         3         3       19h
replicaset.apps/nginx-gorapa-2-8559b5bc76   3         3         3       19h
replicaset.apps/order-6d7c686964            1         1         1       20h
replicaset.apps/siege-55b7f7d5fc            1         1         1       10s
replicaset.apps/user02-order-5fd6ff986d     1         1         1       20h

## siege접속
root@labs--311032102:/home/project/user/user02# kubectl exec -it siege -- bash

## 접속후 거래 날림.
root@siege:/# http http://userapp:8080/users02s name="dklsjf" email="dslafkjsdfs"

http: error: ConnectionError: HTTPConnectionPool(host='userapp', port=8080): Max retries exceeded with url: /users02s (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1b40d1f198>: Failed to establish a new connection: [Errno 111] Connection refused')) while doing POST request to URL: http://userapp:8080/users02s
root@siege:/# exit
exit
command terminated with exit code 1


다시... ========================================================

root@labs--311032102:/home/project/user/user02# 

root@labs--311032102:/home/project/user/user02# 

root@labs--311032102:/home/project/user/user02# kubectl create -f user02_pod.yml
kubectl create -f user02_pod.yml
pod/userapp created
root@labs--311032102:/home/project/user/user02# kubectl expose pod userapp --port=8080
kubectl expose pod userapp --port=8080
Error from server (AlreadyExists): services "userapp" already exists
root@labs--311032102:/home/project/user/user02# kubectl logs userapp -f

-------------------------------------------------------------------------------------- siege 로 접속해서 http 거래를 날려야함. (쿠버네틱스안에서...)   <---> GPC 환경

root@labs--311032102:/home/project/user/user02# kubectl exec -it siege -- bash
root@siege:/# http http://userapp:8080/users02s name="dklsjf" email="dslafkjsdfs"
HTTP/1.1 404 
Connection: keep-alive
Content-Type: application/json
Date: Tue, 21 Jun 2022 02:46:24 GMT
Keep-Alive: timeout=60
Transfer-Encoding: chunked
Vary: Origin
Vary: Access-Control-Request-Method
Vary: Access-Control-Request-Headers

{
    "error": "Not Found",
    "message": "",
    "path": "/users02s",
    "status": 404,
    "timestamp": "2022-06-21T02:46:24.348+00:00"
}

root@siege:/# http http://userapp:8080/user02s name="dklsjf" email="dslafkjsdfs"
HTTP/1.1 201 
Connection: keep-alive
Content-Type: application/json
Date: Tue, 21 Jun 2022 02:46:31 GMT
Keep-Alive: timeout=60
Location: http://userapp:8080/user02s/1
Transfer-Encoding: chunked
Vary: Origin
Vary: Access-Control-Request-Method
Vary: Access-Control-Request-Headers

{
    "_links": {
        "self": {
            "href": "http://userapp:8080/user02s/1"
        },
        "user02": {
            "href": "http://userapp:8080/user02s/1"
        }
    },
    "email": "dslafkjsdfs",
    "name": "dklsjf",
    "phone": null
}

root@siege:/# http http://userapp:8080/user02s name="dklsjf" email="dslafkjsdfs"


## 로그 확인하는 방법 
root@labs--311032102:/home/project/user/user02# kubectl logs userapp -f


## 네임스페이스 에러가 나는경우
root@labs--311032102:/home/project/user/user02# kubectl get ns
NAME              STATUS   AGE
default           Active   21h
kube-node-lease   Active   21h
kube-public       Active   21h
kube-system       Active   21h

root@labs--311032102:/home/project/user/user02# kubectl config get-contexts
CURRENT   NAME                                                         CLUSTER                                                      AUTHINFO                                                     NAMESPACE
*         arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks   arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks   arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks   
          kcb-test2.k8s.local                                          kcb-test2.k8s.local                                          labs--311032102                                              labs--311032102
          user02@user02-eks.ap-northeast-1.eksctl.io                   user02-eks.ap-northeast-1.eksctl.io                          user02@user02-eks.ap-northeast-1.eksctl.io                   
root@labs--311032102:/home/project/user/user02# 

root@labs--311032102:/home/project/user/user02# kubectl config get-contexts
kubectl config get-contexts
CURRENT   NAME                                                         CLUSTER                                                      AUTHINFO                                                     NAMESPACE
*         arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks   arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks   arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks   
          kcb-test2.k8s.local                                          kcb-test2.k8s.local                                          labs--311032102                                              labs--311032102
          user02@user02-eks.ap-northeast-1.eksctl.io                   user02-eks.ap-northeast-1.eksctl.io                          user02@user02-eks.ap-northeast-1.eksctl.io                   
root@labs--311032102:/home/project/user/user02# kubectl config set-contexts --current --namespace=default
kubectl config set-contexts --current --namespace=default
Error: unknown flag: --current
See 'kubectl config --help' for usage.
root@labs--311032102:/home/project/user/user02# kubectl config set-context --current --namespace=default
kubectl config set-context --current --namespace=default
Context "arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks" modified.
root@labs--311032102:/home/project/user/user02# kubectl config get-contexts
kubectl config get-contexts
CURRENT   NAME                                                         CLUSTER                                                      AUTHINFO                                                     NAMESPACE
*         arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks   arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks   arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks   default
          kcb-test2.k8s.local                                          kcb-test2.k8s.local                                          labs--311032102                                              labs--311032102
          user02@user02-eks.ap-northeast-1.eksctl.io                   user02-eks.ap-northeast-1.eksctl.io                          user02@user02-eks.ap-northeast-1.eksctl.io                   
root@labs--311032102:/home/project/user/user02#



root@labs--311032102:/home/project/user/user02# kubectl config set-context --current --namespace kube-system
kubectl config set-context --current --namespace kube-system
Context "arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks" modified.
root@labs--311032102:/home/project/user/user02# kubectl get all
kubectl get all
NAME                           READY   STATUS    RESTARTS   AGE
pod/aws-node-krj7z             1/1     Running   0          21h
pod/aws-node-mk88p             1/1     Running   0          21h
pod/aws-node-mlgh2             1/1     Running   0          21h
pod/coredns-59847d77c8-jcdwj   1/1     Running   0          22h
pod/coredns-59847d77c8-rspg4   1/1     Running   0          22h
pod/kube-proxy-5vzbc           1/1     Running   0          21h
pod/kube-proxy-dkmz5           1/1     Running   0          21h
pod/kube-proxy-dqwtl           1/1     Running   0          21h

NAME               TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE
service/kube-dns   ClusterIP   10.100.0.10   <none>        53/UDP,53/TCP   22h

NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/aws-node     3         3         3       3            3           <none>          22h
daemonset.apps/kube-proxy   3         3         3       3            3           <none>          22h

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/coredns   2/2     2            2           22h

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/coredns-59847d77c8   2         2         2       22h


##root@labs--311032102:/home/project/user/user02# kub^Ctl config get-context
root@labs--311032102:/home/project/user/user02# kubectl get ns
kubectl get ns
NAME              STATUS   AGE
default           Active   23h
kube-node-lease   Active   23h
kube-public       Active   23h
kube-system       Active   23h


root@labs--311032102:/home/project/user/user02# kubectl get all -n kube-system
kubectl get all -n kube-system
NAME                           READY   STATUS    RESTARTS   AGE
pod/aws-node-krj7z             1/1     Running   0          23h                    <--- 네트웍 담당  (CNI POD간 통신)
pod/aws-node-mk88p             1/1     Running   0          23h
pod/aws-node-mlgh2             1/1     Running   0          23h
pod/coredns-59847d77c8-jcdwj   1/1     Running   0          23h
pod/coredns-59847d77c8-rspg4   1/1     Running   0          23h
pod/kube-proxy-5vzbc           1/1     Running   0          23h                   <--- 쿠너네틱스 담당
pod/kube-proxy-dkmz5           1/1     Running   0          23h
pod/kube-proxy-dqwtl           1/1     Running   0          23h

NAME               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE
service/kube-dns   ClusterIP   10.100.0.10     <none>        53/UDP,53/TCP   23h
service/userapp    ClusterIP   10.100.162.88   <none>        8080/TCP        84m

NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/aws-node     3         3         3       3            3           <none>          23h
daemonset.apps/kube-proxy   3         3         3       3            3           <none>          23h

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/coredns   2/2     2            2           23h

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/coredns-59847d77c8   2         2         2       23h

root@labs--311032102:/home/project/user/user02# kubectl get pod -o wide    <== 리부팅되면 IP대역이 바뀜 ==> hostname 을 사용함.
kubectl get pod -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP               NODE                                               NOMINATED NODE   READINESS GATES
nginx-6799fc88d8-4b9wl            1/1     Running   0          20h   192.168.43.80    ip-192-168-45-77.ap-northeast-1.compute.internal   <none>           <none>
nginx-6799fc88d8-bh6d7            1/1     Running   0          20h   192.168.3.78     ip-192-168-29-79.ap-northeast-1.compute.internal   <none>           <none>
nginx-6799fc88d8-xhj5j            1/1     Running   0          20h   192.168.88.157   ip-192-168-91-26.ap-northeast-1.compute.internal   <none>           <none>
nginx-gorapa-2-8559b5bc76-8xgc8   1/1     Running   0          20h   192.168.58.153   ip-192-168-45-77.ap-northeast-1.compute.internal   <none>           <none>
nginx-gorapa-2-8559b5bc76-mzblw   1/1     Running   0          20h   192.168.65.154   ip-192-168-91-26.ap-northeast-1.compute.internal   <none>           <none>
nginx-gorapa-2-8559b5bc76-sbwq9   1/1     Running   0          21h   192.168.30.9     ip-192-168-29-79.ap-northeast-1.compute.internal   <none>           <none>
order-6d7c686964-s5vfm            1/1     Running   0          21h   192.168.9.201    ip-192-168-29-79.ap-northeast-1.compute.internal   <none>           <none>
siege                             1/1     Running   0          20h   192.168.51.156   ip-192-168-45-77.ap-northeast-1.compute.internal   <none>           <none>
siege-55b7f7d5fc-vwqk6            1/1     Running   0          81m   192.168.57.195   ip-192-168-45-77.ap-northeast-1.compute.internal   <none>           <none>
user02-order-5fd6ff986d-4q9q6     1/1     Running   0          21h   192.168.75.188   ip-192-168-91-26.ap-northeast-1.compute.internal   <none>           <none>
userapp                           1/1     Running   0          80m   192.168.70.67    ip-192-168-91-26.ap-northeast-1.compute.internal   <none>           <none>
root@labs--311032102:/home/project/user/user02# 

root@labs--311032102:/home/project/user/user02# kubectl delete pod nginx-6799fc88d8-4b9wl
kubectl delete pod nginx-6799fc88d8-4b9wl
pod "nginx-6799fc88d8-4b9wl" deleted
^[[Aroot@labs--311032102:/home/project/user/user02# kubectl get pod -o wide
kubectl get pod -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP               NODE                                               NOMINATED NODE   READINESS GATES
nginx-6799fc88d8-9xsp9            1/1     Running   0          13s   192.168.53.173   ip-192-168-45-77.ap-northeast-1.compute.internal   <none>           <none>
nginx-6799fc88d8-bh6d7            1/1     Running   0          21h   192.168.3.78     ip-192-168-29-79.ap-northeast-1.compute.internal   <none>           <none>
nginx-6799fc88d8-xhj5j            1/1     Running   0          21h   192.168.88.157   ip-192-168-91-26.ap-northeast-1.compute.internal   <none>           <none>
nginx-gorapa-2-8559b5bc76-8xgc8   1/1     Running   0          21h   192.168.58.153   ip-192-168-45-77.ap-northeast-1.compute.internal   <none>           <none>
nginx-gorapa-2-8559b5bc76-mzblw   1/1     Running   0          21h   192.168.65.154   ip-192-168-91-26.ap-northeast-1.compute.internal   <none>           <none>
nginx-gorapa-2-8559b5bc76-sbwq9   1/1     Running   0          21h   192.168.30.9     ip-192-168-29-79.ap-northeast-1.compute.internal   <none>           <none>
order-6d7c686964-s5vfm            1/1     Running   0          21h   192.168.9.201    ip-192-168-29-79.ap-northeast-1.compute.internal   <none>           <none>
siege                             1/1     Running   0          20h   192.168.51.156   ip-192-168-45-77.ap-northeast-1.compute.internal   <none>           <none>
siege-55b7f7d5fc-vwqk6            1/1     Running   0          87m   192.168.57.195   ip-192-168-45-77.ap-northeast-1.compute.internal   <none>           <none>
user02-order-5fd6ff986d-4q9q6     1/1     Running   0          21h   192.168.75.188   ip-192-168-91-26.ap-northeast-1.compute.internal   <none>           <none>
userapp                           1/1     Running   0          86m   192.168.70.67    ip-192-168-91-26.ap-northeast-1.compute.internal   <none>           <none>
root@labs--311032102:/home/project/user/user02# 


root@labs--311032102:/home/project/user/user02# kubectl get pod --show-labels   
==> POD끼리 찾는 방법으 라벨으로 찾아야함.
kubectl get pod --show-labels
NAME                              READY   STATUS    RESTARTS   AGE   LABELS
nginx-6799fc88d8-9xsp9            1/1     Running   0          47s   app=nginx,pod-template-hash=6799fc88d8
nginx-6799fc88d8-bh6d7            1/1     Running   0          21h   app=nginx,pod-template-hash=6799fc88d8
nginx-6799fc88d8-xhj5j            1/1     Running   0          21h   app=nginx,pod-template-hash=6799fc88d8
nginx-gorapa-2-8559b5bc76-8xgc8   1/1     Running   0          21h   app=nginx-gorapa-2,pod-template-hash=8559b5bc76
nginx-gorapa-2-8559b5bc76-mzblw   1/1     Running   0          21h   app=nginx-gorapa-2,pod-template-hash=8559b5bc76
nginx-gorapa-2-8559b5bc76-sbwq9   1/1     Running   0          21h   app=nginx-gorapa-2,pod-template-hash=8559b5bc76
order-6d7c686964-s5vfm            1/1     Running   0          21h   app=order,pod-template-hash=6d7c686964
siege                             1/1     Running   0          20h   run=siege
siege-55b7f7d5fc-vwqk6            1/1     Running   0          88m   pod-template-hash=55b7f7d5fc,run=siege
user02-order-5fd6ff986d-4q9q6     1/1     Running   0          21h   app=user02-order,pod-template-hash=5fd6ff986d
userapp                           1/1     Running   0          87m   app=userapp

## 설명 (그림)
https://github.com/redolence0103/peoplelife

## 네트워크 구조..
1. POD Network
2. Cluster Network
3. NODE Network
  => 공인아이피인경우 별도 구성을 해야함.


###
root@labs--311032102:/home/project/user/user02# kubectl run netcom --image=zasmin/net-command:1.0
kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
Error from server (AlreadyExists): deployments.apps "netcom" already exists

## 버전업
root@labs--311032102:/home/project/user/user02# curl -LO https://dl.k8s.io/release/v1.19.0/bin/linux/amd64/kubectl
curl -LO https://dl.k8s.io/release/v1.19.0/bin/linux/amd64/kubectl
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   154  100   154    0     0    572      0 --:--:-- --:--:-- --:--:--   572
100 41.0M  100 41.0M    0     0  60.9M      0 --:--:-- --:--:-- --:--:--  321M
root@labs--311032102:/home/project/user/user02# cp ./kubectl /usr/bin/kubectl
cp ./kubectl /usr/bin/kubectl
root@labs--311032102:/home/project/user/user02# kubectl version --short
kubectl version --short
Client Version: v1.19.0
Server Version: v1.19.16-eks-de875a99

## 
root@labs--311032102:/home/project/user/user02#  kubectl run netcom --image=zasmin/net-command:1.0
 kubectl run netcom --image=zasmin/net-command:1.0
pod/netcom created

NAME                              READY   STATUS             RESTARTS  AGE
netcom                            0/1     Completed          2  29s
user02-order-5fd6ff986d-4q9q6     1/1     Running            0  22h
userapp                           1/1     Running            0  100m

root@labs--311032102:/home/project/user/user02# kubectl delete pod netcom
pod "netcom" deleted

root@labs--311032102:/home/project/user/user02# kubectl run netcom --image=zasmin/net-command:1.0 --command sleep 3600
kubectl run netcom --image=zasmin/net-command:1.0 --command sleep 3600
pod/netcom created


## GPC 환경 ==> 서비스를 찾아야함.
root@labs--311032102:/home/project/user/user02# kubectl get service
NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
kubernetes       ClusterIP   10.100.0.1       <none>        443/TCP    23h
nginx-gorapa-2   ClusterIP   10.100.91.162    <none>        8080/TCP   20h
userapp          ClusterIP   10.100.170.131   <none>        8080/TCP   129m
root@labs--311032102:/home/project/user/user02# 

oot@labs--311032102:/home/project/user/user02# kubectl get service -n kube-system
NAME       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE
kube-dns   ClusterIP   10.100.0.10     <none>        53/UDP,53/TCP   23h

root@labs--311032102:/home/project/user/user02# kubectl exec -it netcom -- bash
: 쿠버네틱스 
kubectl exec -it netcom -- bash
utils@netcom:~$ nslookup
Name:   userapp.default.svc.cluster.local        <-- default 는 자기것.... 
Address: 10.100.170.131
> 
> kube-dns
Server:         10.100.0.10
Address:        10.100.0.10#53

** server can't find kube-dns: NXDOMAIN     <-- 다른 namespace 있는 kube-system 찾을때..
> kube-dns.kube-system
Server:         10.100.0.10
Address:        10.100.0.10#53

Name:   kube-dns.kube-system.svc.cluster.local
Address: 10.100.0.10


###
oot@labs--311032102:~# ls -la
total 88
drwx------ 1 root root  4096 Jun 21 02:27 .
drwxr-xr-x 1 root root  4096 Jun 20 23:00 ..
drwxr-xr-x 2 root root  4096 Jun 21 00:30 .aws
-rw------- 1 root root   464 Jun 21 02:29 .bash_history
-rw-r--r-- 1 root root  3148 Jun 17 08:31 .bashrc
drwxr-xr-x 1 root root  4096 Jun 21 00:28 .cache
drwxr-xr-x 3 root root  4096 May  4 19:53 .config
drwx------ 2 root root  4096 Jun 21 01:09 .docker
-rw-r--r-- 1 root root   298 Jun 17 08:31 .fzf.bash
drwx------ 1 root root  4096 May  4 19:53 .gnupg
drwx------ 2 root root  4096 Jun 21 00:53 .httpie
drwxr-xr-x 4 root root  4096 Jun 21 02:43 .kube
drwx------ 4 root root  4096 May  4 19:51 .local
drwxr-xr-x 3 root root  4096 Jun 21 00:28 .m2
-rw-r--r-- 1 root root   148 Aug 17  2015 .profile
drwxr-xr-x 6 root root  4096 Jun 21 00:28 .theia
-rw------- 1 root root 14601 Jun 21 02:27 .viminfo
-rw-r--r-- 1 root root   203 Jun 17 08:31 .wget-hsts
root@labs--311032102:~# cd .kube
root@labs--311032102:~/.kube# ls
cache  config  config.eksctl.lock  http-cache
root@labs--311032102:~/.kube# ls -lrt
total 16
-rw------- 1 root root    0 Jun 20 04:43 config.eksctl.lock
drwxr-x--- 4 root root 4096 Jun 20 06:46 cache
-rw-r--r-- 1 root root 5658 Jun 21 02:43 config
drwxr-x--- 3 root root 4096 Jun 21 04:21 http-cache
root@labs--311032102:~/.kube# cd config
bash: cd: config: Not a directory
root@labs--311032102:~/.kube# ls
cache  config  config.eksctl.lock  http-cache
root@labs--311032102:~/.kube# vi config
root@labs--311032102:~/.kube# cat config
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1EWXlNREEwTXpFMU9Gb1hEVE15TURZeE56QTBNekUxT0Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHNECkhWUHNhMUVuUGhNdjR3NkZXa01KR2oyd1V0UHNGWmxKRGhGK3ZBNkwvS293OUJEVW5KTG5IdXFrWXl1ZlBtSDIKNSs0aTNQZVUzeWIzckFCTGdZRlNpU2k2VERCbVFXejVxbGNOeUxlYU5CWExvNFlFdFNNWTBhcFlYMVF4RFBlKwprM2l5VXdjMkpNa0xEVTM2clpFR3pyYi8zZFZ3c05BUmJHUjZmd3gxMEZpYXR3cVRnUVpmK2JOZ05NdWplR0p2CmJIRER2eHEzaFVSa3kvUjNlNUxPNDlWNTBLbzFPR21DSitDaVliY0tFY0tWanU2SmptYVhwNjU5VU83LzNMTmwKVDl2bTVlTHhLYkZkYTA1SWtzeno2TUZ1SFhmdTdCZm05Y3NzYzkrZFRJZlVrR05YVTFsV3pkMFVnbS9Vb0RTbgpHRGoyeXFwOERSY1RQMzI2bkFFQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZKT0VRbFpERWVYN2Nnc1BXOG9meEdQcUdodjZNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFCQ01vTWtKazQxV1l6elU2OEg3aml6SFByWkpaYTg2UGJ0T203YXlwNllrbnJjeVMvTQpPaStQLyt4NjM2N3dHYTc0bjFsSkFTRHpFS3dFV013U2NKRmxKUU5qWExoRW5obUhWa3FLRXZ4SUNEa3YyZVA2CmdRZ1p4OFcwT3QyeWxnR1lvdU1yb09zL1RlSXJDR3dtcmVZR2pEUHh3RnJSVFg3b1NNZTVxSktqa2xyakFwa08KaFFmdUhLVjMzZnlMaXF2SzB5dnNsS2pETzQ3dlFIWFZkQWR0WkZQOXZ0eWluNHRBdkc2SVRTN05lcDY2UEZOWgpWMElwK20wQm0wdVBLTHVUMC81VEt5MHlSUGhEd2lQL1BKbnRNRnFBVlRubUUwWlJqcUUzK2l2VjdaQjNiY0NwCjlmT09TWmtWMXZCdXIrU0xJbUpFRTBaczBqeFdtelpYSmxCTQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://77E4F9C82F02ADF5802F48EC25844915.gr7.ap-northeast-1.eks.amazonaws.com
  name: arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks
- cluster:
    insecure-skip-tls-verify: true
    server: https://35.189.156.127
  name: kcb-test2.k8s.local
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1EWXlNREEwTXpFMU9Gb1hEVE15TURZeE56QTBNekUxT0Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHNECkhWUHNhMUVuUGhNdjR3NkZXa01KR2oyd1V0UHNGWmxKRGhGK3ZBNkwvS293OUJEVW5KTG5IdXFrWXl1ZlBtSDIKNSs0aTNQZVUzeWIzckFCTGdZRlNpU2k2VERCbVFXejVxbGNOeUxlYU5CWExvNFlFdFNNWTBhcFlYMVF4RFBlKwprM2l5VXdjMkpNa0xEVTM2clpFR3pyYi8zZFZ3c05BUmJHUjZmd3gxMEZpYXR3cVRnUVpmK2JOZ05NdWplR0p2CmJIRER2eHEzaFVSa3kvUjNlNUxPNDlWNTBLbzFPR21DSitDaVliY0tFY0tWanU2SmptYVhwNjU5VU83LzNMTmwKVDl2bTVlTHhLYkZkYTA1SWtzeno2TUZ1SFhmdTdCZm05Y3NzYzkrZFRJZlVrR05YVTFsV3pkMFVnbS9Vb0RTbgpHRGoyeXFwOERSY1RQMzI2bkFFQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZKT0VRbFpERWVYN2Nnc1BXOG9meEdQcUdodjZNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFCQ01vTWtKazQxV1l6elU2OEg3aml6SFByWkpaYTg2UGJ0T203YXlwNllrbnJjeVMvTQpPaStQLyt4NjM2N3dHYTc0bjFsSkFTRHpFS3dFV013U2NKRmxKUU5qWExoRW5obUhWa3FLRXZ4SUNEa3YyZVA2CmdRZ1p4OFcwT3QyeWxnR1lvdU1yb09zL1RlSXJDR3dtcmVZR2pEUHh3RnJSVFg3b1NNZTVxSktqa2xyakFwa08KaFFmdUhLVjMzZnlMaXF2SzB5dnNsS2pETzQ3dlFIWFZkQWR0WkZQOXZ0eWluNHRBdkc2SVRTN05lcDY2UEZOWgpWMElwK20wQm0wdVBLTHVUMC81VEt5MHlSUGhEd2lQL1BKbnRNRnFBVlRubUUwWlJqcUUzK2l2VjdaQjNiY0NwCjlmT09TWmtWMXZCdXIrU0xJbUpFRTBaczBqeFdtelpYSmxCTQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://77E4F9C82F02ADF5802F48EC25844915.gr7.ap-northeast-1.eks.amazonaws.com
  name: user02-eks.ap-northeast-1.eksctl.io
contexts:
- context:
    cluster: arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks
    namespace: default
    user: arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks
  name: arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks
- context:
    cluster: kcb-test2.k8s.local
    namespace: labs--311032102
    user: labs--311032102
  name: kcb-test2.k8s.local
- context:
    cluster: user02-eks.ap-northeast-1.eksctl.io
    user: user02@user02-eks.ap-northeast-1.eksctl.io
  name: user02@user02-eks.ap-northeast-1.eksctl.io
current-context: arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks
kind: Config
preferences: {}
users:
- name: arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - --region
      - ap-northeast-1
      - eks
      - get-token
      - --cluster-name
      - user02-eks
      command: aws
      env: null
- name: labs--311032102
  user:
    token: eyJhbGciOiJSUzI1NiIsImtpZCI6InFQaE1menpURGlISnprRm44Tmt6UGRNeEVBS3ZoMHQ0MGRrR04zSmZwUjQifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJsYWJzLS0zMTEwMzIxMDIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoibGFicy0tMzExMDMyMTAyLXRva2VuLXg3NXFzIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImxhYnMtLTMxMTAzMjEwMiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImFkYjRkMjUzLTQyMGItNGNhOS1iNjM0LWMzODc2ODUwZjk3YyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpsYWJzLS0zMTEwMzIxMDI6bGFicy0tMzExMDMyMTAyIn0.emrq_HwF-jxjPJuICYl_7mxbuR5-Re8HErPNZriqPHLj9U5ZQnPD27y5WCjx7IrAirvWvBZg-rCfn0TbOhB31V8Iahug0QftMEKcXBSayB8nQk_cXrWdkRic0k0_sU3JfyLQ4OSDyqfDhuPJOua5rtbMYvwLgFA-PeasZU4Hsxjztc0f1ier_ecR4C_ZBtcl_t1ojh-eVqrChIv6Wje9z3M-juhnMVV75fP4JHKUk_8ZxMXrDOUY6OY5fImLEO1MyZeGNE4IW817D_GPL00LhNs4Wr-LZm-ZNm4bsKtAmziir_SqS02w8E3KPZagIrHCT-u9_ErlFyXul2-8bP5leg
- name: user02@user02-eks.ap-northeast-1.eksctl.io
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1alpha1
      args:
      - eks
      - get-token
      - --cluster-name
      - user02-eks
      - --region
      - ap-northeast-1
      command: aws
      env:
      - name: AWS_STS_REGIONAL_ENDPOINTS
        value: regional
root@labs--311032102:~/.kube#

## 토큰으로 jwt.io 에서 확인.

root@labs--311032102:/home/project/user/user02# kubectl exec -it netcom -- bash
kubectl exec -it netcom -- bash
utils@netcom:~$ nslookup
> my-kafka
Server:         10.100.0.10
Address:        10.100.0.10#53

** server can't find my-kafka: NXDOMAIN
> 

## 구글 helm kafka install

root@labs--311032102:/home/project/user/user02# helm repo list
helm repo list
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /root/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /root/.kube/config
Error: no repositories to show
root@labs--311032102:/home/project/user/user02# ls -la /root/.kube/config
ls -la /root/.kube/config
-rw-r--r-- 1 root root 5658 Jun 21 02:43 /root/.kube/config
root@labs--311032102:/home/project/user/user02# chmod 600 /root/.kube/config
chmod 600 /root/.kube/config
root@labs--311032102:/home/project/user/user02# helm repo list

root@labs--311032102:/home/project/user/user02# helm repo list
helm repo list
NAME    URL                               
bitnami https://charts.bitnami.com/bitnami
root@labs--311032102:/home/project/user/user02# ls -lrt
ls -lrt
total 42040
-rw-r--r-- 1 root root     4548 Jun 21 00:45 azure-pipelines.yml
-rw-r--r-- 1 root root     2838 Jun 21 00:45 cloudbuild.yaml
-rw-r--r-- 1 root root      295 Jun 21 00:45 Dockerfile
drwxr-xr-x 2 root root     4096 Jun 21 00:45 kubernetes
-rw-r--r-- 1 root root     3549 Jun 21 00:45 pom.xml
-rw-r--r-- 1 root root     1617 Jun 21 00:45 README.md
drwxr-xr-x 4 root root     4096 Jun 21 00:45 src
drwxr-xr-x 9 root root     4096 Jun 21 00:57 target
-rw-r--r-- 1 root root      186 Jun 21 02:19 user02_pod.yml
-rw-r--r-- 1 root root 43003904 Jun 21 04:22 kubectl
root@labs--311032102:/home/project/user/user02# helm fetch -h
helm fetch -h

Retrieve a package from a package repository, and download it locally.

This is useful for fetching packages to inspect, modify, or repackage. It can
also be used to perform cryptographic verification of a chart without installing
the chart.

There are options for unpacking the chart after download. This will create a
directory for the chart and uncompress into that directory.

If the --verify flag is specified, the requested chart MUST have a provenance
file, and MUST pass the verification process. Failure in any part of this will
result in an error, and the chart will not be saved locally.

Usage:
  helm pull [chart URL | repo/chartname] [...] [flags]

Aliases:
  pull, fetch

Flags:
      --ca-file string             verify certificates of HTTPS-enabled servers using this CA bundle
      --cert-file string           identify HTTPS client using this SSL certificate file
  -d, --destination string         location to write the chart. If this and untardir are specified, untardir is appended to this (default ".")
      --devel                      use development versions, too. Equivalent to version '>0.0.0-0'. If --version is set, this is ignored.
  -h, --help                       help for pull
      --insecure-skip-tls-verify   skip tls certificate checks for the chart download
      --key-file string            identify HTTPS client using this SSL key file
      --keyring string             location of public keys used for verification (default "/root/.gnupg/pubring.gpg")
      --pass-credentials           pass credentials to all domains
      --password string            chart repository password where to locate the requested chart
      --prov                       fetch the provenance file, but don't perform verification
      --repo string                chart repository url where to locate the requested chart
      --untar                      if set to true, will untar the chart after downloading it
      --untardir string            if untar is specified, this flag specifies the name of the directory into which the chart is expanded (default ".")
      --username string            chart repository username where to locate the requested chart
      --verify                     verify the package before using it
      --version string             specify a version constraint for the chart version to use. This constraint can be a specific tag (e.g. 1.1.1) or it may reference a valid range (e.g. ^2.0.0). If this is not specified, the latest version is used

Global Flags:
      --debug                       enable verbose output
      --kube-apiserver string       the address and the port for the Kubernetes API server
      --kube-as-group stringArray   group to impersonate for the operation, this flag can be repeated to specify multiple groups.
      --kube-as-user string         username to impersonate for the operation
      --kube-ca-file string         the certificate authority file for the Kubernetes API server connection
      --kube-context string         name of the kubeconfig context to use
      --kube-token string           bearer token used for authentication
      --kubeconfig string           path to the kubeconfig file
  -n, --namespace string            namespace scope for this request
      --registry-config string      path to the registry config file (default "/root/.config/helm/registry/config.json")
      --repository-cache string     path to the file containing cached repository indexes (default "/root/.cache/helm/repository")
      --repository-config string    path to the file containing repository names and URLs (default "/root/.config/helm/repositories.yaml")
root@labs--311032102:/home/project/user/user02# helm fetch bitnami/kafka
helm fetch bitnami/kafka
root@labs--311032102:/home/project/user/user02/kafka# pwd
pwd
/home/project/user/user02/kafka
root@labs--311032102:/home/project/user/user02# tar xzf kafka-18.0.0.tgz
tar xzf kafka-18.0.0.tgz
root@labs--311032102:/home/project/user/user02# ls -lrt
ls -lrt
total 42156
-rw-r--r-- 1 root root     4548 Jun 21 00:45 azure-pipelines.yml
-rw-r--r-- 1 root root     2838 Jun 21 00:45 cloudbuild.yaml
-rw-r--r-- 1 root root      295 Jun 21 00:45 Dockerfile
drwxr-xr-x 2 root root     4096 Jun 21 00:45 kubernetes
-rw-r--r-- 1 root root     3549 Jun 21 00:45 pom.xml
-rw-r--r-- 1 root root     1617 Jun 21 00:45 README.md
drwxr-xr-x 4 root root     4096 Jun 21 00:45 src
drwxr-xr-x 9 root root     4096 Jun 21 00:57 target
-rw-r--r-- 1 root root      186 Jun 21 02:19 user02_pod.yml
-rw-r--r-- 1 root root 43003904 Jun 21 04:22 kubectl
-rw-r--r-- 1 root root   112191 Jun 21 05:16 kafka-18.0.0.tgz
drwxr-xr-x 4 root root     4096 Jun 21 05:17 kafka
root@labs--311032102:/home/project/user/user02# 

root@labs--311032102:/home/project/user/user02# helm fetch bitnami/kafka
helm fetch bitnami/kafka
root@labs--311032102:/home/project/user/user02# ls-lrt
ls-lrt
bash: ls-lrt: command not found
root@labs--311032102:/home/project/user/user02# ls -lrt
ls -lrt
total 42152
-rw-r--r-- 1 root root     4548 Jun 21 00:45 azure-pipelines.yml
-rw-r--r-- 1 root root     2838 Jun 21 00:45 cloudbuild.yaml
-rw-r--r-- 1 root root      295 Jun 21 00:45 Dockerfile
drwxr-xr-x 2 root root     4096 Jun 21 00:45 kubernetes
-rw-r--r-- 1 root root     3549 Jun 21 00:45 pom.xml
-rw-r--r-- 1 root root     1617 Jun 21 00:45 README.md
drwxr-xr-x 4 root root     4096 Jun 21 00:45 src
drwxr-xr-x 9 root root     4096 Jun 21 00:57 target
-rw-r--r-- 1 root root      186 Jun 21 02:19 user02_pod.yml
-rw-r--r-- 1 root root 43003904 Jun 21 04:22 kubectl
-rw-r--r-- 1 root root   112191 Jun 21 05:16 kafka-18.0.0.tgz
root@labs--311032102:/home/project/user/user02# tar xzf kafka-18.0.0.tgz
tar xzf kafka-18.0.0.tgz
root@labs--311032102:/home/project/user/user02# ls -lrt
ls -lrt
total 42156
-rw-r--r-- 1 root root     4548 Jun 21 00:45 azure-pipelines.yml
-rw-r--r-- 1 root root     2838 Jun 21 00:45 cloudbuild.yaml
-rw-r--r-- 1 root root      295 Jun 21 00:45 Dockerfile
drwxr-xr-x 2 root root     4096 Jun 21 00:45 kubernetes
-rw-r--r-- 1 root root     3549 Jun 21 00:45 pom.xml
-rw-r--r-- 1 root root     1617 Jun 21 00:45 README.md
drwxr-xr-x 4 root root     4096 Jun 21 00:45 src
drwxr-xr-x 9 root root     4096 Jun 21 00:57 target
-rw-r--r-- 1 root root      186 Jun 21 02:19 user02_pod.yml
-rw-r--r-- 1 root root 43003904 Jun 21 04:22 kubectl
-rw-r--r-- 1 root root   112191 Jun 21 05:16 kafka-18.0.0.tgz
drwxr-xr-x 4 root root     4096 Jun 21 05:17 kafka
root@labs--311032102:/home/project/user/user02# cd kafka
cd kafka
root@labs--311032102:/home/project/user/user02/kafka# 

root@labs--311032102:/home/project/user/user02/kafka# 

root@labs--311032102:/home/project/user/user02/kafka# ls -lrt
ls -lrt
total 208
-rw-r--r-- 1 root root  74790 Jun 15 09:53 values.yaml
-rw-r--r-- 1 root root 117724 Jun 15 09:53 README.md
-rw-r--r-- 1 root root    918 Jun 15 09:53 Chart.yaml
-rw-r--r-- 1 root root    307 Jun 15 09:53 Chart.lock
drwxr-xr-x 2 root root   4096 Jun 21 05:17 templates
drwxr-xr-x 4 root root   4096 Jun 21 05:17 charts
root@labs--311032102:/home/project/user/user02/kafka# cp values.yaml values.yaml_org
cp values.yaml values.yaml_org
root@labs--311032102:/home/project/user/user02/kafka# vi values.yaml


root@labs--311032102:/home/project/user/user02/kafka# helm install my-kafka .
helm install my-kafka .
NAME: my-kafka
LAST DEPLOYED: Tue Jun 21 05:26:10 2022
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: kafka
CHART VERSION: 18.0.0
APP VERSION: 3.2.0

** Please be patient while the chart is being deployed **

Kafka can be accessed by consumers via port 9092 on the following DNS name from within your cluster:

    my-kafka.default.svc.cluster.local

Each Kafka broker can be accessed by producers via port 9092 on the following DNS name(s) from within your cluster:

    my-kafka-0.my-kafka-headless.default.svc.cluster.local:9092

To create a pod that you can use as a Kafka client run the following commands:

    kubectl run my-kafka-client --restart='Never' --image docker.io/bitnami/kafka:3.2.0-debian-11-r3 --namespace default --command -- sleep infinity
    kubectl exec --tty -i my-kafka-client --namespace default -- bash

    PRODUCER:
        kafka-console-producer.sh \
            
            --broker-list my-kafka-0.my-kafka-headless.default.svc.cluster.local:9092 \
            --topic test

    CONSUMER:
        kafka-console-consumer.sh \
            
            --bootstrap-server my-kafka.default.svc.cluster.local:9092 \
            --topic test \
            --from-beginning
root@labs--311032102:/home/project/user/user02/kafka# 
root@labs--311032102:/home/project/user/user02/kafka# kubectl get pod
kubectl get pod
NAME                              READY   STATUS             RESTARTS   AGE
my-kafka-0                        1/1     Running            1          86s
my-kafka-zookeeper-0              1/1     Running            0          86s

root@labs--311032102:/home/project/user/user02/kafka# kubectl get services
kubectl get services
NAME                          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
kubernetes                    ClusterIP   10.100.0.1       <none>        443/TCP                      24h
my-kafka                      ClusterIP   10.100.147.203   <none>        9092/TCP                     2m27s
my-kafka-headless             ClusterIP   None             <none>        9092/TCP,9093/TCP            2m27s
my-kafka-zookeeper            ClusterIP   10.100.124.20    <none>        2181/TCP,2888/TCP,3888/TCP   2m27s
my-kafka-zookeeper-headless   ClusterIP   None             <none>        2181/TCP,2888/TCP,3888/TCP   2m27s
nginx-gorapa-2                ClusterIP   10.100.91.162    <none>        8080/TCP                     21h
userapp                       ClusterIP   10.100.170.131   <none>        8080/TCP                     3h8m



root@labs--311032102:~/.kube# kubectl get services -n kube-system
NAME       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE
kube-dns   ClusterIP   10.100.0.10     <none>        53/UDP,53/TCP   24h
userapp    ClusterIP   10.100.162.88   <none>        8080/TCP        122m
root@labs--311032102:~/.kube# kubectl exec -it netcom -- bash
utils@netcom:~$ nslookup
> my-kafca
Server:         10.100.0.10
Address:        10.100.0.10#53

** server can't find my-kafca: NXDOMAIN
> my-kafka
Server:         10.100.0.10
Address:        10.100.0.10#53

Name:   my-kafka.default.svc.cluster.local
Address: 10.100.147.203
> 

root@labs--311032102:/home/project/user/user02/kafka# kubectl get pod
kubectl get pod
NAME                              READY   STATUS             RESTARTS   AGE
my-kafka-0                        1/1     Running            1          86s
my-kafka-zookeeper-0              1/1     Running            0          86s
netcom                            1/1     Running            0          59m
netcom-d598f8f9b-tgv8l            0/1     ImagePullBackOff   0          63m
nginx-6799fc88d8-9xsp9            1/1     Running            0          75m
nginx-6799fc88d8-bh6d7            1/1     Running            0          22h
nginx-6799fc88d8-xhj5j            1/1     Running            0          22h
nginx-gorapa-2-8559b5bc76-8xgc8   1/1     Running            0          22h
nginx-gorapa-2-8559b5bc76-mzblw   1/1     Running            0          22h
nginx-gorapa-2-8559b5bc76-sbwq9   1/1     Running            0          22h
order-6d7c686964-s5vfm            1/1     Running            0          23h
siege                             1/1     Running            0          21h
siege-55b7f7d5fc-vwqk6            1/1     Running            0          162m
user02-order-5fd6ff986d-4q9q6     1/1     Running            0          23h
userapp                           1/1     Running            0          161m
root@labs--311032102:/home/project/user/user02/kafka# helm ^C
root@labs--311032102:/home/project/user/user02/kafka# kubectl get services
kubectl get services
NAME                          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
kubernetes                    ClusterIP   10.100.0.1       <none>        443/TCP                      24h
my-kafka                      ClusterIP   10.100.147.203   <none>        9092/TCP                     2m27s
my-kafka-headless             ClusterIP   None             <none>        9092/TCP,9093/TCP            2m27s
my-kafka-zookeeper            ClusterIP   10.100.124.20    <none>        2181/TCP,2888/TCP,3888/TCP   2m27s
my-kafka-zookeeper-headless   ClusterIP   None             <none>        2181/TCP,2888/TCP,3888/TCP   2m27s
nginx-gorapa-2                ClusterIP   10.100.91.162    <none>        8080/TCP                     21h
userapp                       ClusterIP   10.100.170.131   <none>        8080/TCP                     3h8m
root@labs--311032102:/home/project/user/user02/kafka# kubectl delete pod order
kubectl delete pod order
Error from server (NotFound): pods "order" not found
root@labs--311032102:/home/project/user/user02/kafka# kubectl delete pod userapp
kubectl delete pod userapp
pod "userapp" deleted
root@labs--311032102:/home/project/user/user02/kafka# 

root@labs--311032102:/home/project/user/user02/kafka# 

root@labs--311032102:/home/project/user/user02/kafka# kubectl run user^C
root@labs--311032102:/home/project/user/user02/kafka# ls
ls
Chart.lock  Chart.yaml  README.md  charts  templates  values.yaml  values.yaml_org
root@labs--311032102:/home/project/user/user02/kafka# cd ..
cd ..
root@labs--311032102:/home/project/user/user02# ls
ls
Dockerfile  README.md  azure-pipelines.yml  cloudbuild.yaml  kafka  kafka-18.0.0.tgz  kubectl  kubernetes  pom.xml  src  target  user02_pod.yml
root@labs--311032102:/home/project/user/user02# kubectl create -f user02_pod.yml
kubectl create -f user02_pod.yml
pod/userapp created
root@labs--311032102:/home/project/user/user02# kubectl get pod
kubectl get pod
NAME                              READY   STATUS             RESTARTS   AGE
my-kafka-0                        1/1     Running            1          6m35s
my-kafka-zookeeper-0              1/1     Running            0          6m35s
netcom                            1/1     Running            1          64m
netcom-d598f8f9b-tgv8l            0/1     ImagePullBackOff   0          68m
nginx-6799fc88d8-9xsp9            1/1     Running            0          80m
nginx-6799fc88d8-bh6d7            1/1     Running            0          22h
nginx-6799fc88d8-xhj5j            1/1     Running            0          22h
nginx-gorapa-2-8559b5bc76-8xgc8   1/1     Running            0          22h
nginx-gorapa-2-8559b5bc76-mzblw   1/1     Running            0          22h
nginx-gorapa-2-8559b5bc76-sbwq9   1/1     Running            0          22h
order-6d7c686964-s5vfm            1/1     Running            0          23h
siege                             1/1     Running            0          21h
siege-55b7f7d5fc-vwqk6            1/1     Running            0          168m
user02-order-5fd6ff986d-4q9q6     1/1     Running            0          23h
userapp                           1/1     Running            0          11s
root@labs--311032102:/home/project/user/user02# kubectl logs userapp
kubectl logs userapp

root@labs--311032102:/home/project/user/user02# kubectl get all
kubectl get all
NAME                                  READY   STATUS             RESTARTS   AGE
pod/my-kafka-0                        1/1     Running            1          7m33s
pod/my-kafka-zookeeper-0              1/1     Running            0          7m33s
pod/netcom                            1/1     Running            1          65m
pod/netcom-d598f8f9b-tgv8l            0/1     ImagePullBackOff   0          69m
pod/nginx-6799fc88d8-9xsp9            1/1     Running            0          81m
pod/nginx-6799fc88d8-bh6d7            1/1     Running            0          22h
pod/nginx-6799fc88d8-xhj5j            1/1     Running            0          22h
pod/nginx-gorapa-2-8559b5bc76-8xgc8   1/1     Running            0          22h
pod/nginx-gorapa-2-8559b5bc76-mzblw   1/1     Running            0          22h
pod/nginx-gorapa-2-8559b5bc76-sbwq9   1/1     Running            0          22h
pod/order-6d7c686964-s5vfm            1/1     Running            0          23h
pod/siege                             1/1     Running            0          21h
pod/siege-55b7f7d5fc-vwqk6            1/1     Running            0          169m
pod/user02-order-5fd6ff986d-4q9q6     1/1     Running            0          23h
pod/userapp                           1/1     Running            0          69s

NAME                                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
service/kubernetes                    ClusterIP   10.100.0.1       <none>        443/TCP                      25h
service/my-kafka                      ClusterIP   10.100.147.203   <none>        9092/TCP                     7m33s
service/my-kafka-headless             ClusterIP   None             <none>        9092/TCP,9093/TCP            7m33s
service/my-kafka-zookeeper            ClusterIP   10.100.124.20    <none>        2181/TCP,2888/TCP,3888/TCP   7m33s
service/my-kafka-zookeeper-headless   ClusterIP   None             <none>        2181/TCP,2888/TCP,3888/TCP   7m33s
service/nginx-gorapa-2                ClusterIP   10.100.91.162    <none>        8080/TCP                     21h
service/userapp                       ClusterIP   10.100.170.131   <none>        8080/TCP                     3h13m

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/netcom           0/1     1            0           73m
deployment.apps/nginx            3/3     3            3           22h
deployment.apps/nginx-gorapa-2   3/3     3            3           22h
deployment.apps/order            1/1     1            1           23h
deployment.apps/siege            1/1     1            1           169m
deployment.apps/user02-order     1/1     1            1           23h

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/netcom-d598f8f9b            1         1         0       73m
replicaset.apps/nginx-6799fc88d8            3         3         3       22h
replicaset.apps/nginx-gorapa-2-8559b5bc76   3         3         3       22h
replicaset.apps/order-6d7c686964            1         1         1       23h
replicaset.apps/siege-55b7f7d5fc            1         1         1       169m
replicaset.apps/user02-order-5fd6ff986d     1         1         1       23h

NAME                                  READY   AGE
statefulset.apps/my-kafka             1/1     7m33s
statefulset.apps/my-kafka-zookeeper   1/1     7m33s
root@labs--311032102:/home/project/user/user02# kubectl get services
kubectl get services
NAME                          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
kubernetes                    ClusterIP   10.100.0.1       <none>        443/TCP                      25h
my-kafka                      ClusterIP   10.100.147.203   <none>        9092/TCP                     7m55s
my-kafka-headless             ClusterIP   None             <none>        9092/TCP,9093/TCP            7m55s
my-kafka-zookeeper            ClusterIP   10.100.124.20    <none>        2181/TCP,2888/TCP,3888/TCP   7m55s
my-kafka-zookeeper-headless   ClusterIP   None             <none>        2181/TCP,2888/TCP,3888/TCP   7m55s
nginx-gorapa-2                ClusterIP   10.100.91.162    <none>        8080/TCP                     21h
userapp                       ClusterIP   10.100.170.131   <none>        8080/TCP                     3h13m

##  용량확인 service/metrics-server
설치를 해야함.

wget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.5.0/components.yaml

root@labs--311032102:/home/project/user/user02# wget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.5.0/components.yaml
wget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.5.0/components.yaml
--2022-06-21 05:37:39--  https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.5.0/components.yaml
Resolving github.com (github.com)... 52.192.72.89
Connecting to github.com (github.com)|52.192.72.89|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/92132038/cd1df380-bfda-11eb-9ef4-3e2b52c08a3e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220621%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220621T053712Z&X-Amz-Expires=300&X-Amz-Signature=9988d92660c17898e8b758e740f082ea149f9c131ee72065439081aa2ec13692&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=92132038&response-content-disposition=attachment%3B%20filename%3Dcomponents.yaml&response-content-type=application%2Foctet-stream [following]
--2022-06-21 05:37:39--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/92132038/cd1df380-bfda-11eb-9ef4-3e2b52c08a3e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220621%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220621T053712Z&X-Amz-Expires=300&X-Amz-Signature=9988d92660c17898e8b758e740f082ea149f9c131ee72065439081aa2ec13692&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=92132038&response-content-disposition=attachment%3B%20filename%3Dcomponents.yaml&response-content-type=application%2Foctet-stream
Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...
Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 4115 (4.0K) [application/octet-stream]
Saving to: ‘components.yaml’

components.yaml                       100%[=======================================================================>]   4.02K  --.-KB/s    in 0s      

2022-06-21 05:37:39 (22.9 MB/s) - ‘components.yaml’ saved [4115/4115]

root@labs--311032102:/home/project/user/user02# ls -lrt
ls -lrt
total 42164
-rw-r--r-- 1 root root     4115 Dec  8  2021 components.yaml
-rw-r--r-- 1 root root     4548 Jun 21 00:45 azure-pipelines.yml
-rw-r--r-- 1 root root     2838 Jun 21 00:45 cloudbuild.yaml
-rw-r--r-- 1 root root      295 Jun 21 00:45 Dockerfile
drwxr-xr-x 2 root root     4096 Jun 21 00:45 kubernetes
-rw-r--r-- 1 root root     3549 Jun 21 00:45 pom.xml
-rw-r--r-- 1 root root     1617 Jun 21 00:45 README.md
drwxr-xr-x 4 root root     4096 Jun 21 00:45 src
drwxr-xr-x 9 root root     4096 Jun 21 00:57 target
-rw-r--r-- 1 root root      186 Jun 21 02:19 user02_pod.yml
-rw-r--r-- 1 root root 43003904 Jun 21 04:22 kubectl
-rw-r--r-- 1 root root   112191 Jun 21 05:16 kafka-18.0.0.tgz
drwxr-xr-x 4 root root     4096 Jun 21 05:25 kafka
root@labs--311032102:/home/project/user/user02# kubectl create -f components.yaml
kubectl create -f components.yaml
serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
root@labs--311032102:/home/project/user/user02# 


root@labs--311032102:/home/project/user/user02# vi components.yaml
vi components.yaml

  service:
    name: metrics-server
    namespace: kube-system


       image: k8s.gcr.io/metrics-server/metrics-server:v0.5.0



root@labs--311032102:/home/project/user/user02# vi components.yaml
vi components.yaml
root@labs--311032102:/home/project/user/user02# kubectl get pod -n kube-system
kubectl get pod -n kube-system
NAME                             READY   STATUS    RESTARTS   AGE
aws-node-krj7z                   1/1     Running   0          24h
aws-node-mk88p                   1/1     Running   0          24h
aws-node-mlgh2                   1/1     Running   0          24h
coredns-59847d77c8-jcdwj         1/1     Running   0          25h
coredns-59847d77c8-rspg4         1/1     Running   0          25h
kube-proxy-5vzbc                 1/1     Running   0          24h
kube-proxy-dkmz5                 1/1     Running   0          24h
kube-proxy-dqwtl                 1/1     Running   0          24h
metrics-server-9f459d97b-n4wbw   1/1     Running   0          3m4s



root@labs--311032102:/home/project/user/user02# kubectl top pod
kubectl top pod
NAME                              CPU(cores)   MEMORY(bytes)   
my-kafka-0                        9m           390Mi           
my-kafka-zookeeper-0              5m           98Mi            
netcom                            0m           3Mi             
nginx-6799fc88d8-9xsp9            0m           3Mi             
nginx-6799fc88d8-bh6d7            0m           3Mi             
nginx-6799fc88d8-xhj5j            0m           3Mi             
nginx-gorapa-2-8559b5bc76-8xgc8   0m           3Mi             
nginx-gorapa-2-8559b5bc76-mzblw   0m           3Mi             
nginx-gorapa-2-8559b5bc76-sbwq9   0m           3Mi             
order-6d7c686964-s5vfm            1m           210Mi           
siege                             0m           4Mi             
siege-55b7f7d5fc-vwqk6            0m           2Mi             
user02-order-5fd6ff986d-4q9q6     2m           209Mi           
userapp                           5m           224Mi


       resources:
          requests:
            cpu: 100m
            memory: 200Mi

root@labs--311032102:/home/project/user/user02# kubectl top pod
kubectl top pod
NAME                              CPU(cores)   MEMORY(bytes)   
my-kafka-0                        9m           390Mi           
my-kafka-zookeeper-0              5m           98Mi            
netcom                            0m           3Mi             
nginx-6799fc88d8-9xsp9            0m           3Mi             
nginx-6799fc88d8-bh6d7            0m           3Mi             
nginx-6799fc88d8-xhj5j            0m           3Mi             
nginx-gorapa-2-8559b5bc76-8xgc8   0m           3Mi             
nginx-gorapa-2-8559b5bc76-mzblw   0m           3Mi             
nginx-gorapa-2-8559b5bc76-sbwq9   0m           3Mi             
order-6d7c686964-s5vfm            1m           210Mi           
siege                             0m           4Mi             
siege-55b7f7d5fc-vwqk6            0m           2Mi             
user02-order-5fd6ff986d-4q9q6     2m           209Mi           
userapp                           5m           224Mi           
root@labs--311032102:/home/project/user/user02# ls -lrt

## 성능제어 (용량제어)
root@labs--311032102:/home/project/user/user02# cat user02_pod.yml
cat user02_pod.yml
apiVersion: v1
kind: Pod
metadata:
  name: userapp
  labels:
    app: userapp 
spec:
  containers:
  - name: userapp
    image: gorapadd/userapp:1.0
    resources:
      requests:
        cpu: 100m
        memory: 300Mi
      limits:
        cpu: 300m
        memory: 500Mi
    ports:
    - containerPort: 8080

root@labs--311032102:/home/project/user/user02# kubectl delete pod userapp
kubectl delete pod userapp
pod "userapp" deleted
root@labs--311032102:/home/project/user/user02# kubectl create -f user02_pod.yml
kubectl create -f user02_pod.yml
pod/userapp created
root@labs--311032102:/home/project/user/user02# kubectl describe pod userapp
kubectl describe pod userapp
Name:         userapp
Namespace:    default
Priority:     0
Node:         ip-192-168-91-26.ap-northeast-1.compute.internal/192.168.91.26
Start Time:   Tue, 21 Jun 2022 05:53:09 +0000
Labels:       app=userapp
Annotations:  kubernetes.io/psp: eks.privileged
Status:       Running
IP:           192.168.81.185
IPs:
  IP:  192.168.81.185
Containers:
  userapp:
    Container ID:   docker://23dd1fbd93caa57182bff2adf524b019ae4ef921ff91a26840a34f5216b1c6bf
    Image:          gorapadd/userapp:1.0
    Image ID:       docker-pullable://gorapadd/userapp@sha256:c1c4a78c128d25a92da8101fea78ea0e17b2455c3d0d24b8af2abe4fb7dab267
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Tue, 21 Jun 2022 05:53:10 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     300m
      memory:  500Mi
    Requests:
      cpu:        100m
      memory:     300Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zkb74 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-zkb74:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-zkb74
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From                                                       Message
  ----    ------     ----  ----                                                       -------
  Normal  Scheduled  9s    default-scheduler                                          Successfully assigned default/userapp to ip-192-168-91-26.ap-northeast-1.compute.internal
  Normal  Pulled     8s    kubelet, ip-192-168-91-26.ap-northeast-1.compute.internal  Container image "gorapadd/userapp:1.0" already present on machine
  Normal  Created    8s    kubelet, ip-192-168-91-26.ap-northeast-1.compute.internal  Created container userapp
  Normal  Started    8s    kubelet, ip-192-168-91-26.ap-northeast-1.compute.internal  Started container userapp
root@labs--311032102:/home/project/user/user02# 

#부하발생

oot@labs--311032102:/home/project/user/user02# kubectl exec -it siege -- bash
root@siege:/# http http://userapp:8080/user02s
HTTP/1.1 200 
Connection: keep-alive
Content-Type: application/hal+json
Date: Tue, 21 Jun 2022 05:57:55 GMT
Keep-Alive: timeout=60
Transfer-Encoding: chunked
Vary: Origin
Vary: Access-Control-Request-Method
Vary: Access-Control-Request-Headers

{
    "_embedded": {
        "user02s": []
    },
    "_links": {
        "profile": {
            "href": "http://userapp:8080/profile/user02s"
        },
        "self": {
            "href": "http://userapp:8080/user02s"
        }
    },
    "page": {
        "number": 0,
        "size": 20,
        "totalElements": 0,
        "totalPages": 0
    }
}

root@siege:/# siege -c2 -t10S  -v --content-type "application/json" 'http://userapp:8088/user02s '
** SIEGE 4.0.4
** Preparing 2 concurrent users for battle.
The server is now under siege...

Lifting the server siege...
Transactions:                      0 hits
Availability:                   0.00 %
Elapsed time:                   9.87 secs
Data transferred:               0.00 MB
Response time:                  0.00 secs
Transaction rate:               0.00 trans/sec
Throughput:                     0.00 MB/sec
Concurrency:                    0.00
Successful transactions:           0
Failed transactions:               0
Longest transaction:            0.00
Shortest transaction:           0.00
 
root@siege:/# 


## 실패 제한 restartPolicy     ==> end Point가없으면 계속 재부팅된다 

root@labs--311032102:/home/project/user/user02# kubectl explain pod.spec.restartPolicy
kubectl explain pod.spec.restartPolicy
KIND:     Pod
VERSION:  v1

FIELD:    restartPolicy <string>

DESCRIPTION:
     Restart policy for all containers within the pod. One of Always, OnFailure,
     Never. Default to Always. More info:
     https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy



###  restartPolicy 정책 

root@labs--311032102:/home/project/user/user02# cat busy_box.yml
cat busy_box.yml
apiVersion: v1
kind: Pod
metadata:
  name: busybox 
  labels:
    app: userapp 
spec:
  containers:
  - name: busybox
    image: busybox
    ports:
    - containerPort: 8080
  restartPolicy: Never


oot@labs--311032102:/home/project/user/user02# vi busy_box.yml
vi busy_box.yml
root@labs--311032102:/home/project/user/user02# 

root@labs--311032102:/home/project/user/user02# vi busy_box.yml
vi busy_box.yml
root@labs--311032102:/home/project/user/user02# ls -lrt
ls -lrt
total 42172
-rw-r--r-- 1 root root     4115 Dec  8  2021 components.yaml
-rw-r--r-- 1 root root     4548 Jun 21 00:45 azure-pipelines.yml
-rw-r--r-- 1 root root     2838 Jun 21 00:45 cloudbuild.yaml
-rw-r--r-- 1 root root      295 Jun 21 00:45 Dockerfile
drwxr-xr-x 2 root root     4096 Jun 21 00:45 kubernetes
-rw-r--r-- 1 root root     3549 Jun 21 00:45 pom.xml
-rw-r--r-- 1 root root     1617 Jun 21 00:45 README.md
drwxr-xr-x 4 root root     4096 Jun 21 00:45 src
drwxr-xr-x 9 root root     4096 Jun 21 00:57 target
-rw-r--r-- 1 root root 43003904 Jun 21 04:22 kubectl
-rw-r--r-- 1 root root   112191 Jun 21 05:16 kafka-18.0.0.tgz
drwxr-xr-x 4 root root     4096 Jun 21 05:25 kafka
-rw-r--r-- 1 root root      186 Jun 21 05:48 user02_pod.yml_org
-rw-r--r-- 1 root root      311 Jun 21 06:28 busy_box.yml
-rw-r--r-- 1 root root      203 Jun 21 06:31 user02_pod.yml
root@labs--311032102:/home/project/user/user02# vi busy_box.yml
vi busy_box.yml

root@labs--311032102:/home/project/user/user02# 

root@labs--311032102:/home/project/user/user02# kubectl create -f busy_box.yml
kubectl create -f busy_box.yml
Error from server (AlreadyExists): error when creating "busy_box.yml": pods "busybox" already exists
root@labs--311032102:/home/project/user/user02# kubectl create -f busy_box.yml^C
root@labs--311032102:/home/project/user/user02# kubectl delete pod busybox
kubectl delete pod busybox
pod "busybox" deleted
root@labs--311032102:/home/project/user/user02# kubectl create -f busy_box.yml
kubectl create -f busy_box.yml
pod/busybox created
root@labs--311032102:/home/project/user/user02# watch kubectl get pod
watch kubectl get pod
root@labs--311032102:/home/project/user/user02# 


##

root@labs--311032102:/home/project/user/user02# kubectl get pvc --> 볼륨 
kubectl get pvc
NAME                        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
data-my-kafka-0             Bound    pvc-51bd31bf-1c26-4eac-b309-bdd35865d211   2Gi        RWO            gp2            72m
data-my-kafka-zookeeper-0   Bound    pvc-30d9e6a6-edbf-4870-bc81-ccec551897b7   2Gi        RWO            gp2            72m

root@labs--311032102:/home/project/user/user02# kubectl get pvc
kubectl get pvc
NAME                        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
data-my-kafka-0             Bound    pvc-51bd31bf-1c26-4eac-b309-bdd35865d211   2Gi        RWO            gp2            72m
data-my-kafka-zookeeper-0   Bound    pvc-30d9e6a6-edbf-4870-bc81-ccec551897b7   2Gi        RWO            gp2            72m
root@labs--311032102:/home/project/user/user02# kubectl get pv
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                               STORAGECLASS   REASON   AGE
pvc-30d9e6a6-edbf-4870-bc81-ccec551897b7   2Gi        RWO            Delete           Bound    default/data-my-kafka-zookeeper-0   gp2                     72m
pvc-51bd31bf-1c26-4eac-b309-bdd35865d211   2Gi        RWO            Delete           Bound    default/data-my-kafka-0             gp2                     72m
root@labs--311032102:/home/project/user/user02# kubectl get sc  <-- 스토리지클래스 (아마존)
kubectl get sc
NAME            PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE ==>아마존제공하는gp2 를 사용
gp2 (default)   kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  26h
root@labs--311032102:/home/project/user/user02# 


root@labs--311032102:/home/project/user/user02# kubectl get sc
kubectl get sc
NAME            PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
gp2 (default)   kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  26h
root@labs--311032102:/home/project/user/user02# kubectl get sc gp2 -o yaml
kubectl get sc gp2 -o yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"name":"gp2"},"parameters":{"fsType":"ext4","type":"gp2"},"provisioner":"kubernetes.io/aws-ebs","volumeBindingMode":"WaitForFirstConsumer"}
    storageclass.kubernetes.io/is-default-class: "true"
  creationTimestamp: "2022-06-20T04:32:31Z"
  managedFields:
  - apiVersion: storage.k8s.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:kubectl.kubernetes.io/last-applied-configuration: {}
          f:storageclass.kubernetes.io/is-default-class: {}
      f:parameters:
        .: {}
        f:fsType: {}
        f:type: {}
      f:provisioner: {}
      f:reclaimPolicy: {}
      f:volumeBindingMode: {}
    manager: kubectl-client-side-apply
    operation: Update
    time: "2022-06-20T04:32:31Z"
  name: gp2
  resourceVersion: "197"
  selfLink: /apis/storage.k8s.io/v1/storageclasses/gp2
  uid: b43f839d-6fd9-437a-b1c0-aa325f5641f2
parameters:
  fsType: ext4           --> 리눅스파일타입
  type: gp2
provisioner: kubernetes.io/aws-ebs
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer   --> POD 접속했을때만 기동
root@labs--311032102:/home/project/user/user02# 


root@labs--311032102:/home/project/user/user02# cat userapp-pvc.yml
cat userapp-pvc.yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim1
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: gp2
  resources:
    requests:
      storage: 30Mi

root@labs--311032102:/home/project/user/user02# kubectl create -f userapp-pvc.yml
kubectl create -f userapp-pvc.yml
persistentvolumeclaim/claim1 created
root@labs--311032102:/home/project/user/user02# kubectlge pvc
kubectlge pvc
bash: kubectlge: command not found
root@labs--311032102:/home/project/user/user02# kubectl get pvc
kubectl get pvc
NAME                        STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
claim1                      Pending                                                                        gp2            15s
data-my-kafka-0             Bound     pvc-51bd31bf-1c26-4eac-b309-bdd35865d211   2Gi        RWO            gp2            88m
data-my-kafka-zookeeper-0   Bound     pvc-30d9e6a6-edbf-4870-bc81-ccec551897b7   2Gi        RWO            gp2            88m


root@labs--311032102:/home/project/user/user02# cat user-data.yml
cat user-data.yml
apiVersion: v1
kind: Pod
metadata:
  name: user-data
spec:
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: claim1
  containers:
    - name: user-data
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: task-pv-storage

root@labs--311032102:/home/project/user/user02# kubectl get pvc
kubectl get pvc
NAME                        STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
claim1                      Pending                                                                        gp2            8m26s
data-my-kafka-0             Bound     pvc-51bd31bf-1c26-4eac-b309-bdd35865d211   2Gi        RWO            gp2            96m
data-my-kafka-zookeeper-0   Bound     pvc-30d9e6a6-edbf-4870-bc81-ccec551897b7   2Gi        RWO            gp2            96m
root@labs--311032102:/home/project/user/user02# ls -lrt
ls -lrt
total 42180
-rw-r--r-- 1 root root     4115 Dec  8  2021 components.yaml
-rw-r--r-- 1 root root     4548 Jun 21 00:45 azure-pipelines.yml
-rw-r--r-- 1 root root     2838 Jun 21 00:45 cloudbuild.yaml
-rw-r--r-- 1 root root      295 Jun 21 00:45 Dockerfile
drwxr-xr-x 2 root root     4096 Jun 21 00:45 kubernetes
-rw-r--r-- 1 root root     3549 Jun 21 00:45 pom.xml
-rw-r--r-- 1 root root     1617 Jun 21 00:45 README.md
drwxr-xr-x 4 root root     4096 Jun 21 00:45 src
drwxr-xr-x 9 root root     4096 Jun 21 00:57 target
-rw-r--r-- 1 root root 43003904 Jun 21 04:22 kubectl
-rw-r--r-- 1 root root   112191 Jun 21 05:16 kafka-18.0.0.tgz
drwxr-xr-x 4 root root     4096 Jun 21 05:25 kafka
-rw-r--r-- 1 root root      186 Jun 21 05:48 user02_pod.yml_org
-rw-r--r-- 1 root root      203 Jun 21 06:31 user02_pod.yml
-rw-r--r-- 1 root root      197 Jun 21 06:37 busy_box.yml
-rw-r--r-- 1 root root      180 Jun 21 06:53 userapp-pvc.yml
-rw-r--r-- 1 root root      377 Jun 21 07:02 user-data.yml
root@labs--311032102:/home/project/user/user02# kubectl create -f user-data.yml
kubectl create -f user-data.yml
pod/user-data created
root@labs--311032102:/home/project/user/user02# kubectl get pvc      
kubectl get pvc
NAME                        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
claim1                      Bound    pvc-47220cfa-9a7f-4815-8d80-a643c7100af5   1Gi        RWO            gp2            9m8s       <=== 1G단위로함.
data-my-kafka-0             Bound    pvc-51bd31bf-1c26-4eac-b309-bdd35865d211   2Gi        RWO            gp2            97m
data-my-kafka-zookeeper-0   Bound    pvc-30d9e6a6-edbf-4870-bc81-ccec551897b7   2Gi        RWO            gp2            97m
root@labs--311032102:/home/project/user/user02# kubectl get pv   <=== PVC 만들면자동으로 생성됨.
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                               STORAGECLASS   REASON   AGE
pvc-30d9e6a6-edbf-4870-bc81-ccec551897b7   2Gi        RWO            Delete           Bound    default/data-my-kafka-zookeeper-0   gp2                     97m
pvc-47220cfa-9a7f-4815-8d80-a643c7100af5   1Gi        RWO            Delete           Bound    default/claim1                      gp2                     21s
pvc-51bd31bf-1c26-4eac-b309-bdd35865d211   2Gi        RWO            Delete           Bound    default/data-my-kafka-0             gp2                     97m
root@labs--311032102:/home/project/user/user02# 

root@labs--311032102:/home/project/user/user02# kubectl exec -it user-data -- bash
root@user-data:/# df-k
bash: df-k: command not found
root@user-data:/# df -k
Filesystem     1K-blocks    Used Available Use% Mounted on
overlay         83873772 3951280  79922492   5% /
tmpfs              65536       0     65536   0% /dev
tmpfs            1982704       0   1982704   0% /sys/fs/cgroup
/dev/nvme0n1p1  83873772 3951280  79922492   5% /etc/hosts
shm                65536       0     65536   0% /dev/shm
/dev/nvme2n1      996780      24    980372   1% /usr/share/nginx/html
tmpfs            1982704      12   1982692   1% /run/secrets/kubernetes.io/serviceaccount
tmpfs            1982704       0   1982704   0% /proc/acpi
tmpfs            1982704       0   1982704   0% /sys/firmware
root@user-data:/# 

root@user-data:/usr/share/nginx/html# touch sss
root@user-data:/usr/share/nginx/html# ls -lrt
total 16
drwx------ 2 root root 16384 Jun 21 07:03 lost+found
-rw-r--r-- 1 root root     0 Jun 21 07:05 sss
root@user-data:/usr/share/nginx/html# vi sss     <==== 파일을 생성함.

##  교육내용 ## 
0. KAFKA 설치함.
1. 네트워크 설정함.
2. 스토리지 설정함.   --> CPU/메모리할당함.

===> yml 파일이너무많아져서..... ==>헬름이만들어짐

## PVC 삭제를요청함.
root@labs--311032102:/home/project/user/user02# kubectl delete -f user-data.yml     <== PVC를 삭제함.
pod "user-data" deleted
root@labs--311032102:/home/project/user/user02# kubectl get pvc
NAME                        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
claim1                      Bound    pvc-47220cfa-9a7f-4815-8d80-a643c7100af5   1Gi        RWO            gp2            12m                   <== 실제 claim1 은삭제되지않음 (따로 지워야함)
data-my-kafka-0             Bound    pvc-51bd31bf-1c26-4eac-b309-bdd35865d211   2Gi        RWO            gp2            100m
data-my-kafka-zookeeper-0   Bound    pvc-30d9e6a6-edbf-4870-bc81-ccec551897b7   2Gi        RWO            gp2            100m
root@labs--311032102:/home/project/user/user02# kubectl exec -it siege -- bash^C
root@labs--311032102:/home/project/user/user02# kubectl create -f user-data.yml                                                                    <== 재생성함.
pod/user-data created
root@labs--311032102:/home/project/user/user02# kubectl exec -it user-data -- bash
root@user-data:/# 
root@user-data:/# 
root@user-data:/# df-k
bash: df-k: command not found
root@user-data:/# df -k
Filesystem     1K-blocks    Used Available Use% Mounted on
overlay         83873772 3951512  79922260   5% /
tmpfs              65536       0     65536   0% /dev
tmpfs            1982704       0   1982704   0% /sys/fs/cgroup
/dev/nvme0n1p1  83873772 3951512  79922260   5% /etc/hosts
shm                65536       0     65536   0% /dev/shm
/dev/nvme2n1      996780      24    980372   1% /usr/share/nginx/html                                                                                   <== 경로확인함.  밑에 sss 파일로 있음.확인함.
tmpfs            1982704      12   1982692   1% /run/secrets/kubernetes.io/serviceaccount
tmpfs            1982704       0   1982704   0% /proc/acpi
tmpfs            1982704       0   1982704   0% /sys/firmware
root@user-data:/# cd /usr/share/nginx/html
root@user-data:/usr/share/nginx/html# ls
lost+found  sss
root@user-data:/usr/share/nginx/html# ls-lrt
bash: ls-lrt: command not found
root@user-data:/usr/share/nginx/html# ls -lrt
total 16
drwx------ 2 root root 16384 Jun 21 07:03 lost+found
-rw-r--r-- 1 root root     0 Jun 21 07:05 sss
root@user-data:/usr/share/nginx/html# 

## RDB , mysql 기동
root@labs--311032102:/home/project/user/user02# kubectl create ns db
kubectl create ns db
namespace/db created
root@labs--311032102:/home/project/user/user02# kubectl get ns
kubectl get ns
NAME              STATUS   AGE
db                Active   18s
default           Active   27h
kube-node-lease   Active   27h
kube-public       Active   27h
kube-system       Active   27h
root@labs--311032102:/home/project/user/user02# helm repo list
helm repo list
NAME    URL                               
bitnami https://charts.bitnami.com/bitnami
root@labs--311032102:/home/project/user/user02# helm search
helm search
\
Search provides the ability to search for Helm charts in the various places
they can be stored including the Artifact Hub and repositories you have added.
Use search subcommands to search different locations for charts.

Usage:
  helm search [command]

Available Commands:
  hub         search for charts in the Artifact Hub or your own hub instance
  repo        search repositories for a keyword in charts

Flags:
  -h, --help   help for search

Global Flags:
      --debug                       enable verbose output
      --kube-apiserver string       the address and the port for the Kubernetes API server
      --kube-as-group stringArray   group to impersonate for the operation, this flag can be repeated to specify multiple groups.
      --kube-as-user string         username to impersonate for the operation
      --kube-ca-file string         the certificate authority file for the Kubernetes API server connection
      --kube-context string         name of the kubeconfig context to use
      --kube-token string           bearer token used for authentication
      --kubeconfig string           path to the kubeconfig file
  -n, --namespace string            namespace scope for this request
      --registry-config string      path to the registry config file (default "/root/.config/helm/registry/config.json")
      --repository-cache string     path to the file containing cached repository indexes (default "/root/.cache/helm/repository")
      --repository-config string    path to the file containing repository names and URLs (default "/root/.config/helm/repositories.yaml")

Use "helm search [command] --help" for more information about a command.
root@labs--311032102:/home/project/user/user02# helm fetch bitnami/mysql
helm fetch bitnami/mysql
root@labs--311032102:/home/project/user/user02# ls -lrt
ls -lrt
total 42224
-rw-r--r-- 1 root root     4115 Dec  8  2021 components.yaml
-rw-r--r-- 1 root root     4548 Jun 21 00:45 azure-pipelines.yml
-rw-r--r-- 1 root root     2838 Jun 21 00:45 cloudbuild.yaml
-rw-r--r-- 1 root root      295 Jun 21 00:45 Dockerfile
drwxr-xr-x 2 root root     4096 Jun 21 00:45 kubernetes
-rw-r--r-- 1 root root     3549 Jun 21 00:45 pom.xml
-rw-r--r-- 1 root root     1617 Jun 21 00:45 README.md
drwxr-xr-x 4 root root     4096 Jun 21 00:45 src
drwxr-xr-x 9 root root     4096 Jun 21 00:57 target
-rw-r--r-- 1 root root 43003904 Jun 21 04:22 kubectl
-rw-r--r-- 1 root root   112191 Jun 21 05:16 kafka-18.0.0.tgz
drwxr-xr-x 4 root root     4096 Jun 21 05:25 kafka
-rw-r--r-- 1 root root      186 Jun 21 05:48 user02_pod.yml_org
-rw-r--r-- 1 root root      203 Jun 21 06:31 user02_pod.yml
-rw-r--r-- 1 root root      197 Jun 21 06:37 busy_box.yml
-rw-r--r-- 1 root root      180 Jun 21 06:53 userapp-pvc.yml
-rw-r--r-- 1 root root      377 Jun 21 07:02 user-data.yml
-rw-r--r-- 1 root root    44174 Jun 21 07:39 mysql-9.1.7.tgz


root@labs--311032102:/home/project/user/user02/mysql# cp values.yaml values.yaml_org
cp values.yaml values.yaml_org
root@labs--311032102:/home/project/user/user02/mysql# 

root@labs--311032102:/home/project/user/user02/mysql# 

root@labs--311032102:/home/project/user/user02/mysql# 

root@labs--311032102:/home/project/user/user02/mysql# 

root@labs--311032102:/home/project/user/user02/mysql# pwd
pwd
/home/project/user/user02/mysql
root@labs--311032102:/home/project/user/user02/mysql# 

root@labs--311032102:/home/project/user/user02/mysql# 

== 패스워드 , 용량 변경하고 
== 설치함...
root@labs--311032102:/home/project/user/user02/mysql# helm install mysql . --namespace db
helm install mysql . --namespace db
NAME: mysql
LAST DEPLOYED: Tue Jun 21 07:46:26 2022
NAMESPACE: db
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: mysql
CHART VERSION: 9.1.7
APP VERSION: 8.0.29

** Please be patient while the chart is being deployed **

Tip:

  Watch the deployment status using the command: kubectl get pods -w --namespace db

Services:

  echo Primary: mysql.db.svc.cluster.local:3306

Execute the following to get the administrator credentials:

  echo Username: root
  MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace db mysql -o jsonpath="{.data.mysql-root-password}" | base64 -d)

To connect to your database:

  1. Run a pod that you can use as a client:

      kubectl run mysql-client --rm --tty -i --restart='Never' --image  docker.io/bitnami/mysql:8.0.29-debian-11-r3 --namespace db --env MYSQL_ROOT_PASSWORD=$MYSQL_ROOT_PASSWORD --command -- bash

  2. To connect to primary service (read/write):

      mysql -h mysql.db.svc.cluster.local -uroot -p"$MYSQL_ROOT_PASSWORD"
root@labs--311032102:/home/project/user/user02/mysql# 

root@labs--311032102:/home/project/user/user02/mysql# kubectl get pod -n db
kubectl get pod -n db
NAME      READY   STATUS    RESTARTS   AGE
mysql-0   1/1     Running   0          63s

## 

root@labs--311032102:/home/project/user/user02/mysql# MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace db mysql -o jsonpath="{.data.mysql-root-password}" | base64 -d)
MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace db mysql -o jsonpath="{.data.mysql-root-password}" | base64 -d)
root@labs--311032102:/home/project/user/user02/mysql# echo $MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace db mysql -o jsonpath="{.data.mysql-root-password}" | base64 -d)^C
root@labs--311032102:/home/project/user/user02/mysql# echo $MYSQL_ROOT_PASSWORD
echo $MYSQL_ROOT_PASSWORD
root

root@labs--311032102:/home/project/user/user02/mysql# kubectl get svc -n db
kubectl get svc -n db
NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
mysql            ClusterIP   10.100.211.186   <none>        3306/TCP   4m28s
mysql-headless   ClusterIP   None             <none>        3306/TCP   4m28s      ==> state full set 사용할때만 사용함.


root@labs--311032102:/home/project/user/user02# kubectl exec -it netcom -- bash
utils@netcom:~$ nslookup mysql.db
Server:         10.100.0.10
Address:        10.100.0.10#53

Name:   mysql.db.svc.cluster.local
Address: 10.100.211.186


root@labs--311032102:/home/project/user/user02/mysql# kubectl get pvc        <===여기에는 안보임.
kubectl get pvc
NAME                        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
claim1                      Bound    pvc-47220cfa-9a7f-4815-8d80-a643c7100af5   1Gi        RWO            gp2            59m
data-my-kafka-0             Bound    pvc-51bd31bf-1c26-4eac-b309-bdd35865d211   2Gi        RWO            gp2            147m
data-my-kafka-zookeeper-0   Bound    pvc-30d9e6a6-edbf-4870-bc81-ccec551897b7   2Gi        RWO            gp2            147m
root@labs--311032102:/home/project/user/user02/mysql# kubectl get pv          <=== 클러스터롤 
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                               STORAGECLASS   REASON   AGE
pvc-30d9e6a6-edbf-4870-bc81-ccec551897b7   2Gi        RWO            Delete           Bound    default/data-my-kafka-zookeeper-0   gp2                     147m
pvc-47220cfa-9a7f-4815-8d80-a643c7100af5   1Gi        RWO            Delete           Bound    default/claim1                      gp2                     50m
pvc-51bd31bf-1c26-4eac-b309-bdd35865d211   2Gi        RWO            Delete           Bound    default/data-my-kafka-0             gp2                     147m
pvc-e4cd1f8e-f402-41af-bcb6-edd670bb0615   2Gi        RWO            Delete           Bound    db/data-mysql-0                     gp2                     7m19s
root@labs--311032102:/home/project/user/user02/mysql# 

root@labs--311032102:/home/project/user/user02/mysql# kubectl get pvc -n db
kubectl get pvc -n db
NAME           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
data-mysql-0   Bound    pvc-e4cd1f8e-f402-41af-bcb6-edd670bb0615   2Gi        RWO            gp2            9m
root@labs--311032102:/home/project/user/user02/mysql# 

## 접속을 해보자.

root@labs--311032102:/home/project/user/user02/mysql# kubectl get pod -n db
kubectl get pod -n db
NAME      READY   STATUS    RESTARTS   AGE
mysql-0   1/1     Running   0          10m
root@labs--311032102:/home/project/user/user02/mysql# kubectl -n db exec -it mysql-0 -- bash

I have no name!@mysql-0:/$ mysql -h localhost -u root -p
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 129
Server version: 8.0.29 Source distribution

Copyright (c) 2000, 2022, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| my_database        |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
5 rows in set (0.01 sec)

mysql> 

## 3일차
1. 무정지 배포 ( readinessProbe )

root@labs--311032102:/home/project/ops-readiness# cat order-noredi.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: order
  labels:
    app: order                           <-- 디플로이 명
spec:
  replicas: 1                              <-- 레플리카
  selector:
    matchLabels:
      app: order
  template:
    metadata:
      labels:
        app: order
    spec:                                  <-- POD 정보 
      containers:
        - name: order
          image: jinyoung/order:stable
          ports:
            - containerPort: 8080
root@labs--311032102:/home/project/ops-readiness# cat order-svc.yaml
apiVersion: "v1"
kind: "Service"
metadata: 
  name: "order"
  labels: 
    app: "order"
spec: 
  ports: 
    - 
      port: 8080
      targetPort: 8080
  selector: 
    app: "order"
  type: "ClusterIP"
root@labs--311032102:/home/project/ops-readiness# 




root@labs--311032102:/home/project/ops-readiness# kubectl create -f order-noredi.yaml
deployment.apps/order created

root@labs--311032102:/home/project/ops-readiness# kubectl get all
NAME                                  READY   STATUS             RESTARTS   AGE
pod/order-698987b774-l5wgg            1/1     Running            0          22s                             <==== 생성됨

NAME                                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/order            1/1     1            1           22s                                              <==== 생성됨

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/order-698987b774            1         1         1       22s                                      <==== 생성됨 

NAME                                  READY   AGE
statefulset.apps/my-kafka             1/1     19h
statefulset.apps/my-kafka-zookeeper   1/1     19h



root@labs--311032102:/home/project/ops-readiness# kubectl create -f  order-svc.yaml
service/order created

root@labs--311032102:/home/project/ops-readiness# kubectl get all
NAME                                  READY   STATUS             RESTARTS   AGE
pod/order-698987b774-l5wgg            1/1     Running            0          2m4s

NAME                                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
service/order                         ClusterIP   10.100.91.188    <none>        8080/TCP                     13s             <==== 생성됨

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/order            1/1     1            1           2m4s

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/order-698987b774            1         1         1       2m4s

NAME                                  READY   AGE
statefulset.apps/my-kafka             1/1     19h
statefulset.apps/my-kafka-zookeeper   1/1     19h
root@labs--311032102:/home/project/ops-readiness# 



## order-noredi.yaml) 디플로이 설정을 변경한다.
root@labs--311032102:/home/project/ops-readiness# cat order-noredi.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: order
  labels:
    app: order
spec:
  replicas: 1
  selector:
    matchLabels:
      app: order
  template:
    metadata:
      labels:
        app: order
    spec:
      containers:
        - name: order
          image: jinyoung/order:canary                 <== stable -> canary 로 변경함. (kanary) 하면 오류발생. (버전이 없기때문에...당연함.)
          ports:
            - containerPort: 8080
          readinessProbe:    # 이부분 (튜닝을 해야함) , 두개의 마이크로 서비스는 Init 컨테이너를 여러개로 해서 체크하는 로직을 집어넣으면 됨.
            httpGet:
              path: '/orders'
              port: 8080
            initialDelaySeconds: 10
            timeoutSeconds: 2
            periodSeconds: 5
            failureThreshold: 10

#### 시즈로 들어가서 거래 전송한다. 
kubectl exec -it siege -- /bin/bash
siege -c1 -t60S -v http://order:8080/orders --delay=1S

---------------------------------------------------------------------------------------------
Every 2.0s: kubectl get pod                                                                            labs--311032102: Wed Jun 22 01:05:56 2022

NAME                              READY   STATUS             RESTARTS   AGE
busybox                           0/1     CrashLoopBackOff   220        18h
order-58f469bfd8-w4xgl            1/1     Running            0          2m22s
siege                             1/1     Running            0          41h
siege-55b7f7d5fc-vwqk6            1/1     Running            0          22h
user-data                         1/1     Running            0          17h

####


root@labs--311032102:/home/project/ops-readiness# kubectl describe pod order-698987b774-l5wgg
Name:         order-698987b774-l5wgg
Namespace:    default
Priority:     0
Node:         ip-192-168-29-79.ap-northeast-1.compute.internal/192.168.29.79
Start Time:   Wed, 22 Jun 2022 00:48:13 +0000
Labels:       app=order
              pod-template-hash=698987b774
Annotations:  kubernetes.io/psp: eks.privileged
Status:       Running
IP:           192.168.9.201
IPs:
  IP:           192.168.9.201
Controlled By:  ReplicaSet/order-698987b774
Containers:
  order:
    Container ID:   docker://cfeaf628c35f21a91290e32bf77abacf5d0f19949bc9b051caa1a0fea9d284dd
    Image:          jinyoung/order:stable
    Image ID:       docker-pullable://jinyoung/order@sha256:081b09d34cc5b00cf22e6a6d647081f315c3ddfd239c5fa56ec64510b9131f4a
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 22 Jun 2022 00:48:14 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zkb74 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-zkb74:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-zkb74
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From                                                       Message
  ----    ------     ----  ----                                                       -------
  Normal  Scheduled  12m   default-scheduler                                          Successfully assigned default/order-698987b774-l5wgg to ip-192-168-29-79.ap-northeast-1.compute.internal
  Normal  Pulled     12m   kubelet, ip-192-168-29-79.ap-northeast-1.compute.internal  Container image "jinyoung/order:stable" already present on machine
  Normal  Created    12m   kubelet, ip-192-168-29-79.ap-northeast-1.compute.internal  Created container order
  Normal  Started    12m   kubelet, ip-192-168-29-79.ap-northeast-1.compute.internal  Started container order
root@labs--311032102:/home/project/ops-readiness# ls
order-noredi.yaml  order-svc.yaml  shopmall
root@labs--311032102:/home/project/ops-readiness# vi order-noredi.yaml
root@labs--311032102:/home/project/ops-readiness# 
root@labs--311032102:/home/project/ops-readiness# 
root@labs--311032102:/home/project/ops-readiness# 
root@labs--311032102:/home/project/ops-readiness# kubectl apply -f order-noredi.yaml
Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply
deployment.apps/order configured
root@labs--311032102:/home/project/ops-readiness# kubectl apply -f order-noredi.yaml
deployment.apps/order configured
root@labs--311032102:/home/project/ops-readiness# kubectl describe pod order-58f469bfd8-w4xgl
Name:         order-58f469bfd8-w4xgl
Namespace:    default
Priority:     0
Node:         ip-192-168-45-77.ap-northeast-1.compute.internal/192.168.45.77
Start Time:   Wed, 22 Jun 2022 01:03:36 +0000
Labels:       app=order
              pod-template-hash=58f469bfd8
Annotations:  kubernetes.io/psp: eks.privileged
Status:       Running
IP:           192.168.37.139
IPs:
  IP:           192.168.37.139
Controlled By:  ReplicaSet/order-58f469bfd8
Containers:
  order:
    Container ID:   docker://9f033a042af30196474d6c6bf3503327db79e29bd619feb5716f42bbe55c7299
    Image:          jinyoung/order:canary
    Image ID:       docker-pullable://jinyoung/order@sha256:081b09d34cc5b00cf22e6a6d647081f315c3ddfd239c5fa56ec64510b9131f4a
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 22 Jun 2022 01:03:43 +0000
    Ready:          True
    Restart Count:  0
    Readiness:      http-get http://:8080/orders delay=10s timeout=2s period=5s #success=1 #failure=10
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zkb74 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-zkb74:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-zkb74
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From                                                       Message
  ----     ------     ----               ----                                                       -------
  Normal   Scheduled  93s                default-scheduler                                          Successfully assigned default/order-58f469bfd8-w4xgl to ip-192-168-45-77.ap-northeast-1.compute.internal
  Normal   Pulling    92s                kubelet, ip-192-168-45-77.ap-northeast-1.compute.internal  Pulling image "jinyoung/order:canary"
  Normal   Pulled     86s                kubelet, ip-192-168-45-77.ap-northeast-1.compute.internal  Successfully pulled image "jinyoung/order:canary" in 6.223520853s
  Normal   Created    86s                kubelet, ip-192-168-45-77.ap-northeast-1.compute.internal  Created container order
  Normal   Started    86s                kubelet, ip-192-168-45-77.ap-northeast-1.compute.internal  Started container order
  Warning  Unhealthy  64s (x3 over 74s)  kubelet, ip-192-168-45-77.ap-northeast-1.compute.internal  Readiness probe failed: Get "http://192.168.37.139:8080/orders": dial tcp 192.168.37.139:8080: connect: connection refused
root@labs--311032102:/home/project/ops-readiness# 

root@labs--311032102:/home/project/ops-liveness# kubectl get all | grep order
pod/order-58f469bfd8-w4xgl            1/1     Running            0          11m
pod/user02-order-5fd6ff986d-4q9q6     1/1     Running            0          42h
service/order                         ClusterIP   10.100.91.188    <none>        8080/TCP                     24m
deployment.apps/order            1/1     1            1           26m
deployment.apps/user02-order     1/1     1            1           42h
replicaset.apps/order-58f469bfd8            1         1         1       11m
replicaset.apps/order-698987b774            0         0         0       26m
replicaset.apps/order-77f444dcbd            0         0         0       13m
replicaset.apps/user02-order-5fd6ff986d     1         1         1       42h
root@labs--311032102:/home/project/ops-liveness# kubectl delete deployment.apps/order
deployment.apps "order" deleted
root@labs--311032102:/home/project/ops-liveness# kubectl get all | grep order
pod/order-58f469bfd8-w4xgl            0/1     Terminating        0          11m
pod/user02-order-5fd6ff986d-4q9q6     1/1     Running            0          42h
service/order                         ClusterIP   10.100.91.188    <none>        8080/TCP                     25m
deployment.apps/user02-order     1/1     1            1           42h
replicaset.apps/user02-order-5fd6ff986d     1         1         1       42h
root@labs--311032102:/home/project/ops-liveness# kubectl get all | grep order
pod/user02-order-5fd6ff986d-4q9q6     1/1     Running            0          42h
service/order                         ClusterIP   10.100.91.188    <none>        8080/TCP                     25m
deployment.apps/user02-order     1/1     1            1           42h
replicaset.apps/user02-order-5fd6ff986d     1         1         1       42h
root@labs--311032102:/home/project/ops-liveness# kubectl delete service/order
service "order" deleted
root@labs--311032102:/home/project/ops-liveness# kubectl get all | grep ord
pod/user02-order-5fd6ff986d-4q9q6     1/1     Running            0          42h
deployment.apps/user02-order     1/1     1            1           42h
replicaset.apps/user02-order-5fd6ff986d     1         1         1       42h
root@labs--311032102:/home/project/ops-liveness# kubectl get all | grep order
pod/user02-order-5fd6ff986d-4q9q6     1/1     Running            0          42h
deployment.apps/user02-order     1/1     1            1           42h
replicaset.apps/user02-order-5fd6ff986d     1      

2.셀프힐링 실습 (livenessProbe) 설정  (느려지는 경우 처리할수 있음 -> ScaleOut 대상아님)

root@labs--311032102:/home/project/ops-liveness/dp-cqrs/order#  mvn package -B
실행후 

root@labs--311032102:/home/project/ops-liveness/dp-cqrs/order# 
root@labs--311032102:/home/project/ops-liveness/dp-cqrs/order# 
root@labs--311032102:/home/project/ops-liveness/dp-cqrs/order# docker build -t gorapadd/order:memleak .
Sending build context to Docker daemon 59.77 MB
Step 1/4 : FROM openjdk:8u212-jdk-alpine
 ---> a3562aa0b991
Step 2/4 : COPY target/*SNAPSHOT.jar app.jar
 ---> 44231564dd90
Step 3/4 : EXPOSE 8080
 ---> Running in 624cc32387e0
Removing intermediate container 624cc32387e0
 ---> 43f69c41d7ea
Step 4/4 : ENTRYPOINT ["java","-Xmx400M","-Djava.security.egd=file:/dev/./urandom","-jar","/app.jar","--spring.profiles.active=docker"]
 ---> Running in 652843c9a91f
Removing intermediate container 652843c9a91f
 ---> b730ec011f1c
Successfully built b730ec011f1c
Successfully tagged gorapadd/order:memleak
root@labs--311032102:/home/project/ops-liveness/dp-cqrs/order# docker push -t gorapadd/order:memleak
unknown shorthand flag: 't' in -t
See 'docker push --help'.
root@labs--311032102:/home/project/ops-liveness/dp-cqrs/order# docker push  gorapadd/order:memleak
The push refers to repository [docker.io/gorapadd/order]
49bd6a17c195: Pushed 
ceaf9e1ebef5: Layer already exists 
9b9b7f3d56a0: Layer already exists 
f1b5933fe4b5: Layer already exists 
memleak: digest: sha256:1db014319ca6e425e24a4243476b6476638aef7aa5e4ade9b9102c6e55f5ddfb size: 1159
root@labs--311032102:/home/project/ops-liveness/dp-cqrs/order# 
root@labs--311032102:/home/project/ops-liveness/dp-cqrs/order# 
root@labs--311032102:/home/project/ops-liveness/dp-cqrs/order# ls
Dockerfile  azure-pipelines.yml  cloudbuild.yaml  kubernetes  pom.xml  src  target
root@labs--311032102:/home/project/ops-liveness/dp-cqrs/order# cd ku*
root@labs--311032102:/home/project/ops-liveness/dp-cqrs/order/kubernetes# ls
deployment.yml  service.yaml
root@labs--311032102:/home/project/ops-liveness/dp-cqrs/order/kubernetes# vi de*.yml

==>  gorapadd/order:memleak 수정 


root@labs--311032102:/home/project/ops-liveness/dp-cqrs/order/kubernetes# kubectl apply -f deployment.yml
deployment.apps/order created
root@labs--311032102:/home/project/ops-liveness/dp-cqrs/order/kubernetes# kubectl describe pod order
Name:         order-679677687b-w5p7r
Namespace:    default
Priority:     0
Node:         ip-192-168-29-79.ap-northeast-1.compute.internal/192.168.29.79
Start Time:   Wed, 22 Jun 2022 01:24:29 +0000
Labels:       app=order
              pod-template-hash=679677687b
Annotations:  kubernetes.io/psp: eks.privileged
Status:       Running
IP:           192.168.21.74
IPs:
  IP:           192.168.21.74
Controlled By:  ReplicaSet/order-679677687b
Containers:
  order:
    Container ID:   docker://8b277bfc0a4f22b27fda7612b638b90071a26113d4d9d4735135bc68762b0e5f
    Image:          gorapadd/order:memleak
    Image ID:       docker-pullable://gorapadd/order@sha256:1db014319ca6e425e24a4243476b6476638aef7aa5e4ade9b9102c6e55f5ddfb
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 22 Jun 2022 01:24:38 +0000
    Ready:          False
    Restart Count:  0
    Liveness:       http-get http://:8080/actuator/health delay=120s timeout=2s period=5s #success=1 #failure=5
    Readiness:      http-get http://:8080/actuator/health delay=10s timeout=2s period=5s #success=1 #failure=10
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zkb74 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  default-token-zkb74:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-zkb74
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From                                                       Message
  ----     ------     ----               ----                                                       -------
  Normal   Scheduled  58s                default-scheduler                                          Successfully assigned default/order-679677687b-w5p7r to ip-192-168-29-79.ap-northeast-1.compute.internal
  Normal   Pulling    57s                kubelet, ip-192-168-29-79.ap-northeast-1.compute.internal  Pulling image "gorapadd/order:memleak"
  Normal   Pulled     49s                kubelet, ip-192-168-29-79.ap-northeast-1.compute.internal  Successfully pulled image "gorapadd/order:memleak" in 7.696831011s
  Normal   Created    49s                kubelet, ip-192-168-29-79.ap-northeast-1.compute.internal  Created container order
  Normal   Started    49s                kubelet, ip-192-168-29-79.ap-northeast-1.compute.internal  Started container order
  Warning  Unhealthy  33s (x2 over 38s)  kubelet, ip-192-168-29-79.ap-northeast-1.compute.internal  Readiness probe failed: Get "http://192.168.21.74:8080/actuator/health": dial tcp 192.168.21.74:8080: connect: connection refused
  Warning  Unhealthy  2s (x6 over 27s)   kubelet, ip-192-168-29-79.ap-northeast-1.compute.internal  Readiness probe failed: HTTP probe failed with statuscode: 503

root@labs--311032102:/home/project/ops-liveness/dp-cqrs/order/kubernetes# vi de*.yml

==> readinessProbe 설정을 지워서 진행함.  


root@labs--311032102:/home/project/ops-liveness/dp-cqrs/order/kubernetes# kubectl apply -f deployment.yml
deployment.apps/order configured
root@labs--311032102:/home/project/ops-liveness/dp-cqrs/order/kubernetes# kubectl describe pod order
Name:         order-78df866548-bp4xj
Namespace:    default
Priority:     0
Node:         ip-192-168-29-79.ap-northeast-1.compute.internal/192.168.29.79
Start Time:   Wed, 22 Jun 2022 01:26:21 +0000
Labels:       app=order
              pod-template-hash=78df866548
Annotations:  kubernetes.io/psp: eks.privileged
Status:       Running
IP:           192.168.0.88
IPs:
  IP:           192.168.0.88
Controlled By:  ReplicaSet/order-78df866548
Containers:
  order:
    Container ID:   docker://d2b0bac5a1ab7bd08010be8cd515f8aec88ed74127d38d4266891f727ced7af2
    Image:          gorapadd/order:memleak
    Image ID:       docker-pullable://gorapadd/order@sha256:1db014319ca6e425e24a4243476b6476638aef7aa5e4ade9b9102c6e55f5ddfb
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 22 Jun 2022 01:26:22 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       http-get http://:8080/actuator/health delay=120s timeout=2s period=5s #success=1 #failure=5
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zkb74 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-zkb74:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-zkb74
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From                                                       Message
  ----    ------     ----  ----                                                       -------
  Normal  Scheduled  17s   default-scheduler                                          Successfully assigned default/order-78df866548-bp4xj to ip-192-168-29-79.ap-northeast-1.compute.internal
  Normal  Pulled     16s   kubelet, ip-192-168-29-79.ap-northeast-1.compute.internal  Container image "gorapadd/order:memleak" already present on machine
  Normal  Created    16s   kubelet, ip-192-168-29-79.ap-northeast-1.compute.internal  Created container order
  Normal  Started    16s   kubelet, ip-192-168-29-79.ap-northeast-1.compute.internal  Started container order

root@labs--311032102:/home/project/ops-liveness/dp-cqrs/order/kubernetes# kubectl describe pod order
Name:         order-78df866548-bp4xj
Namespace:    default
Priority:     0
Node:         ip-192-168-29-79.ap-northeast-1.compute.internal/192.168.29.79
Start Time:   Wed, 22 Jun 2022 01:26:21 +0000
Labels:       app=order
              pod-template-hash=78df866548
Annotations:  kubernetes.io/psp: eks.privileged
Status:       Running
IP:           192.168.0.88
IPs:
  IP:           192.168.0.88
Controlled By:  ReplicaSet/order-78df866548
Containers:
  order:
    Container ID:   docker://d2b0bac5a1ab7bd08010be8cd515f8aec88ed74127d38d4266891f727ced7af2
    Image:          gorapadd/order:memleak
    Image ID:       docker-pullable://gorapadd/order@sha256:1db014319ca6e425e24a4243476b6476638aef7aa5e4ade9b9102c6e55f5ddfb
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 22 Jun 2022 01:26:22 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       http-get http://:8080/actuator/health delay=120s timeout=2s period=5s #success=1 #failure=5
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zkb74 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-zkb74:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-zkb74
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From                                                       Message
  ----    ------     ----  ----                                                       -------
  Normal  Scheduled  25s   default-scheduler                                          Successfully assigned default/order-78df866548-bp4xj to ip-192-168-29-79.ap-northeast-1.compute.internal
  Normal  Pulled     24s   kubelet, ip-192-168-29-79.ap-northeast-1.compute.internal  Container image "gorapadd/order:memleak" already present on machine
  Normal  Created    24s   kubelet, ip-192-168-29-79.ap-northeast-1.compute.internal  Created container order
  Normal  Started    24s   kubelet, ip-192-168-29-79.ap-northeast-1.compute.internal  Started container order

root@labs--311032102:/home/project/ops-liveness/dp-cqrs/order/kubernetes# kubectl describe pod order
Name:         order-78df866548-bp4xj
Namespace:    default
Priority:     0
Node:         ip-192-168-29-79.ap-northeast-1.compute.internal/192.168.29.79
Start Time:   Wed, 22 Jun 2022 01:26:21 +0000
Labels:       app=order
              pod-template-hash=78df866548
Annotations:  kubernetes.io/psp: eks.privileged
Status:       Running
IP:           192.168.0.88
IPs:
  IP:           192.168.0.88
Controlled By:  ReplicaSet/order-78df866548
Containers:
  order:
    Container ID:   docker://d2b0bac5a1ab7bd08010be8cd515f8aec88ed74127d38d4266891f727ced7af2
    Image:          gorapadd/order:memleak
    Image ID:       docker-pullable://gorapadd/order@sha256:1db014319ca6e425e24a4243476b6476638aef7aa5e4ade9b9102c6e55f5ddfb
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 22 Jun 2022 01:26:22 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       http-get http://:8080/actuator/health delay=120s timeout=2s period=5s #success=1 #failure=5
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zkb74 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-zkb74:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-zkb74
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From                                                       Message
  ----    ------     ----  ----                                                       -------
  Normal  Scheduled  28s   default-scheduler                                          Successfully assigned default/order-78df866548-bp4xj to ip-192-168-29-79.ap-northeast-1.compute.internal
  Normal  Pulled     27s   kubelet, ip-192-168-29-79.ap-northeast-1.compute.internal  Container image "gorapadd/order:memleak" already present on machine
  Normal  Created    27s   kubelet, ip-192-168-29-79.ap-northeast-1.compute.internal  Created container order
  Normal  Started    27s   kubelet, ip-192-168-29-79.ap-northeast-1.compute.internal  Started container order


root@labs--311032102:/home/project/ops-liveness/dp-cqrs/order/kubernetes# kubectl create -f service.yaml
service/order created

root@siege:/# http http://order:8080/orders^C
root@siege:/# http http://order:8080/callMemleak

http: error: ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')) while doing GET request to URL: http://order:8080/callMemleak
root@siege:/# http http://order:8080/orders

http: error: ConnectionError: HTTPConnectionPool(host='order', port=8080): Max retries exceeded with url: /orders (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff35e28ef28>: Failed to establish a new connection: [Errno 111] Connection refused')) while doing GET request to URL: http://order:8080/orders
root@siege:/# http http://order:8080/orders

http: error: ConnectionError: HTTPConnectionPool(host='order', port=8080): Max retries exceeded with url: /orders (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8c3df35f98>: Failed to establish a new connection: [Errno 111] Connection refused')) while doing GET request to URL: http://order:8080/orders
root@siege:/# http http://order:8080/orders

http: error: ConnectionError: HTTPConnectionPool(host='order', port=8080): Max retries exceeded with url: /orders (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f7ad02c3f60>: Failed to establish a new connection: [Errno 111] Connection refused')) while doing GET request to URL: http://order:8080/orders
root@siege:/# http http://order:8080/orders

http: error: ConnectionError: HTTPConnectionPool(host='order', port=8080): Max retries exceeded with url: /orders (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f3338827f60>: Failed to establish a new connection: [Errno 111] Connection refused')) while doing GET request to URL: http://order:8080/orders
root@siege:/# http http://order:8080/orders

http: error: ConnectionError: HTTPConnectionPool(host='order', port=8080): Max retries exceeded with url: /orders (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa9c6e09e80>: Failed to establish a new connection: [Errno 111] Connection refused')) while doing GET request to URL: http://order:8080/orders
root@siege:/# http http://order:8080/orders

http: error: ConnectionError: HTTPConnectionPool(host='order', port=8080): Max retries exceeded with url: /orders (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f6d0ceb2f98>: Failed to establish a new connection: [Errno 111] Connection refused')) while doing GET request to URL: http://order:8080/orders
root@siege:/# http http://order:8080/orders

http: error: ConnectionError: HTTPConnectionPool(host='order', port=8080): Max retries exceeded with url: /orders (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f09f7a7ef98>: Failed to establish a new connection: [Errno 111] Connection refused')) while doing GET request to URL: http://order:8080/orders
root@siege:/# http http://order:8080/orders

http: error: ConnectionError: HTTPConnectionPool(host='order', port=8080): Max retries exceeded with url: /orders (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4c94ec9eb8>: Failed to establish a new connection: [Errno 111] Connection refused')) while doing GET request to URL: http://order:8080/orders
root@siege:/# http http://order:8080/orders

http: error: ConnectionError: HTTPConnectionPool(host='order', port=8080): Max retries exceeded with url: /orders (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f6a42f42f60>: Failed to establish a new connection: [Errno 111] Connection refused')) while doing GET request to URL: http://order:8080/orders
root@siege:/# http http://order:8080/orders

http: error: ConnectionError: HTTPConnectionPool(host='order', port=8080): Max retries exceeded with url: /orders (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f18084d4f28>: Failed to establish a new connection: [Errno 111] Connection refused')) while doing GET request to URL: http://order:8080/orders
root@siege:/# http http://order:8080/orders

http: error: ConnectionError: HTTPConnectionPool(host='order', port=8080): Max retries exceeded with url: /orders (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb843274f28>: Failed to establish a new connection: [Errno 111] Connection refused')) while doing GET request to URL: http://order:8080/orders
root@siege:/# http http://order:8080/orders

http: error: ConnectionError: HTTPConnectionPool(host='order', port=8080): Max retries exceeded with url: /orders (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb92d16a080>: Failed to establish a new connection: [Errno 111] Connection refused')) while doing GET request to URL: http://order:8080/orders
root@siege:/# http http://order:8080/orders

http: error: ConnectionError: HTTPConnectionPool(host='order', port=8080): Max retries exceeded with url: /orders (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa94a261f98>: Failed to establish a new connection: [Errno 111] Connection refused')) while doing GET request to URL: http://order:8080/orders
root@siege:/# http http://order:8080/orders

http: error: ConnectionError: HTTPConnectionPool(host='order', port=8080): Max retries exceeded with url: /orders (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f06555f7f98>: Failed to establish a new connection: [Errno 111] Connection refused')) while doing GET request to URL: http://order:8080/orders
root@siege:/# http http://order:8080/orders

http: error: ConnectionError: HTTPConnectionPool(host='order', port=8080): Max retries exceeded with url: /orders (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4f75ee6f60>: Failed to establish a new connection: [Errno 111] Connection refused')) while doing GET request to URL: http://order:8080/orders
root@siege:/# http http://order:8080/orders

http: error: ConnectionError: HTTPConnectionPool(host='order', port=8080): Max retries exceeded with url: /orders (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7efdb2aadfd0>: Failed to establish a new connection: [Errno 111] Connection refused')) while doing GET request to URL: http://order:8080/orders
root@siege:/# http http://order:8080/orders

http: error: ConnectionError: HTTPConnectionPool(host='order', port=8080): Max retries exceeded with url: /orders (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7faf0d309e80>: Failed to establish a new connection: [Errno 111] Connection refused')) while doing GET request to URL: http://order:8080/orders
root@siege:/# http http://order:8080/orders

http: error: ConnectionError: HTTPConnectionPool(host='order', port=8080): Max retries exceeded with url: /orders (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f15bd2fdf28>: Failed to establish a new connection: [Errno 111] Connection refused')) while doing GET request to URL: http://order:8080/orders
root@siege:/# http http://order:8080/orders

http: error: ConnectionError: HTTPConnectionPool(host='order', port=8080): Max retries exceeded with url: /orders (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fee75733f28>: Failed to establish a new connection: [Errno 111] Connection refused')) while doing GET request to URL: http://order:8080/orders
root@siege:/# http http://order:8080/orders
HTTP/1.1 200 
Content-Type: application/hal+json;charset=UTF-8
Date: Wed, 22 Jun 2022 01:36:18 GMT
Transfer-Encoding: chunked

{
    "_embedded": {
        "orders": []
    },
    "_links": {
        "profile": {
            "href": "http://order:8080/profile/orders"
        },
        "self": {
            "href": "http://order:8080/orders{?page,size,sort}",
            "templated": true
        }
    },
    "page": {
        "number": 0,
        "size": 20,
        "totalElements": 0,
        "totalPages": 0
    }
}

3.오토스케일링 (  Auto Scale-out) HPA 
================================================
  실습은 CPU / 메모리 , 뒤에 연결갯수로 (후에..)


root@labs--311032102:/home/project/ops-autoscale# kubectl explain hpa
KIND:     HorizontalPodAutoscaler
VERSION:  autoscaling/v1

DESCRIPTION:
     configuration of a horizontal pod autoscaler.

FIELDS:
   apiVersion   <string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind <string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata     <Object>
     Standard object metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec <Object>
     behaviour of autoscaler. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status.

   status       <Object>
     current information about the autoscaler.

--> helm 설치를 해야함.

root@labs--311032102:/home/project/ops-autoscale# kubectl top nodes
NAME                                               CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
ip-192-168-29-79.ap-northeast-1.compute.internal   74m          3%     671Mi           20%       
ip-192-168-45-77.ap-northeast-1.compute.internal   60m          3%     1389Mi          41%       
ip-192-168-91-26.ap-northeast-1.compute.internal   79m          4%     1382Mi          41%    

root@labs--311032102:/home/project/ops-autoscale# kubectl autoscale -h
Creates an autoscaler that automatically chooses and sets the number of pods that run in a kubernetes cluster.

 Looks up a Deployment, ReplicaSet, StatefulSet, or ReplicationController by name and creates an autoscaler that uses
the given resource as a reference. An autoscaler can automatically increase or decrease number of pods deployed within
the system as needed.

Examples:
  # Auto scale a deployment "foo", with the number of pods between 2 and 10, no target CPU utilization specified so a
default autoscaling policy will be used:
  kubectl autoscale deployment foo --min=2 --max=10
  
  # Auto scale a replication controller "foo", with the number of pods between 1 and 5, target CPU utilization at 80%:
  kubectl autoscale rc foo --max=5 --cpu-percent=80

Options:
      --allow-missing-template-keys=true: If true, ignore any errors in templates when a field or map key is missing in
the template. Only applies to golang and jsonpath output formats.
      --cpu-percent=-1: The target average CPU utilization (represented as a percent of requested CPU) over all the
pods. If it's not specified or negative, a default autoscaling policy will be used.
      --dry-run='none': Must be "none", "server", or "client". If client strategy, only print the object that would be
sent, without sending it. If server strategy, submit server-side request without persisting the resource.
      --field-manager='kubectl-autoscale': Name of the manager used to track field ownership.
  -f, --filename=[]: Filename, directory, or URL to files identifying the resource to autoscale.
      --generator='horizontalpodautoscaler/v1': The name of the API generator to use. Currently there is only 1
generator.
  -k, --kustomize='': Process the kustomization directory. This flag can't be used together with -f or -R.
      --max=-1: The upper limit for the number of pods that can be set by the autoscaler. Required.
      --min=-1: The lower limit for the number of pods that can be set by the autoscaler. If it's not specified or
negative, the server will apply a default value.
      --name='': The name for the newly created object. If not specified, the name of the input resource will be used.
  -o, --output='': Output format. One of:
json|yaml|name|go-template|go-template-file|template|templatefile|jsonpath|jsonpath-as-json|jsonpath-file.
      --record=false: Record current kubectl command in the resource annotation. If set to false, do not record the
command. If set to true, record the command. If not set, default to updating the existing annotation value only if one
already exists.
  -R, --recursive=false: Process the directory used in -f, --filename recursively. Useful when you want to manage
related manifests organized within the same directory.
      --save-config=false: If true, the configuration of current object will be saved in its annotation. Otherwise, the
annotation will be unchanged. This flag is useful when you want to perform kubectl apply on this object in the future.
      --template='': Template string or path to template file to use when -o=go-template, -o=go-template-file. The
template format is golang templates [http://golang.org/pkg/text/template/#pkg-overview].

Usage:
  kubectl autoscale (-f FILENAME | TYPE NAME | TYPE/NAME) [--min=MINPODS] --max=MAXPODS [--cpu-percent=CPU] [options]

Use "kubectl options" for a list of global command-line options (applies to all commands).

root@labs--311032102:/home/project/ops-autoscale# kubectl autoscale deployment order --cpu-percent=20 --min=1 --max=5
horizontalpodautoscaler.autoscaling/order autoscaled

root@labs--311032102:/home/project/ops-autoscale# kubectl get all
NAME                                  READY   STATUS             RESTARTS   AGE
pod/busybox                           0/1     CrashLoopBackOff   228        19h
pod/my-kafka-0                        1/1     Running            1          20h
pod/my-kafka-zookeeper-0              1/1     Running            1          20h
pod/netcom                            1/1     Running            21         21h
pod/netcom-d598f8f9b-tgv8l            0/1     ImagePullBackOff   0          21h
pod/nginx-6799fc88d8-9xsp9            1/1     Running            0          21h
pod/nginx-6799fc88d8-bh6d7            1/1     Running            0          42h
pod/nginx-6799fc88d8-xhj5j            1/1     Running            0          42h
pod/nginx-gorapa-2-8559b5bc76-8xgc8   1/1     Running            0          42h
pod/nginx-gorapa-2-8559b5bc76-mzblw   1/1     Running            0          42h
pod/nginx-gorapa-2-8559b5bc76-sbwq9   1/1     Running            0          42h
pod/order-78df866548-z4ntt            0/1     CrashLoopBackOff   6          16m
pod/siege                             1/1     Running            0          41h
pod/siege-55b7f7d5fc-vwqk6            1/1     Running            0          23h
pod/user-data                         1/1     Running            0          18h
pod/user02-order-5fd6ff986d-4q9q6     1/1     Running            0          43h

NAME                                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
service/kubernetes                    ClusterIP   10.100.0.1       <none>        443/TCP                      45h
service/my-kafka                      ClusterIP   10.100.147.203   <none>        9092/TCP                     20h
service/my-kafka-headless             ClusterIP   None             <none>        9092/TCP,9093/TCP            20h
service/my-kafka-zookeeper            ClusterIP   10.100.124.20    <none>        2181/TCP,2888/TCP,3888/TCP   20h
service/my-kafka-zookeeper-headless   ClusterIP   None             <none>        2181/TCP,2888/TCP,3888/TCP   20h
service/nginx-gorapa-2                ClusterIP   10.100.91.162    <none>        8080/TCP                     42h
service/order                         ClusterIP   10.100.94.237    <none>        8080/TCP                     12m
service/userapp                       ClusterIP   10.100.170.131   <none>        8080/TCP                     23h

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/netcom           0/1     1            0           21h
deployment.apps/nginx            3/3     3            3           42h
deployment.apps/nginx-gorapa-2   3/3     3            3           42h
deployment.apps/order            0/1     1            0           16m
deployment.apps/siege            1/1     1            1           23h
deployment.apps/user02-order     1/1     1            1           43h

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/netcom-d598f8f9b            1         1         0       21h
replicaset.apps/nginx-6799fc88d8            3         3         3       42h
replicaset.apps/nginx-gorapa-2-8559b5bc76   3         3         3       42h
replicaset.apps/order-78df866548            1         1         0       16m
replicaset.apps/siege-55b7f7d5fc            1         1         1       23h
replicaset.apps/user02-order-5fd6ff986d     1         1         1       43h

NAME                                  READY   AGE
statefulset.apps/my-kafka             1/1     20h
statefulset.apps/my-kafka-zookeeper   1/1     20h

NAME                                        REFERENCE          TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
horizontalpodautoscaler.autoscaling/order   Deployment/order   <unknown>/20%   1         5         1          23s
root@labs--311032102:/home/project/ops-autoscale# kubectl get hpa
NAME    REFERENCE          TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
order   Deployment/order   <unknown>/20%   1         5         1          40s



root@labs--311032102:/home/project/ops-autoscale/shopmall/order/kubernetes# ls -lrt
total 8
-rwxr-xr-x 1 root root 159 Jul 20  2020 service.yaml
-rwxr-xr-x 1 root root 837 Jun 22 01:54 deployment.yml
root@labs--311032102:/home/project/ops-autoscale/shopmall/order/kubernetes# vi deployment.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: order
  labels:
    app: order
spec:
  replicas: 1
  selector:
    matchLabels:
      app: order
  template:
    metadata:
      labels:
        app: order
    spec:
      containers:
        - name: order
          image: jinyoung/monolith-order:v20210602
          ports:
            - containerPort: 8080
          resources:
            requests:
              cpu: "200m"                             <=== CPU 구간 
          readinessProbe:
            httpGet:
              path: '/actuator/health'
              port: 8080
            initialDelaySeconds: 10
            timeoutSeconds: 2
            periodSeconds: 5
            failureThreshold: 10
          livenessProbe:
            httpGet:
              path: '/actuator/health'
              port: 8080
            initialDelaySeconds: 120
            timeoutSeconds: 2
            periodSeconds: 5
            failureThreshold: 5

kubectl exec -it siege -- /bin/bash
siege -c20 -t40S -v http://order:8080/orders
exit


root@labs--311032102:/home/project/ops-autoscale# kubectl get pod -w
NAME                              READY   STATUS             RESTARTS   AGE
busybox                           0/1     CrashLoopBackOff   230        19h
my-kafka-0                        1/1     Running            1          20h
my-kafka-zookeeper-0              1/1     Running            1          20h
netcom                            1/1     Running            21         21h
netcom-d598f8f9b-tgv8l            0/1     ImagePullBackOff   0          21h
nginx-6799fc88d8-9xsp9            1/1     Running            0          21h
nginx-6799fc88d8-bh6d7            1/1     Running            0          42h
nginx-6799fc88d8-xhj5j            1/1     Running            0          42h
nginx-gorapa-2-8559b5bc76-8xgc8   1/1     Running            0          42h
nginx-gorapa-2-8559b5bc76-mzblw   1/1     Running            0          42h
nginx-gorapa-2-8559b5bc76-sbwq9   1/1     Running            0          42h
order-7b66547f76-9tnsr            1/1     Running            0          97s
siege                             1/1     Running            0          42h
siege-55b7f7d5fc-vwqk6            1/1     Running            0          23h
user-data                         1/1     Running            0          18h
user02-order-5fd6ff986d-4q9q6     1/1     Running            0          43h
order-7b66547f76-tv9x8            0/1     Pending            0          0s
order-7b66547f76-tv9x8            0/1     Pending            0          0s
order-7b66547f76-rx4s6            0/1     Pending            0          0s
order-7b66547f76-ldgvr            0/1     Pending            0          0s
order-7b66547f76-tv9x8            0/1     ContainerCreating   0          0s
order-7b66547f76-ldgvr            0/1     Pending             0          0s
order-7b66547f76-rx4s6            0/1     Pending             0          0s
order-7b66547f76-rx4s6            0/1     ContainerCreating   0          0s
order-7b66547f76-ldgvr            0/1     ContainerCreating   0          0s
order-7b66547f76-tv9x8            0/1     Running             0          1s
order-7b66547f76-ldgvr            0/1     Running             0          2s
order-7b66547f76-rx4s6            0/1     Running             0          6s
order-7b66547f76-js88d            0/1     Pending             0          0s
order-7b66547f76-js88d            0/1     Pending             0          0s
order-7b66547f76-js88d            0/1     ContainerCreating   0          0s
order-7b66547f76-tv9x8            1/1     Running             0          16s
order-7b66547f76-js88d            0/1     Running             0          2s
order-7b66547f76-ldgvr            1/1     Running             0          21s
order-7b66547f76-rx4s6            1/1     Running             0          25s
order-7b66547f76-js88d            1/1     Running             0          20s
netcom-d598f8f9b-tgv8l            0/1     ErrImagePull        0          21h
netcom-d598f8f9b-tgv8l            0/1     ImagePullBackOff    0          21h
busybox                           0/1     Completed           231        19h
busybox                           0/1     CrashLoopBackOff    231        19h



##
http :8081/orders/
http :8081/orders/1/purchas_2 stockid=1 qty=1 userid=1
http :8081/orders/1/purchas_2 stockid=1 qty=1 userid=1

## 데이타입력 
http POST :8081/orders/ stockid=1 qty=1 userid=1

##
http PUT http://localhost:8081/orders/1/purchas_2  stockid=1 qty=3 userid=3

## 테스트1
http PUT "http://localhost:8081/orders/purchas_3"  stockid=9999 qty=3 userid=3
## 테스트2
http PUT "http://localhost:8081/orders/purchas_3?stockid=9999"   ===> 성공

## 테스트3
http PATCH "http://localhost:8081/orders/purchas_99"  stockid=9999 qty=3 userid=3
http POST http://localhost:8081/orders/purchas_99?stockid=9999&qty=3   ==> 성공
## 테스트4
http GET "http://localhost:8081/orders/purchas_98?stockid=9999&qty=3&userid=3  ===>성공

http http://localhost:8081/orders  stockid=1 qty=1 userid=1


##########

-- 주문 하기
http localhost:8088/orders productId=1 quantity=3 customerId="1@uengine.org" customerName="홍길동" customerAddr="서울시"

-- 주문 후 변경된 상품 수량 확인
http http://localhost:8088/orders/1/product

-- 주문 후 delivery 내역중 order  확인
http http://localhost:8088/deliveries
http http://localhost:8088/orders/1/delivery

-- 배송 완료하기
http PATCH localhost:8088/deliveries/1 deliveryState=DeliveryCompleted

-- 주문 취소 하기
http PATCH localhost:8088/orders/1 state=OrderCancelled






##### POD 안에 설정값을 적용하는 방법 

root@labs--311032102:/home/project/ops-autoscale/shopmall/simplegorapa# kubectl create configmap -h
Create a configmap based on a file, directory, or specified literal value.

 A single configmap may package one or more key/value pairs.

 When creating a configmap based on a file, the key will default to the basename of the file, and the value will default
to the file content.  If the basename is an invalid key, you may specify an alternate key.

 When creating a configmap based on a directory, each file whose basename is a valid key in the directory will be
packaged into the configmap.  Any directory entries except regular files are ignored (e.g. subdirectories, symlinks,
devices, pipes, etc).

Aliases:
configmap, cm

Examples:
  # Create a new configmap named my-config based on folder bar
  kubectl create configmap my-config --from-file=path/to/bar
  
  # Create a new configmap named my-config with specified keys instead of file basenames on disk
  kubectl create configmap my-config --from-file=key1=/path/to/bar/file1.txt --from-file=key2=/path/to/bar/file2.txt
  
  # Create a new configmap named my-config with key1=config1 and key2=config2
  kubectl create configmap my-config --from-literal=key1=config1 --from-literal=key2=config2
  
  # Create a new configmap named my-config from the key=value pairs in the file
  kubectl create configmap my-config --from-file=path/to/bar

root@labs--311032102:/home/project/ops-autoscale/shopmall/simplegorapa#  kubectl create configmap my-config --from-literal=key1=config1 --from-literal=key2=config2
configmap/my-config created
root@labs--311032102:/home/project/ops-autoscale/shopmall/simplegorapa# kubectl get cm
NAME                         DATA   AGE
my-config                    2      8s
my-kafka-scripts             1      23h
my-kafka-zookeeper-scripts   2      23h

root@labs--311032102:/home/project/ops-autoscale/shopmall/simplegorapa# kubectl get cm my-config  -o yaml
apiVersion: v1
data:
  key1: config1
  key2: config2
kind: ConfigMap
metadata:
  creationTimestamp: "2022-06-22T04:40:00Z"
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:data:
        .: {}
        f:key1: {}
        f:key2: {}
    manager: kubectl-create
    operation: Update
    time: "2022-06-22T04:40:00Z"
  name: my-config
  namespace: default
  resourceVersion: "503055"
  selfLink: /api/v1/namespaces/default/configmaps/my-config
  uid: ce3343ce-3ec0-4508-8577-d7119ff0d95d
root@labs--311032102:/home/project/ops-autoscale/shopmall/simplegorapa# 	


##
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
    envFrom:
    - configMapRef:
      name: my-config
      

oot@labs--311032102:/home/project/ops-autoscale/shopmall/simplegorapa# kubectl explain pod.spec.containers.envFrom
KIND:     Pod
VERSION:  v1


##
spring:
  profiles: docker
  jpa:
    hibernate:
      naming:
        physical-strategy: org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl
      ddl-auto: update
    properties:
      hibernate:
        show_sql: true
        format_sql: true
        dialect: org.hibernate.dialect.MySQL57Dialect
  datasource:
    url: jdbc:mysql://${_DATASOURCE_ADDRESS:3306}/${_DATASOURCE_TABLESPACE}
    username: ${_DATASOURCE_USERNAME}
    password: ${_DATASOURCE_PASSWORD}
    driverClassName: com.mysql.cj.jdbc.Driver  


####  교재내용 (앱에 MYSQL 사용하는것) , CONFIGMAP 을 사용하지 않았음.
주문 서비스에 Database 설정의 변경
데이터베이스 설정이 이루어진 order project 를 다운로드 받는다

cd ~
git clone https://github.com/event-storming/monolith      ==> 모노리스 에서 다운 받는다.

cd monolith
Dockerfile-prod 를 Dockerfile 로 바꾼다.
기존 Dockerfile 삭제.

rm Dockerfile
mv Dockerfile-prod Dockerfile
빌드하여 order:database 라는 이미지명으로 레지스트리에 등록한다
e.g. (도커허브경우)

mvn package -B 

# docker hub 경우
docker build -t jinyoung/order:database .

# azure 의 경우
az acr build --registry user27 --image user27.azurecr.io/order:database .
왼쪽 에디터에서 생성된 프로젝트를 열기위해서는, Explorer 의 바의 우측 … 버튼을 클릭한후, Add folder to workspace 를 클릭한후, 다운받은 monolith 를 선택한다.

order 서비스의 Database 설정을 아래와 같이 변경하므로써, 외부의 데이터베이스에 접근 가능하게 된다:

application.yml (or application-prod.yml)

spring:
  jpa:
    hibernate:
      naming:
        physical-strategy: org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl
      ddl-auto: update
    properties:
      hibernate:
        show_sql: true
        format_sql: true
        dialect: org.hibernate.dialect.MySQL57Dialect
  datasource:
    url: jdbc:mysql://${_DATASOURCE_ADDRESS:35.221.110.118:3306}/${_DATASOURCE_TABLESPACE:my-database}
    username: ${_DATASOURCE_USERNAME:root1}
    password: ${_DATASOURCE_PASSWORD:secretpassword}
    driverClassName: com.mysql.cj.jdbc.Driver
변경한 정보를 환경변수에서 얻어오도록 설정하였고, Deployment 에서 위의 값이 전달되도록 주입할 수 있다:

apiVersion: "apps/v1"
kind: "Deployment"
metadata: 
  name: "order"
  labels: 
    app: "order"
spec: 
  selector: 
    matchLabels: 
      app: "order"
  replicas: 1
  template: 
    metadata: 
      labels: 
        app: "order"
    spec: 
      containers: 
        - 
          name: "order"
          image: "jinyoung/order:database"
          ports: 
            - 
              containerPort: 80
          env:
            - name: superuser.userId
              value: some_value					
            - name: _DATASOURCE_ADDRESS
              value: mysql
            - name: _DATASOURCE_TABLESPACE
              value: orderdb
            - name: _DATASOURCE_USERNAME
              value: root
            - name: _DATASOURCE_PASSWORD
              value: admin

설정후에 kubectl logs 로 로그를 확인하면 다음과 같은 오류를 발견할 수 있다:

kubectl get po # pod 명 확인
kubectl logs <pod 명>
로그 내용중:

Caused by: java.net.UnknownHostException: mysql: Name does not resolve
우리가 제공한 DB server 의 주소를 환경변수로 잘 받아왔고 (mysql), 그 주소로 접근을 시도했으나 서비스가 올라있지 않으므로 발생하는 오류이다.

값을 위와 같이 Deployment 설정에 직접 입력하는것은 개발자와 운영자사이에 역할이 혼재되므로, 운영자가 해당 설정 부분만을 관리할 수 있도록 별도의 Configuration 을 위한 쿠버네티스 객체인 ConfigMap (혹은 Secret)을 선언하여 연결할 수 있다. 여기서는 패스워드가 노출되면 안되므로 PASSWORD 에 대해서만 Secret 을 이용하여 분리해준다:

apiVersion: v1
kind: Secret
metadata:
  name: mysql-pass
type: Opaque
data:
  password: YWRtaW4=     
"YWRtaW4="는 ‘admin’ 문자열의 BASE64 인코딩된 문자열이다. “echo -n ‘admin’ | base64” 명령을 통해 생성가능하다.

Secret 객체의 내용을 기존 deployment.yaml 에 추가하고:

$ kubectl apply -f deployment.yaml

secret/mysql-pass created
생성된 secret 을 확인한다:

$ kubectl get secrets

NAME                  TYPE                                  DATA   AGE
default-token-l7t7b   kubernetes.io/service-account-token   3      4h24m
mysql-pass            Opaque                                1      1m
해당 Secret 을 Order Deployment 에 설정:

          env:
            - name: superuser.userId
              value: userId
            - name: _DATASOURCE_ADDRESS
              value: mysql
            - name: _DATASOURCE_TABLESPACE
              value: orderdb
            - name: _DATASOURCE_USERNAME
              value: root
            - name: _DATASOURCE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mysql-pass
                  key: password

Database 서비스의 생성
주문 서비스를 위한 데이터베이스로 MySQL 을 사용하기로 한다. MySQL 이미지명으로 간단하게 Pod 하나를 생성한다.

MySQL 을 위한 Pod 설치:

apiVersion: v1
kind: Pod
metadata:
  name: mysql
  labels:
    name: lbl-k8s-mysql
spec:
  containers:
  - name: mysql
    image: mysql:latest
    env:
    - name: MYSQL_ROOT_PASSWORD
      valueFrom:
        secretKeyRef:
          name: mysql-pass
          key: password
    ports:
    - name: mysql
      containerPort: 3306
      protocol: TCP
    volumeMounts:
    - name: k8s-mysql-storage
      mountPath: /var/lib/mysql
  volumes:
  - name: k8s-mysql-storage
    emptyDir: {}
생성된 yaml 을 deployment.yaml 에 추가한후:

$ kubectl apply -f deployment.yaml

pod/k8s-mysql created
Pod 실행을 확인한다:

$ kubectl get pod

NAME        READY   STATUS    RESTARTS   AGE
k8s-mysql   1/1     Running   0          30s
Now, we can connect to the k8s-mysql pod:
Pod 에 접속하여 orderdb 데이터베이스 공간을 만들어주고 데이터베이스가 잘 동작하는지 확인한다:

$ kubectl exec mysql -it -- bash

# echo $MYSQL_ROOT_PASSWORD
admin

# mysql --user=root --password=$MYSQL_ROOT_PASSWORD

mysql> create database orderdb;
    -> ;
Query OK, 1 row affected (0.01 sec)

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| orderdb            |
| performance_schema |
| sys                |
+--------------------+
5 rows in set (0.01 sec)

mysql> exit
주문 마이크로 서비스를 쿠버네티스 DNS 체계내에서 접근가능하게 하기 위해 ClusterIP 로 서비스를 생성해준다. 주문 서비스에서 mysql 접근을 위하여 "mysql"이라는 도메인명으로 접근하고 있으므로, 같은 이름으로 서비스를 만들어준다:

apiVersion: v1
kind: Service
metadata:
  labels:
    name: lbl-k8s-mysql
  name: mysql
  namespace: default
spec:
  ports:
  - port: 3306
    protocol: TCP
    targetPort: 3306
  selector:
    name: lbl-k8s-mysql
  type: ClusterIP
마찬가지 위의 내용을 deployment.yaml 에 추가하고 kubectl apply -f deployment.yaml
앞으로 계속 마찬가지!

주문 마이크로 서비스만을 새로 재기동 시키기 위해서는 아래와 같이 po 를 삭제해주면 deployment 에 의해서 알아서 재시작된다:

kubectl get po -l app=order
kubectl delete po [order-po-name]
이렇게 잘 접속이 된다면 다음의 로그를 확인할 수 있다:

Hibernate: 
    insert 
    into
        ProductOption
        (description, name, optionName, PRODUCT_ID) 
    values
        (?, ?, ?, ?)
Hibernate: 
    insert 
    into
        Product
        (imageUrl, name, price, stock) 
    values
        (?, ?, ?, ?)
Hibernate: 
    insert 
    into
        ProductOption
        (description, name, optionName, PRODUCT_ID) 
    values
        (?, ?, ?, ?)
Hibernate: 
    insert 
    into
        ProductOption
        (description, name, optionName, PRODUCT_ID) 
    values
        (?, ?, ?, ?)
주문 걸어보기
httpie pod 에 들어가서 주문을 걸어준다:

kubectl exec -it httpie /bin/bash 

root@httpie:/# http order:8080/orders productId=1 customerId="jjy"
위의 ‘order’ 도메인 주소로 접근이 되도록 하려면 order 를 위한 Service (이름 order) 객체가 꼭 만들어져 있어야 한다.

apiVersion: "v1"
kind: "Service"
metadata: 
  name: "order"
  labels: 
    app: "order"
spec: 
  ports: 
    - 
      port: 8080
      targetPort: 8080
  selector: 
    app: "order"
  type: "ClusterIP"

order 라는 도메인 주소로 호출이 되려면 order 와 같은 클러스터내부에서 호출해야 한다. 그러려면 httpie pod 를 만들어서 order 와 같은 네임스페이스 (default) 내에 httpie pod 가 생성되어있어야 한다.
httpie pod 를 만들기:

cat <<EOF | kubectl apply -f -
apiVersion: "v1"
kind: "Pod"
metadata: 
  name: httpie
  labels: 
    name: httpie
spec: 
  containers: 
    - 
      name: httpie
      image: clue/httpie
      command:
        - sleep
        - "36000"
EOF

httpie pod 를 만들기 귀찮다면… kubectl port-forward deploy/order 8085:8080 를 해놓은 후, localhost:8085 번으로 호출해도 된다.

주문 마이크로 서비스의 데이터가 설치한 MySQL을 통하여 보존되는 것을 확인한다.

mysql> use orderdb
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql> show tables;
+-------------------+
| Tables_in_orderdb |
+-------------------+
| Delivery          |
| Product           |
| ProductOption     |
| order_table       |
+-------------------+
4 rows in set (0.00 sec)

mysql> select * from order_table

+----+--------------+------------+--------------+-------+-----------+-------------+----------+-------------+-------------+
| id | customerAddr | customerId | customerName | price | productId | productName | quantity | state       | product_idx |
+----+--------------+------------+--------------+-------+-----------+-------------+----------+-------------+-------------+
|  1 | NULL         | jjy        | NULL         | 10000 |         1 | TV          |        0 | OrderPlaced |           1 |
+----+--------------+------------+--------------+-------+-----------+-------------+----------+-------------+-------------+
1 row in set (0.00 sec)

주문 마이크로 서비스를 내렸다가 올려도 주문한 내역이 그대로 존재함을 확인할 수 있어야 한다.

PersistenceVolume 을 통한 데이터베이스 데이터 보존
먼저, MySQL 서비스의 설치를 제거했다가 다시 기동시켜본다:

kubectl delete pod mysql
kubectl apply -f deployment.yaml
애플리케이션 데이터가 소실됨을 확인할 수 있다.

이는, MySQL 자체가 사용하는 볼륨이 해당 Pod 에 기본 부착된 파일시스템이기 때문이다. 이를 해결하기 위하여 PersistenceVolume 으로 된 파일시스템에 연결하도록 설정한다:

spec:
  containers:
    volumeMounts:
    - name: k8s-mysql-storage
      mountPath: /var/lib/mysql
  volumes:
  - name: k8s-mysql-storage
    persistentVolumeClaim:
      claimName: "fs"
변경후 apply 를 해주고 기다려보면 해당 Pod 는 Pending 상태에 잠기게 된다. 이유는 해당 Pod 를 위한 fs 라는 PVC가 생성되지 않았기 때문이다.

우리는 저 “fs” 라고 하는 PVC 를 플랫폼(애져 or AWS등)의 파일 서비스에서 만들어와야 한다.

PVC 생성
PersistentVolumeClaim - PVC 를 생성하는 방법은 간단하다. 기본으로 장착된 gp2라고 하는 StorageClass 를 통해서 얻을 수 있기 때문이다.
아래와 같은 yaml 을 설정한 후, kubectl get pvc 를 통해 PVC가 생성되는 것을 확인한다.

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: fs
  labels:
    app: test-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Mi
PVC가 생성되면, mysql pod 의 pending 이 해제될 것이다.

Volume mount 에 대한 설정변경은 그냥 apply 만 해서는 반영이 될 수 없다는 오류가 발견될 것이다. 이를 해결하기 위해서는 완전히 기존 mysql 설정을 삭제한 후 다시 기동해주어야 한다:

kubectl delete -f deployment.yaml
kubectl apply -f deployment.yaml
다시 기동후에는 orderdb 테이블 스페이스를 다시 만들어주어야 한다.

그런후에 PVC 상태를 확인:

# kubectl get pvc
NAME            STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
aws-ebs         Bound     pvc-225ebe47-cc67-4985-8f94-c0d4d795dede   1Gi        RWO            gp2            9m47s
이제, MySQL 이 소실된다하더라도, PersistenceVolume 에 실제 연결된 클라우드 파일 시스템에 데이터가 보존됨을 확인할 수 있다.

주문 한건을 걸어본 후,

mysql pod 를 삭제하고,

kubectl delete pod mysql
kubectl apply -f deployment.yaml
그 후에도, 해당 데이터가 존재함을 확인한다.

#########################

# stock

## Running in local development environment

```
mvn spring-boot:run
```

## Packaging and Running in docker environment

```
mvn package -B
docker build -t username/stock:v1 .
docker run username/stock:v1
```

## Push images and running in Kubernetes

```
docker login 
# in case of docker hub, enter your username and password

docker push username/stock:v1
```

Edit the deployment.yaml under the /kubernetes directory:
```
    spec:
      containers:
        - name: stock
          image: username/stock:latest   # change this image name
          ports:
            - containerPort: 8080

```

Apply the yaml to the Kubernetes:
```
kubectl apply -f kubernetes/deployment.yml
```

See the pod status:
```
kubectl get pods -l app=stock
```

If you have no problem, you can connect to the service by opening a proxy between your local and the kubernetes by using this command:
```
# new terminal
kubectl port-forward deploy/stock 8080:8080

# another terminal
http localhost:8080
```

If you have any problem on running the pod, you can find the reason by hitting this:
```
kubectl logs -l app=stock
```

Following problems may be occurred:

1. ImgPullBackOff:  Kubernetes failed to pull the image with the image name you've specified at the deployment.yaml. Please check your image name and ensure you have pushed the image properly.
1. CrashLoopBackOff: The spring application is not running properly. If you didn't provide the kafka installation on the kubernetes, the application may crash. Please install kafka firstly:

https://labs.msaez.io/#/courses/cna-full/full-course-cna/ops-utility


## 4일차
1 인그레스 : 서비스 라우팅규칙 

vi ingress.yaml

apiVersion: "extensions/v1beta1"
kind: "Ingress"
metadata: 
  name: "shopping-ingress"
  annotations: 
    kubernetes.io/ingress.class: "nginx"
spec: 
  rules: 
    - 
      http: 
        paths: 
          - 
            path: /orders
            pathType: Prefix
            backend: 
              serviceName: order
              servicePort: 8080
          - 
            path: /deliveries
            pathType: Prefix
            backend: 
              serviceName: delivery
              servicePort: 8080
          - 
            path: /products
            pathType: Prefix
            backend: 
              serviceName: product
              servicePort: 8080



root@labs--311032102:/home/project/ops-autoscale# kubectl create -f inress.yaml
error: the path "inress.yaml" does not exist
root@labs--311032102:/home/project/ops-autoscale# kubectl create -f ingress.yaml
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
ingress.extensions/shopping-ingress created
root@labs--311032102:/home/project/ops-autoscale# kubectl get all
NAME                                READY   STATUS             RESTARTS   AGE
pod/busybox                         0/1     CrashLoopBackOff   489        41h
pod/my-kafka-0                      1/1     Running            1          42h
pod/my-kafka-zookeeper-0            1/1     Running            1          42h
pod/netcom                          1/1     Running            43         43h
pod/netcom-d598f8f9b-tgv8l          0/1     ImagePullBackOff   0          43h
pod/order-7b66547f76-rx4s6          1/1     Running            0          22h
pod/siege                           1/1     Running            0          2d16h
pod/siege-55b7f7d5fc-vwqk6          1/1     Running            0          45h
pod/user-data                       1/1     Running            0          41h
pod/user02-order-5fd6ff986d-4q9q6   1/1     Running            0          2d17h

NAME                                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
service/kubernetes                    ClusterIP   10.100.0.1       <none>        443/TCP                      2d19h
service/my-kafka                      ClusterIP   10.100.147.203   <none>        9092/TCP                     42h
service/my-kafka-headless             ClusterIP   None             <none>        9092/TCP,9093/TCP            42h
service/my-kafka-zookeeper            ClusterIP   10.100.124.20    <none>        2181/TCP,2888/TCP,3888/TCP   42h
service/my-kafka-zookeeper-headless   ClusterIP   None             <none>        2181/TCP,2888/TCP,3888/TCP   42h
service/nginx-gorapa-2                ClusterIP   10.100.91.162    <none>        8080/TCP                     2d16h
service/order                         ClusterIP   10.100.94.237    <none>        8080/TCP                     22h
service/userapp                       ClusterIP   10.100.170.131   <none>        8080/TCP                     45h

NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/netcom         0/1     1            0           43h
deployment.apps/order          1/1     1            1           22h
deployment.apps/siege          1/1     1            1           45h
deployment.apps/user02-order   1/1     1            1           2d17h

NAME                                      DESIRED   CURRENT   READY   AGE
replicaset.apps/netcom-d598f8f9b          1         1         0       43h
replicaset.apps/order-754d784c87          0         0         0       22h
replicaset.apps/order-7b66547f76          1         1         1       22h
replicaset.apps/siege-55b7f7d5fc          1         1         1       45h
replicaset.apps/user02-order-5fd6ff986d   1         1         1       2d17h

NAME                                  READY   AGE
statefulset.apps/my-kafka             1/1     42h
statefulset.apps/my-kafka-zookeeper   1/1     42h

NAME                                        REFERENCE          TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
horizontalpodautoscaler.autoscaling/order   Deployment/order   1%/20%    1         5         1          22h
root@labs--311032102:/home/project/ops-autoscale# kubectl get ingress
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME               CLASS    HOSTS   ADDRESS   PORTS   AGE
shopping-ingress   <none>   *                 80      38s
root@labs--311032102:/home/project/ops-autoscale# kubectl get secret
NAME                             TYPE                                  DATA   AGE
default-token-zkb74              kubernetes.io/service-account-token   3      2d19h
my-kafka-token-p4lrr             kubernetes.io/service-account-token   3      42h
sh.helm.release.v1.my-kafka.v1   helm.sh/release.v1                    1      42h
root@labs--311032102:/home/project/ops-autoscale# kubectl get configmap
NAME                         DATA   AGE
my-config                    2      19h
my-kafka-scripts             1      42h
my-kafka-zookeeper-scripts   2      42h
root@labs--311032102:/home/project/ops-autoscale# kubectl get cm
NAME                         DATA   AGE
my-config                    2      19h
my-kafka-scripts             1      42h
my-kafka-zookeeper-scripts   2      42h

##


root@labs--311032102:/home/project/ops-autoscale# kubectl get pod -n ingress-basic
NAME                                                      READY   STATUS    RESTARTS   AGE
nginx-ingress-ingress-nginx-controller-86df55b594-splfv   1/1     Running   0          96s
root@labs--311032102:/home/project/ops-autoscale# kubectl get pod -n ingress-basic -w
NAME                                                      READY   STATUS    RESTARTS   AGE
nginx-ingress-ingress-nginx-controller-86df55b594-splfv   1/1     Running   0          100s

^Croot@labs--311032102:/home/project/ops-autoscale# kubectl get ingress
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME               CLASS    HOSTS   ADDRESS                                                                        PORTS   AGE
shopping-ingress   <none>   *       a577abeeec4b04f138fa14a11b9e4d54-1026617604.ap-northeast-1.elb.amazonaws.com   80      12m


## 

root@labs--311032102:/home/project/ops-autoscale# kubectl get ingress shopping-ingress -w
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME               CLASS    HOSTS   ADDRESS   PORTS   AGE
shopping-ingress   <none>   *                 80      4m
^Croot@labs--311032102:/home/project/ops-autoscale# helm repo list
NAME    URL                               
bitnami https://charts.bitnami.com/bitnami
root@labs--311032102:/home/project/ops-autoscale# helm repo add stable https://charts.helm.sh/stable
"stable" has been added to your repositories
root@labs--311032102:/home/project/ops-autoscale# ls
ingress.yaml  shopmall
root@labs--311032102:/home/project/ops-autoscale# helm repo list
NAME    URL                               
bitnami https://charts.bitnami.com/bitnami
stable  https://charts.helm.sh/stable     
root@labs--311032102:/home/project/ops-autoscale# helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
"ingress-nginx" has been added to your repositories
root@labs--311032102:/home/project/ops-autoscale# helm repo list
NAME            URL                                       
bitnami         https://charts.bitnami.com/bitnami        
stable          https://charts.helm.sh/stable             
ingress-nginx   https://kubernetes.github.io/ingress-nginx
root@labs--311032102:/home/project/ops-autoscale# helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "ingress-nginx" chart repository
...Successfully got an update from the "stable" chart repository
...Successfully got an update from the "bitnami" chart repository
Update Complete. ⎈Happy Helming!⎈
root@labs--311032102:/home/project/ops-autoscale# kubectl create namespace ingress-basic
namespace/ingress-basic created
root@labs--311032102:/home/project/ops-autoscale# kubectl get ns
NAME              STATUS   AGE
db                Active   40h
default           Active   2d19h
ingress-basic     Active   31s
kube-node-lease   Active   2d19h
kube-public       Active   2d19h
kube-system       Active   2d19h
root@labs--311032102:/home/project/ops-autoscale# helm install nginx-ingress ingress-nginx/ingress-nginx --namespace=ingress-basic
NAME: nginx-ingress
LAST DEPLOYED: Thu Jun 23 00:21:17 2022
NAMESPACE: ingress-basic
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
The ingress-nginx controller has been installed.
It may take a few minutes for the LoadBalancer IP to be available.
You can watch the status by running 'kubectl --namespace ingress-basic get services -o wide -w nginx-ingress-ingress-nginx-controller'

An example Ingress that makes use of the controller:
  apiVersion: networking.k8s.io/v1
  kind: Ingress
  metadata:
    name: example
    namespace: foo
  spec:
    ingressClassName: nginx
    rules:
      - host: www.example.com
        http:
          paths:
            - pathType: Prefix
              backend:
                service:
                  name: exampleService
                  port:
                    number: 80
              path: /
    # This section is only required if TLS is to be enabled for the Ingress
    tls:
      - hosts:
        - www.example.com
        secretName: example-tls

If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided:

  apiVersion: v1
  kind: Secret
  metadata:
    name: example-tls
    namespace: foo
  data:
    tls.crt: <base64 encoded cert>
    tls.key: <base64 encoded key>
  type: kubernetes.io/tls
root@labs--311032102:/home/project/ops-autoscale# 
root@labs--311032102:/home/project/ops-autoscale# kubectl get ingress
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME               CLASS    HOSTS   ADDRESS                                                                        PORTS   AGE
shopping-ingress   <none>   *       a577abeeec4b04f138fa14a11b9e4d54-1026617604.ap-northeast-1.elb.amazonaws.com   80      10m
root@labs--311032102:/home/project/ops-autoscale# kubectl get all -n ingress-basic
NAME                                                          READY   STATUS    RESTARTS   AGE
pod/nginx-ingress-ingress-nginx-controller-86df55b594-splfv   1/1     Running   0          2m56s

NAME                                                       TYPE           CLUSTER-IP      EXTERNAL-IP                                                                    PORT(S)                      AGE
service/nginx-ingress-ingress-nginx-controller             LoadBalancer   10.100.229.90   a577abeeec4b04f138fa14a11b9e4d54-1026617604.ap-northeast-1.elb.amazonaws.com   80:32170/TCP,443:31151/TCP   2m57s
service/nginx-ingress-ingress-nginx-controller-admission   ClusterIP      10.100.22.236   <none>                                                                         443/TCP                      2m57s

NAME                                                     READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx-ingress-ingress-nginx-controller   1/1     1            1           2m57s

NAME                                                                DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-ingress-ingress-nginx-controller-86df55b594   1         1         1       2m57s
root@labs--311032102:/home/project/ops-autoscale# kubectl ^C
root@labs--311032102:/home/project/ops-autoscale# kubectl get all -n ingress-basic^C
root@labs--311032102:/home/project/ops-autoscale# ping
bash: ping: command not found
root@labs--311032102:/home/project/ops-autoscale# apt-get install iputils-ping
Reading package lists... Done
Building dependency tree       
Reading state information... Done
E: Unable to locate package iputils-ping
root@labs--311032102:/home/project/ops-autoscale# ping
bash: ping: command not found
root@labs--311032102:/home/project/ops-autoscale# apt-update
bash: apt-update: command not found
root@labs--311032102:/home/project/ops-autoscale# apt-get update
Get:1 https://dl.yarnpkg.com/debian stable InRelease [17.1 kB]
Get:2 http://mirror.kakao.com/ubuntu bionic InRelease [242 kB]                                                                                 
Ign:5 https://storage.googleapis.com/download.dartlang.org/linux/debian stable InRelease                                                       
Get:6 https://packages.microsoft.com/repos/azure-cli bionic InRelease [3965 B]                                                                 
Get:7 https://packages.microsoft.com/ubuntu/18.04/prod bionic InRelease [4003 B]                                                               
Ign:8 https://storage.googleapis.com/download.dartlang.org/linux/debian testing InRelease                                                      
Get:3 http://mirror.kakao.com/ubuntu bionic-updates InRelease [88.7 kB]                                                                        
Get:9 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]                                                                    
Get:10 https://packages.cloud.google.com/apt cloud-sdk InRelease [6751 B]                                                                      
Get:4 http://mirror.kakao.com/ubuntu bionic-backports InRelease [74.6 kB]                                                                      
Ign:11 https://storage.googleapis.com/download.dartlang.org/linux/debian unstable InRelease                                                    
Get:12 https://storage.googleapis.com/download.dartlang.org/linux/debian stable Release [944 B]                                                
Get:13 https://dl.yarnpkg.com/debian stable/main all Packages [11.1 kB]                                    
Get:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]                          
Get:15 https://dl.yarnpkg.com/debian stable/main amd64 Packages [11.1 kB]                                                                      
Get:16 https://storage.googleapis.com/download.dartlang.org/linux/debian testing Release [946 B]                                               
Get:17 https://storage.googleapis.com/download.dartlang.org/linux/debian unstable Release [954 B]                                            
Get:18 https://packages.microsoft.com/repos/azure-cli bionic/main amd64 Packages [17.9 kB]                                                     
Get:19 https://storage.googleapis.com/download.dartlang.org/linux/debian stable Release.gpg [819 B]                                            
Get:20 https://packages.microsoft.com/ubuntu/18.04/prod bionic/main amd64 Packages [292 kB]                                                    
Get:21 https://storage.googleapis.com/download.dartlang.org/linux/debian testing Release.gpg [819 B]                                 
Get:22 https://storage.googleapis.com/download.dartlang.org/linux/debian unstable Release.gpg [819 B]                                
Get:23 http://mirror.kakao.com/ubuntu bionic/main amd64 Packages [1344 kB]                                             
Get:24 http://ppa.launchpad.net/ondrej/php/ubuntu bionic InRelease [20.8 kB]                                
Get:25 https://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [285 kB]                                    
Get:26 http://mirror.kakao.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]                                                   
Get:27 http://mirror.kakao.com/ubuntu bionic/universe amd64 Packages [11.3 MB]          
Get:28 https://storage.googleapis.com/download.dartlang.org/linux/debian stable/main amd64 Packages [6113 B]                            
Get:29 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]                                              
Get:30 http://mirror.kakao.com/ubuntu bionic/multiverse amd64 Packages [186 kB]                                         
Get:31 http://mirror.kakao.com/ubuntu bionic-updates/main amd64 Packages [3298 kB]                                                        
Get:32 http://mirror.kakao.com/ubuntu bionic-updates/universe amd64 Packages [2297 kB]                                                         
Get:33 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1006 kB]                                                   
Get:34 http://mirror.kakao.com/ubuntu bionic-updates/multiverse amd64 Packages [29.8 kB]                                                       
Get:35 http://mirror.kakao.com/ubuntu bionic-updates/restricted amd64 Packages [1047 kB]                                                       
Get:36 http://mirror.kakao.com/ubuntu bionic-backports/main amd64 Packages [12.2 kB]                                                           
Get:37 http://mirror.kakao.com/ubuntu bionic-backports/universe amd64 Packages [12.9 kB]                                                       
Get:38 https://storage.googleapis.com/download.dartlang.org/linux/debian testing/main amd64 Packages [7272 B]                                  
Get:39 https://storage.googleapis.com/download.dartlang.org/linux/debian unstable/main amd64 Packages [36.7 kB]                                
Get:40 http://ppa.launchpad.net/ondrej/php/ubuntu bionic/main amd64 Packages [174 kB]                                     
Get:41 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [22.8 kB]  
Get:42 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2861 kB]   
Get:43 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1521 kB]    
Fetched 26.4 MB in 5s (5594 kB/s)                           
Reading package lists... Done
root@labs--311032102:/home/project/ops-autoscale# apt-get install iputils-ping
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following additional packages will be installed:
  libcap2 libcap2-bin libidn11 libpam-cap
The following NEW packages will be installed:
  iputils-ping libcap2 libcap2-bin libidn11 libpam-cap
0 upgraded, 5 newly installed, 0 to remove and 10 not upgraded.
Need to get 142 kB of archives.
After this operation, 537 kB of additional disk space will be used.
Do you want to continue? [Y/n] y
Get:1 http://mirror.kakao.com/ubuntu bionic/main amd64 libcap2 amd64 1:2.25-1.2 [13.0 kB]
Get:2 http://mirror.kakao.com/ubuntu bionic-updates/main amd64 libidn11 amd64 1.33-2.1ubuntu1.2 [46.6 kB]
Get:3 http://mirror.kakao.com/ubuntu bionic-updates/main amd64 iputils-ping amd64 3:20161105-1ubuntu3 [54.2 kB]
Get:4 http://mirror.kakao.com/ubuntu bionic/main amd64 libcap2-bin amd64 1:2.25-1.2 [20.6 kB]
Get:5 http://mirror.kakao.com/ubuntu bionic/main amd64 libpam-cap amd64 1:2.25-1.2 [7268 B]
Fetched 142 kB in 0s (401 kB/s) 
debconf: delaying package configuration, since apt-utils is not installed
Selecting previously unselected package libcap2:amd64.
(Reading database ... 122835 files and directories currently installed.)
Preparing to unpack .../libcap2_1%3a2.25-1.2_amd64.deb ...
Unpacking libcap2:amd64 (1:2.25-1.2) ...
Selecting previously unselected package libidn11:amd64.
Preparing to unpack .../libidn11_1.33-2.1ubuntu1.2_amd64.deb ...
Unpacking libidn11:amd64 (1.33-2.1ubuntu1.2) ...
Selecting previously unselected package iputils-ping.
Preparing to unpack .../iputils-ping_3%3a20161105-1ubuntu3_amd64.deb ...
Unpacking iputils-ping (3:20161105-1ubuntu3) ...
Selecting previously unselected package libcap2-bin.
Preparing to unpack .../libcap2-bin_1%3a2.25-1.2_amd64.deb ...
Unpacking libcap2-bin (1:2.25-1.2) ...
Selecting previously unselected package libpam-cap:amd64.
Preparing to unpack .../libpam-cap_1%3a2.25-1.2_amd64.deb ...
Unpacking libpam-cap:amd64 (1:2.25-1.2) ...
Setting up libcap2:amd64 (1:2.25-1.2) ...
Setting up libidn11:amd64 (1.33-2.1ubuntu1.2) ...
Setting up iputils-ping (3:20161105-1ubuntu3) ...
Setting up libpam-cap:amd64 (1:2.25-1.2) ...
Setting up libcap2-bin (1:2.25-1.2) ...
Processing triggers for libc-bin (2.27-3ubuntu1.6) ...
root@labs--311032102:/home/project/ops-autoscale# ping a577abeeec4b04f138fa14a11b9e4d54-1026617604.ap-northeast-1.elb.amazonaws.com
PING a577abeeec4b04f138fa14a11b9e4d54-1026617604.ap-northeast-1.elb.amazonaws.com (52.197.127.83) 56(84) bytes of data.

root@labs--311032102:/home/project/ops-autoscale# http 52.197.127.83/orders
HTTP/1.1 200 
Connection: keep-alive
Content-Type: application/hal+json;charset=UTF-8
Date: Thu, 23 Jun 2022 00:31:31 GMT
Transfer-Encoding: chunked

{
    "_embedded": {
        "orders": []
    },
    "_links": {
        "profile": {
            "href": "http://52.197.127.83/profile/orders"
        },
        "search": {
            "href": "http://52.197.127.83/orders/search"
        },
        "self": {
            "href": "http://52.197.127.83/orders{?page,size,sort}",
            "templated": true
        }
    },
    "page": {
        "number": 0,
        "size": 20,
        "totalElements": 0,
        "totalPages": 0
    }
}

root@labs--311032102:/home/project/ops-autoscale# 


root@labs--311032102:/home/project/ops-autoscale# kubectl get pod
NAME                            READY   STATUS             RESTARTS   AGE
busybox                         0/1     CrashLoopBackOff   494        41h
my-kafka-0                      1/1     Running            1          43h
my-kafka-zookeeper-0            1/1     Running            1          43h
netcom                          1/1     Running            44         44h
netcom-d598f8f9b-tgv8l          0/1     ImagePullBackOff   0          44h
order-7b66547f76-5l6ps          1/1     Running            0          24s                    <===== order pod 가 생성되어있어야함.
siege                           1/1     Running            0          2d16h
siege-55b7f7d5fc-vwqk6          1/1     Running            0          45h
user-data                       1/1     Running            0          41h
user02-order-5fd6ff986d-4q9q6   1/1     Running            0          2d18h


###


apiVersion: "extensions/v1beta1"
kind: "Ingress"
metadata: 
  name: "shopping-ingress"
  namespace: "istio-system"
  annotations: 
    kubernetes.io/ingress.class: "nginx"
spec: 
  rules: 
    - host: "prom.service.com"
      http: 
        paths: 
          - 
            path: /
            pathType: Prefix
            backend: 
              serviceName: prometheus
              servicePort: 9090

    - host: "gra.service.com"
      http: 
        paths: 
          - 
            path: /
            pathType: Prefix
            backend: 
              serviceName: grafana
              servicePort: 3000


root@labs--311032102:/home/project/ops-autoscale# kubectl create -f shopping-ingress.yaml
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
Error from server (NotFound): error when creating "shopping-ingress.yaml": namespaces "istio-system" not found
root@labs--311032102:/home/project/ops-autoscale# kubectl create -f shopp^Cg-ingress.yaml
root@labs--311032102:/home/project/ops-autoscale# kubectl creaee ns istio-system
Error: unknown command "creaee" for "kubectl"

Did you mean this?
        create

Run 'kubectl --help' for usage.
root@labs--311032102:/home/project/ops-autoscale# kubectl create ns istio-system
namespace/istio-system created
root@labs--311032102:/home/project/ops-autoscale# kubectl create -f shopping-ingress.yaml
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
ingress.extensions/shopping-ingress created
root@labs--311032102:/home/project/ops-autoscale# kubectl ge ns
Error: unknown command "ge" for "kubectl"

Did you mean this?
        set
        get
        cp

Run 'kubectl --help' for usage.
root@labs--311032102:/home/project/ops-autoscale# kubectl get ns
NAME              STATUS   AGE
db                Active   41h
default           Active   2d20h
ingress-basic     Active   20m
istio-system      Active   16s
kube-node-lease   Active   2d20h
kube-public       Active   2d20h
kube-system       Active   2d20h

oot@labs--311032102:/home/project/ops-autoscale# 
root@labs--311032102:/home/project/ops-autoscale# http 52.197.127.83/orders
HTTP/1.1 503 Service Temporarily Unavailable
Connection: keep-alive
Content-Length: 190
Content-Type: text/html
Date: Thu, 23 Jun 2022 00:34:35 GMT

<html>
<head><title>503 Service Temporarily Unavailable</title></head>
<body>
<center><h1>503 Service Temporarily Unavailable</h1></center>
<hr><center>nginx</center>
</body>
</html>

root@labs--311032102:/home/project/ops-autoscale# http 52.197.127.83/orders
HTTP/1.1 200 
Connection: keep-alive
Content-Type: application/hal+json;charset=UTF-8
Date: Thu, 23 Jun 2022 00:34:50 GMT
Transfer-Encoding: chunked

{
    "_embedded": {
        "orders": []
    },
    "_links": {
        "profile": {
            "href": "http://52.197.127.83/profile/orders"
        },
        "search": {
            "href": "http://52.197.127.83/orders/search"
        },
        "self": {
            "href": "http://52.197.127.83/orders{?page,size,sort}",
            "templated": true
        }
    },
    "page": {
        "number": 0,
        "size": 20,
        "totalElements": 0,
        "totalPages": 0
    }
}

root@labs--311032102:/home/project/ops-autoscale# kubectl create -f shopping-ingress.yaml
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
Error from server (NotFound): error when creating "shopping-ingress.yaml": namespaces "istio-system" not found
root@labs--311032102:/home/project/ops-autoscale# kubectl create -f shopp^Cg-ingress.yaml
root@labs--311032102:/home/project/ops-autoscale# kubectl creaee ns istio-system
Error: unknown command "creaee" for "kubectl"

Did you mean this?
        create

Run 'kubectl --help' for usage.
root@labs--311032102:/home/project/ops-autoscale# kubectl create ns istio-system
namespace/istio-system created
root@labs--311032102:/home/project/ops-autoscale# kubectl create -f shopping-ingress.yaml
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
ingress.extensions/shopping-ingress created
root@labs--311032102:/home/project/ops-autoscale# kubectl ge ns
Error: unknown command "ge" for "kubectl"

Did you mean this?
        set
        get
        cp

Run 'kubectl --help' for usage.
root@labs--311032102:/home/project/ops-autoscale# kubectl get ns
NAME              STATUS   AGE
db                Active   41h
default           Active   2d20h
ingress-basic     Active   20m
istio-system      Active   16s
kube-node-lease   Active   2d20h
kube-public       Active   2d20h
kube-system       Active   2d20h
root@labs--311032102:/home/project/ops-autoscale# kubectl get ingress
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME               CLASS    HOSTS   ADDRESS                                                                        PORTS   AGE
shopping-ingress   <none>   *       a577abeeec4b04f138fa14a11b9e4d54-1026617604.ap-northeast-1.elb.amazonaws.com   80      27m
root@labs--311032102:/home/project/ops-autoscale# kubectl get ingress -ns istis-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
Error from server (NotFound): namespaces "s" not found
root@labs--311032102:/home/project/ops-autoscale# kubectl get ingress -n istis-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
No resources found in istis-system namespace.
root@labs--311032102:/home/project/ops-autoscale# kubectl get ingress -n istio-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME               CLASS    HOSTS                              ADDRESS                                                                        PORTS   AGE
shopping-ingress   <none>   prom.service.com,gra.service.com   a577abeeec4b04f138fa14a11b9e4d54-1026617604.ap-northeast-1.elb.amazonaws.com   80      84s
root@labs--311032102:/home/project/ops-autoscale# 


가상호스트를 테스트하기 위해서 C:\Windows\System32\drivers\etc 내의 hosts 파일에 아래를 추가한다:

<획득한 ingress의 External IP>  prom.service.com, gra.service.com

인터넷브라우저로 http://52.197.127.83/orders 조회하는 나온다.


## SERVICE MESH (nginx 보다 더 좋은 기능 제공 Istio)  <-- L7 의 일종..

1. https://istio.io/ URL 있음..


root@labs--311032102:/home/project/ops-autoscale# curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.11.3 TARGET_ARCH=x86_64 sh -
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   101  100   101    0     0    402      0 --:--:-- --:--:-- --:--:--   404
100  4926  100  4926    0     0  17468      0 --:--:-- --:--:-- --:--:-- 17468

Downloading istio-1.11.3 from https://github.com/istio/istio/releases/download/1.11.3/istio-1.11.3-linux-amd64.tar.gz ...

Istio 1.11.3 Download Complete!

Istio has been successfully downloaded into the istio-1.11.3 folder on your system.

Next Steps:
See https://istio.io/latest/docs/setup/install/ to add Istio to your Kubernetes cluster.

To configure the istioctl client tool for your workstation,
add the /home/project/ops-autoscale/istio-1.11.3/bin directory to your environment path variable with:
         export PATH="$PATH:/home/project/ops-autoscale/istio-1.11.3/bin"

Begin the Istio pre-installation check by running:
         istioctl x precheck 

Need more information? Visit https://istio.io/latest/docs/setup/install/ 
root@labs--311032102:/home/project/ops-autoscale# ls
ingress.yaml  istio-1.11.3  shopmall  shopping-ingress.yaml
root@labs--311032102:/home/project/ops-autoscale# cd istio-1.11.3
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# ls
LICENSE  README.md  bin  manifest.yaml  manifests  samples  tools
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# cd samples
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/samples# ls
README.md  bookinfo  custom-bootstrap  external   health-check  httpbin     kubernetes-blog  operator   security  tcp-echo
addons     certs     extauthz          grpc-echo  helloworld    jwt-server  multicluster     ratelimit  sleep     websockets
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/samples# cd ..
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# ls
LICENSE  README.md  bin  manifest.yaml  manifests  samples  tools
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# ls
LICENSE  README.md  bin  manifest.yaml  manifests  samples  tools
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# cd bin
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# ls
istioctl
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# export PATH=$PWD/bin:$PATH
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# echo $PATH
/home/project/ops-autoscale/istio-1.11.3/bin/bin:/usr/local/go/bin:/usr/lib/dart/bin:/theia/.pub-cache/bin:/opt/java/openjdk/bin:/usr/local/go-packages/bin:/usr/local/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/go-packages/bin:/opt/fzf/bin
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# cd ..
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# ls
LICENSE  README.md  bin  manifest.yaml  manifests  samples  tools
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# export PATH=$PWD/bin:$PATH
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# cd ..^C
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# export $PATH
bash: export: `/home/project/ops-autoscale/istio-1.11.3/bin:/home/project/ops-autoscale/istio-1.11.3/bin/bin:/usr/local/go/bin:/usr/lib/dart/bin:/theia/.pub-cache/bin:/opt/java/openjdk/bin:/usr/local/go-packages/bin:/usr/local/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/go-packages/bin:/opt/fzf/bin': not a valid identifier
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# which istiooctl
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# cd bin
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# cp ^C
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# ls
istioctl
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# cp istioctl /usr/bin
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# which istioctl
/home/project/ops-autoscale/istio-1.11.3/bin/istioctl
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# istioctl install --set profile=demo -y
✔ Istio core installed                                                                                                                          
✔ Istiod installed                                                                                                                              
✔ Egress gateways installed                                                                                                                     
✔ Ingress gateways installed                                                                                                                    
✔ Installation complete                                                                                                                         
Thank you for installing Istio 1.11.  Please take a few minutes to tell us about your install/upgrade experience!  https://forms.gle/kWULBRjUv7hHci7T6
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# kubectl get ns
NAME              STATUS   AGE
db                Active   41h
default           Active   2d20h
ingress-basic     Active   49m
istio-system      Active   29m
kube-node-lease   Active   2d20h
kube-public       Active   2d20h
kube-system       Active   2d20h
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# kubectl label
error: one or more resources must be specified as <resource> <name> or <resource>/<name>
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# kubectl label namespace
error: at least one label update is required
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# kubectl get ns 
NAME              STATUS   AGE
db                Active   41h
default           Active   2d20h
ingress-basic     Active   51m
istio-system      Active   30m
kube-node-lease   Active   2d20h
kube-public       Active   2d20h
kube-system       Active   2d20h
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# kubectl get all -n
Error: flag needs an argument: 'n' in -n
See 'kubectl get --help' for usage.
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# kubectl get all -n istio-system
NAME                                        READY   STATUS    RESTARTS   AGE
pod/istio-egressgateway-5fdc76bf94-2dhcl    1/1     Running   0          3m5s
pod/istio-ingressgateway-6bd7764b48-xwtwp   1/1     Running   0          3m5s
pod/istiod-675949b7c5-lwvmv                 1/1     Running   0          3m21s

NAME                           TYPE           CLUSTER-IP      EXTERNAL-IP                                                                   PORT(S)                                                                      AGE
service/istio-egressgateway    ClusterIP      10.100.59.211   <none>                                                                        80/TCP,443/TCP                                                               3m5s
service/istio-ingressgateway   LoadBalancer   10.100.98.191   ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com   15021:32581/TCP,80:30326/TCP,443:32036/TCP,31400:31166/TCP,15443:32582/TCP   3m5s
service/istiod                 ClusterIP      10.100.93.163   <none>                                                                        15010/TCP,15012/TCP,443/TCP,15014/TCP                                        3m21s

NAME                                   READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/istio-egressgateway    1/1     1            1           3m5s
deployment.apps/istio-ingressgateway   1/1     1            1           3m5s
deployment.apps/istiod                 1/1     1            1           3m21s

NAME                                              DESIRED   CURRENT   READY   AGE
replicaset.apps/istio-egressgateway-5fdc76bf94    1         1         1       3m5s
replicaset.apps/istio-ingressgateway-6bd7764b48   1         1         1       3m5s
replicaset.apps/istiod-675949b7c5                 1         1         1       3m21s


root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# kubectl get all -n ingress-basic    == 참고 ingress 
NAME                                                          READY   STATUS    RESTARTS   AGE
pod/nginx-ingress-ingress-nginx-controller-86df55b594-splfv   1/1     Running   0          51m

NAME                                                       TYPE           CLUSTER-IP      EXTERNAL-IP                                                                    PORT(S)                      AGE
service/nginx-ingress-ingress-nginx-controller             LoadBalancer   10.100.229.90   a577abeeec4b04f138fa14a11b9e4d54-1026617604.ap-northeast-1.elb.amazonaws.com   80:32170/TCP,443:31151/TCP   51m
service/nginx-ingress-ingress-nginx-controller-admission   ClusterIP      10.100.22.236   <none>                                                                         443/TCP                      51m

NAME                                                     READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx-ingress-ingress-nginx-controller   1/1     1            1           51m

NAME                                                                DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-ingress-ingress-nginx-controller-86df55b594   1         1         1       51m








root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml^C
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/bin# cd ..
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# ls
LICENSE  README.md  bin  manifest.yaml  manifests  samples  tools
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml
service/details created
serviceaccount/bookinfo-details created
deployment.apps/details-v1 created
service/ratings created
serviceaccount/bookinfo-ratings created
deployment.apps/ratings-v1 created
service/reviews created
serviceaccount/bookinfo-reviews created
deployment.apps/reviews-v1 created
deployment.apps/reviews-v2 created
deployment.apps/reviews-v3 created
service/productpage created
serviceaccount/bookinfo-productpage created
deployment.apps/productpage-v1 created
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# cat samples/bookinfo/platform/kube/bookinfo.yaml
# Copyright Istio Authors
#
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.

##################################################################################################
# This file defines the services, service accounts, and deployments for the Bookinfo sample.
#
# To apply all 4 Bookinfo services, their corresponding service accounts, and deployments:
#
#   kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml
#
# Alternatively, you can deploy any resource separately:
#
#   kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml -l service=reviews # reviews Service
#   kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml -l account=reviews # reviews ServiceAccount
#   kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml -l app=reviews,version=v3 # reviews-v3 Deployment
##################################################################################################

##################################################################################################
# Details service
##################################################################################################
apiVersion: v1
kind: Service
metadata:
  name: details
  labels:
    app: details
    service: details
spec:
  ports:
  - port: 9080
    name: http
  selector:
    app: details
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: bookinfo-details
  labels:
    account: details
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: details-v1
  labels:
    app: details
    version: v1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: details
      version: v1
  template:
    metadata:
      labels:
        app: details
        version: v1
    spec:
      serviceAccountName: bookinfo-details
      containers:
      - name: details
        image: docker.io/istio/examples-bookinfo-details-v1:1.16.2
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 9080
        securityContext:
          runAsUser: 1000
---
##################################################################################################
# Ratings service
##################################################################################################
apiVersion: v1
kind: Service
metadata:
  name: ratings
  labels:
    app: ratings
    service: ratings
spec:
  ports:
  - port: 9080
    name: http
  selector:
    app: ratings
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: bookinfo-ratings
  labels:
    account: ratings
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ratings-v1
  labels:
    app: ratings
    version: v1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ratings
      version: v1
  template:
    metadata:
      labels:
        app: ratings
        version: v1
    spec:
      serviceAccountName: bookinfo-ratings
      containers:
      - name: ratings
        image: docker.io/istio/examples-bookinfo-ratings-v1:1.16.2
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 9080
        securityContext:
          runAsUser: 1000
---
##################################################################################################
# Reviews service
##################################################################################################
apiVersion: v1
kind: Service
metadata:
  name: reviews
  labels:
    app: reviews
    service: reviews
spec:
  ports:
  - port: 9080
    name: http
  selector:
    app: reviews
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: bookinfo-reviews
  labels:
    account: reviews
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: reviews-v1
  labels:
    app: reviews
    version: v1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: reviews
      version: v1
  template:
    metadata:
      labels:
        app: reviews
        version: v1
    spec:
      serviceAccountName: bookinfo-reviews
      containers:
      - name: reviews
        image: docker.io/istio/examples-bookinfo-reviews-v1:1.16.2
        imagePullPolicy: IfNotPresent
        env:
        - name: LOG_DIR
          value: "/tmp/logs"
        ports:
        - containerPort: 9080
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: wlp-output
          mountPath: /opt/ibm/wlp/output
        securityContext:
          runAsUser: 1000
      volumes:
      - name: wlp-output
        emptyDir: {}
      - name: tmp
        emptyDir: {}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: reviews-v2
  labels:
    app: reviews
    version: v2
spec:
  replicas: 1
  selector:
    matchLabels:
      app: reviews
      version: v2
  template:
    metadata:
      labels:
        app: reviews
        version: v2
    spec:
      serviceAccountName: bookinfo-reviews
      containers:
      - name: reviews
        image: docker.io/istio/examples-bookinfo-reviews-v2:1.16.2
        imagePullPolicy: IfNotPresent
        env:
        - name: LOG_DIR
          value: "/tmp/logs"
        ports:
        - containerPort: 9080
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: wlp-output
          mountPath: /opt/ibm/wlp/output
        securityContext:
          runAsUser: 1000
      volumes:
      - name: wlp-output
        emptyDir: {}
      - name: tmp
        emptyDir: {}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: reviews-v3
  labels:
    app: reviews
    version: v3
spec:
  replicas: 1
  selector:
    matchLabels:
      app: reviews
      version: v3
  template:
    metadata:
      labels:
        app: reviews
        version: v3
    spec:
      serviceAccountName: bookinfo-reviews
      containers:
      - name: reviews
        image: docker.io/istio/examples-bookinfo-reviews-v3:1.16.2
        imagePullPolicy: IfNotPresent
        env:
        - name: LOG_DIR
          value: "/tmp/logs"
        ports:
        - containerPort: 9080
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: wlp-output
          mountPath: /opt/ibm/wlp/output
        securityContext:
          runAsUser: 1000
      volumes:
      - name: wlp-output
        emptyDir: {}
      - name: tmp
        emptyDir: {}
---
##################################################################################################
# Productpage services
##################################################################################################
apiVersion: v1
kind: Service
metadata:
  name: productpage
  labels:
    app: productpage
    service: productpage
spec:
  ports:
  - port: 9080
    name: http
  selector:
    app: productpage
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: bookinfo-productpage
  labels:
    account: productpage
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: productpage-v1
  labels:
    app: productpage
    version: v1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: productpage
      version: v1
  template:
    metadata:
      labels:
        app: productpage
        version: v1
    spec:
      serviceAccountName: bookinfo-productpage
      containers:
      - name: productpage
        image: docker.io/istio/examples-bookinfo-productpage-v1:1.16.2
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 9080
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        securityContext:
          runAsUser: 1000
      volumes:
      - name: tmp
        emptyDir: {}
---
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl ^Cply -f samples/bookinfo/platform/kube/bookinfo.yaml
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl create ns hana
namespace/hana created
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl config set -context --current --namespace=demo
Error: unknown shorthand flag: 'c' in -context
See 'kubectl config set --help' for usage.
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl config set-context --current --namespace=demo
Context "arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks" modified.
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# k get all
bash: k: command not found
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl get all
No resources found in demo namespace.
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl get ns
NAME              STATUS   AGE
db                Active   41h
default           Active   2d20h
hana              Active   90s
ingress-basic     Active   58m
istio-system      Active   38m
kube-node-lease   Active   2d20h
kube-public       Active   2d20h
kube-system       Active   2d20h
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl label namespace hana istio-injection=enabled
namespace/hana labeled
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl get ns
NAME              STATUS   AGE
db                Active   41h
default           Active   2d20h
hana              Active   2m42s
ingress-basic     Active   59m
istio-system      Active   39m
kube-node-lease   Active   2d20h
kube-public       Active   2d20h
kube-system       Active   2d20h
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl label namespace hana istio-injection-
namespace/hana labeled
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl get ns
NAME              STATUS   AGE
db                Active   41h
default           Active   2d20h
hana              Active   2m58s
ingress-basic     Active   60m
istio-system      Active   40m
kube-node-lease   Active   2d20h
kube-public       Active   2d20h
kube-system       Active   2d20h
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl label namespace hana istio-injection=enabled
namespace/hana labeled
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl get ns --show-labels
NAME              STATUS   AGE     LABELS
db                Active   41h     <none>
default           Active   2d20h   <none>
hana              Active   3m36s   istio-injection=enabled
ingress-basic     Active   60m     <none>
istio-system      Active   40m     <none>
kube-node-lease   Active   2d20h   <none>
kube-public       Active   2d20h   <none>
kube-system       Active   2d20h   <none>


root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl config set-context --current --namespace=hana
Context "arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks" modified.
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml
service/details created
serviceaccount/bookinfo-details created
deployment.apps/details-v1 created
service/ratings created
serviceaccount/bookinfo-ratings created
deployment.apps/ratings-v1 created
service/reviews created
serviceaccount/bookinfo-reviews created
deployment.apps/reviews-v1 created
deployment.apps/reviews-v2 created
deployment.apps/reviews-v3 created
service/productpage created
serviceaccount/bookinfo-productpage created
deployment.apps/productpage-v1 created



root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl get pod -n hana
NAME                              READY   STATUS    RESTARTS   AGE
details-v1-79f774bdb9-m5tz7       2/2     Running   0          23s
productpage-v1-6b746f74dc-8f5lx   2/2     Running   0          22s
ratings-v1-b6994bb9-xscmb         2/2     Running   0          23s
reviews-v1-545db77b95-bjxrx       2/2     Running   0          22s
reviews-v2-7bf8c9648f-gbjsf       2/2     Running   0          22s
reviews-v3-84779c7bbc-rf5vv       2/2     Running   0          22s
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl get svc
NAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
details       ClusterIP   10.100.21.27     <none>        9080/TCP   32s
productpage   ClusterIP   10.100.193.154   <none>        9080/TCP   32s
ratings       ClusterIP   10.100.70.12     <none>        9080/TCP   32s
reviews       ClusterIP   10.100.179.185   <none>        9080/TCP   32s
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl get ns
NAME              STATUS   AGE
db                Active   41h
default           Active   2d20h
hana              Active   7m50s
ingress-basic     Active   65m
istio-system      Active   44m
kube-node-lease   Active   2d20h
kube-public       Active   2d20h
kube-system       Active   2d20h
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl get ns --show-label
Error: unknown flag: --show-label
See 'kubectl get --help' for usage.
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl get ns --show-labels
NAME              STATUS   AGE     LABELS
db                Active   41h     <none>
default           Active   2d20h   <none>
hana              Active   8m14s   istio-injection=enabled
ingress-basic     Active   65m     <none>
istio-system      Active   45m     <none>
kube-node-lease   Active   2d20h   <none>
kube-public       Active   2d20h   <none>
kube-system       Active   2d20h   <none>

root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl exec "$(kubectl get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}')" -c ratings -- curl -sS productpage:9080/productpage | grep -o "<title>.*</title>"
<title>Simple Bookstore App</title>


root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].port}')
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].port}')
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# export $GATEWAY_URL
bash: export: `:80': not a valid identifier
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# echo $GATEWAY_URL
:80
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# echo "$GATEWAY_URL"
:80
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# echo "$INGRESS_HOST"

root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl get svc istio-ingressgateway -n istio-system
NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP                                                                   PORT(S)                                                                      AGE
istio-ingressgateway   LoadBalancer   10.100.98.191   ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com   15021:32581/TCP,80:30326/TCP,443:32036/TCP,31400:31166/TCP,15443:32582/TCP   31m
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# echo "$INGRESS_HOST"

root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# ls samples/addon
ls: cannot access 'samples/addon': No such file or directory
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# cd samples
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/samples# ls
README.md  bookinfo  custom-bootstrap  external   health-check  httpbin     kubernetes-blog  operator   security  tcp-echo
addons     certs     extauthz          grpc-echo  helloworld    jwt-server  multicluster     ratelimit  sleep     websockets
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/samples# cdaddons
bash: cdaddons: command not found
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/samples# cd addons
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/samples/addons# ls -lrt
total 284
-rw-r--r-- 1 root root  13633 Sep 21  2021 prometheus.yaml
-rw-r--r-- 1 root root  11008 Sep 21  2021 kiali.yaml
-rw-r--r-- 1 root root   2533 Sep 21  2021 jaeger.yaml
-rw-r--r-- 1 root root 245502 Sep 21  2021 grafana.yaml
drwxr-xr-x 2 root root   4096 Sep 21  2021 extras
-rw-r--r-- 1 root root   5194 Sep 21  2021 README.md
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/samples/addons# cd ../..
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl create -f samples/addons
serviceaccount/grafana created
configmap/grafana created
service/grafana created
deployment.apps/grafana created
configmap/istio-grafana-dashboards created
configmap/istio-services-grafana-dashboards created
deployment.apps/jaeger created
service/tracing created
service/zipkin created
service/jaeger-collector created
serviceaccount/kiali created
configmap/kiali created
clusterrole.rbac.authorization.k8s.io/kiali-viewer created
clusterrole.rbac.authorization.k8s.io/kiali created
clusterrolebinding.rbac.authorization.k8s.io/kiali created
role.rbac.authorization.k8s.io/kiali-controlplane created
rolebinding.rbac.authorization.k8s.io/kiali-controlplane created
service/kiali created
deployment.apps/kiali created
serviceaccount/prometheus created
configmap/prometheus created
clusterrole.rbac.authorization.k8s.io/prometheus created
clusterrolebinding.rbac.authorization.k8s.io/prometheus created
service/prometheus created
deployment.apps/prometheus created
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# cd ../..^C
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl get svc -n istio-system
NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP                                                                   PORT(S)                                                                      AGE
grafana                ClusterIP      10.100.12.143    <none>                                                                        3000/TCP                                                                     97s
istio-egressgateway    ClusterIP      10.100.59.211    <none>                                                                        80/TCP,443/TCP                                                               37m
istio-ingressgateway   LoadBalancer   10.100.98.191    ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com   15021:32581/TCP,80:30326/TCP,443:32036/TCP,31400:31166/TCP,15443:32582/TCP   37m
istiod                 ClusterIP      10.100.93.163    <none>                                                                        15010/TCP,15012/TCP,443/TCP,15014/TCP                                        37m
jaeger-collector       ClusterIP      10.100.88.187    <none>                                                                        14268/TCP,14250/TCP,9411/TCP                                                 97s
kiali                  ClusterIP      10.100.22.39     <none>                                                                        20001/TCP,9090/TCP                                                           97s
prometheus             ClusterIP      10.100.85.109    <none>                                                                        9090/TCP                                                                     97s
tracing                ClusterIP      10.100.98.27     <none>                                                                        80/TCP,16685/TCP                                                             97s
zipkin                 ClusterIP      10.100.105.134   <none>                                                                        9411/TCP                                                                     97s
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl get ingress
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
No resources found in hana namespace.
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl get ingress -n istio-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME               CLASS    HOSTS                                              ADDRESS                                                                        PORTS   AGE
shopping-ingress   <none>   prom.service.com,gra.service.com,gor.service.com   a577abeeec4b04f138fa14a11b9e4d54-1026617604.ap-northeast-1.elb.amazonaws.com   80      66m
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl delete ingress shopping-ingress -n istio-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
ingress.extensions "shopping-ingress" deleted
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# ls
LICENSE  README.md  bin  istio-ingress.yaml  manifest.yaml  manifests  samples  tools
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl create -f istio-ingress.yaml
error: error parsing istio-ingress.yaml: error converting YAML to JSON: yaml: line 2: mapping values are not allowed in this context
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl create -f istio-ingress.yaml
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
ingress.extensions/istio-ingress created
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl create -f istio-ingress.yaml^C
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl get all -n istis-system
No resources found in istis-system namespace.
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl get all -n istio-system
NAME                                        READY   STATUS    RESTARTS   AGE
pod/grafana-556f8998cd-mx59p                1/1     Running   0          7m26s
pod/istio-egressgateway-5fdc76bf94-2dhcl    1/1     Running   0          43m
pod/istio-ingressgateway-6bd7764b48-xwtwp   1/1     Running   0          43m
pod/istiod-675949b7c5-lwvmv                 1/1     Running   0          43m
pod/jaeger-5f65fdbf9b-h7xrh                 1/1     Running   0          7m26s
pod/kiali-787bc487b7-qb4k6                  1/1     Running   0          7m26s
pod/prometheus-9f4947649-5qmbf              2/2     Running   0          7m26s

NAME                           TYPE           CLUSTER-IP       EXTERNAL-IP                                                                   PORT(S)                                                                      AGE
service/grafana                ClusterIP      10.100.12.143    <none>                                                                        3000/TCP                                                                     7m26s
service/istio-egressgateway    ClusterIP      10.100.59.211    <none>                                                                        80/TCP,443/TCP                                                               43m
service/istio-ingressgateway   LoadBalancer   10.100.98.191    ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com   15021:32581/TCP,80:30326/TCP,443:32036/TCP,31400:31166/TCP,15443:32582/TCP   43m
service/istiod                 ClusterIP      10.100.93.163    <none>                                                                        15010/TCP,15012/TCP,443/TCP,15014/TCP                                        43m
service/jaeger-collector       ClusterIP      10.100.88.187    <none>                                                                        14268/TCP,14250/TCP,9411/TCP                                                 7m26s
service/kiali                  ClusterIP      10.100.22.39     <none>                                                                        20001/TCP,9090/TCP                                                           7m26s
service/prometheus             ClusterIP      10.100.85.109    <none>                                                                        9090/TCP                                                                     7m26s
service/tracing                ClusterIP      10.100.98.27     <none>                                                                        80/TCP,16685/TCP                                                             7m26s
service/zipkin                 ClusterIP      10.100.105.134   <none>                                                                        9411/TCP                                                                     7m26s

NAME                                   READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/grafana                1/1     1            1           7m26s
deployment.apps/istio-egressgateway    1/1     1            1           43m
deployment.apps/istio-ingressgateway   1/1     1            1           43m
deployment.apps/istiod                 1/1     1            1           43m
deployment.apps/jaeger                 1/1     1            1           7m26s
deployment.apps/kiali                  1/1     1            1           7m26s
deployment.apps/prometheus             1/1     1            1           7m26s

NAME                                              DESIRED   CURRENT   READY   AGE
replicaset.apps/grafana-556f8998cd                1         1         1       7m26s
replicaset.apps/istio-egressgateway-5fdc76bf94    1         1         1       43m
replicaset.apps/istio-ingressgateway-6bd7764b48   1         1         1       43m
replicaset.apps/istiod-675949b7c5                 1         1         1       43m
replicaset.apps/jaeger-5f65fdbf9b                 1         1         1       7m26s
replicaset.apps/kiali-787bc487b7                  1         1         1       7m26s
replicaset.apps/prometheus-9f4947649              1         1         1       7m26s
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl get all -n istio-system
NAME                                        READY   STATUS    RESTARTS   AGE
pod/grafana-556f8998cd-mx59p                1/1     Running   0          17m
pod/istio-egressgateway-5fdc76bf94-2dhcl    1/1     Running   0          53m
pod/istio-ingressgateway-6bd7764b48-xwtwp   1/1     Running   0          53m
pod/istiod-675949b7c5-lwvmv                 1/1     Running   0          53m
pod/jaeger-5f65fdbf9b-h7xrh                 1/1     Running   0          17m
pod/kiali-787bc487b7-qb4k6                  1/1     Running   0          17m
pod/prometheus-9f4947649-5qmbf              2/2     Running   0          17m

NAME                           TYPE           CLUSTER-IP       EXTERNAL-IP                                                                   PORT(S)                                                                      AGE
service/grafana                ClusterIP      10.100.12.143    <none>                                                                        3000/TCP                                                                     17m
service/istio-egressgateway    ClusterIP      10.100.59.211    <none>                                                                        80/TCP,443/TCP                                                               53m
service/istio-ingressgateway   LoadBalancer   10.100.98.191    ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com   15021:32581/TCP,80:30326/TCP,443:32036/TCP,31400:31166/TCP,15443:32582/TCP   53m
service/istiod                 ClusterIP      10.100.93.163    <none>                                                                        15010/TCP,15012/TCP,443/TCP,15014/TCP                                        53m
service/jaeger-collector       ClusterIP      10.100.88.187    <none>                                                                        14268/TCP,14250/TCP,9411/TCP                                                 17m
service/kiali                  ClusterIP      10.100.22.39     <none>                                                                        20001/TCP,9090/TCP                                                           17m
service/prometheus             ClusterIP      10.100.85.109    <none>                                                                        9090/TCP                                                                     17m
service/tracing                ClusterIP      10.100.98.27     <none>                                                                        80/TCP,16685/TCP                                                             17m
service/zipkin                 ClusterIP      10.100.105.134   <none>                                                                        9411/TCP                                                                     17m

NAME                                   READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/grafana                1/1     1            1           17m
deployment.apps/istio-egressgateway    1/1     1            1           53m
deployment.apps/istio-ingressgateway   1/1     1            1           53m
deployment.apps/istiod                 1/1     1            1           53m
deployment.apps/jaeger                 1/1     1            1           17m
deployment.apps/kiali                  1/1     1            1           17m
deployment.apps/prometheus             1/1     1            1           17m

NAME                                              DESIRED   CURRENT   READY   AGE
replicaset.apps/grafana-556f8998cd                1         1         1       17m
replicaset.apps/istio-egressgateway-5fdc76bf94    1         1         1       53m
replicaset.apps/istio-ingressgateway-6bd7764b48   1         1         1       53m
replicaset.apps/istiod-675949b7c5                 1         1         1       53m
replicaset.apps/jaeger-5f65fdbf9b                 1         1         1       17m
replicaset.apps/kiali-787bc487b7                  1         1         1       17m
replicaset.apps/prometheus-9f4947649              1         1         1       17m
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# 


#########

 export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[1].ip}')





########
다운로드 후, 압축을 해제한다
curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.11.3 TARGET_ARCH=x86_64 sh -
Istio 패키지 폴더로 이동시킨다
istio-< istio_full_version >:

cd istio-< istio_full_version >
해당 디렉토리에는 다음의 내용을 포함하고 있다:

- 샘플애플리케이션: `samples/`
- `istioctl` 클라이언트 툴은
  `bin/` 디렉토리에 포함되어있다.
istioctl 클라이언트를 PATH에 잡아준다:

export PATH=$PWD/bin:$PATH
Install Istio
기본적인 구성인 demo 를 기반으로 설치한다.
   $ istioctl install --set profile=demo -y
    ✔ Istio core installed
    ✔ Istiod installed
    ✔ Egress gateways installed
    ✔ Ingress gateways installed
    ✔ Installation complete
Envoy 사이드카를 생성하는 Pod 들에 자동적으로 주입하게 하기 위해 다음의 설정을 추가한다:
   $ kubectl label namespace default istio-injection=enabled
    namespace/default labeled
Deploy the sample application bookinfo
Bookinfo 샘플 애플리케이션을 설치한다
    $ kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml
    service/details created
    serviceaccount/bookinfo-details created
    deployment.apps/details-v1 created
    service/ratings created
    serviceaccount/bookinfo-ratings created
    deployment.apps/ratings-v1 created
    service/reviews created
    serviceaccount/bookinfo-reviews created
    deployment.apps/reviews-v1 created
    deployment.apps/reviews-v2 created
    deployment.apps/reviews-v3 created
    service/productpage created
    serviceaccount/bookinfo-productpage created
    deployment.apps/productpage-v1 created
애플리케이션이 시작되고 각 Pod들이 준비상태가 된다. Istio Sidecar들이 같이 배포되었을 것이다.
    $ kubectl get services
    NAME          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
    details       ClusterIP   10.0.0.212      <none>        9080/TCP   29s
    kubernetes    ClusterIP   10.0.0.1        <none>        443/TCP    25m
    productpage   ClusterIP   10.0.0.57       <none>        9080/TCP   28s
    ratings       ClusterIP   10.0.0.33       <none>        9080/TCP   29s
    reviews       ClusterIP   10.0.0.28       <none>        9080/TCP   29s
그리고 다음과 같이 확인한다:

    $ kubectl get pods
    NAME                              READY   STATUS    RESTARTS   AGE
    details-v1-558b8b4b76-2llld       2/2     Running   0          2m41s
    productpage-v1-6987489c74-lpkgl   2/2     Running   0          2m40s
    ratings-v1-7dc98c7588-vzftc       2/2     Running   0          2m41s
    reviews-v1-7f99cc4496-gdxfn       2/2     Running   0          2m41s
    reviews-v2-7d79d5bd5d-8zzqd       2/2     Running   0          2m41s
    reviews-v3-7dbcdcbc56-m8dph       2/2     Running   0          2m41s
모든 Pod 가 2/2로 표시될때까지 기다린다
모든 상태가 Running 이 될때까지 기다린다

모든것이 제대로 된 후에는 다음의 주소로 접속하여 웹 페이지 HTML 콘텐츠 내용을 확인할 수 있어야 한다.
    $ kubectl exec "$(kubectl get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}')" -c ratings -- curl -sS productpage:9080/productpage | grep -o "<title>.*</title>"
    <title>Simple Bookstore App</title>
Open the application to outside traffic
애플리케이션이 잘 디플로이 되었지만 외부에서는 접근이 되지 않은 상태이다. 외부접속이 가능하게 하기위해서는 Istio의 Ingress Gateway를 설정해야 한다.

애플리케이션들을 Istio Gateway 에 묶기위한 설정들을 배포한다:
    $ kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml
    gateway.networking.istio.io/bookinfo-gateway created
    virtualservice.networking.istio.io/bookinfo created
발생한 문제가 없는지 확인한다:
    $ istioctl analyze
    ✔ No validation issues found when analyzing namespace: default.
Determining the ingress IP and ports
다음의 환경 변수를 인그레스의 접속 주소를 얻어와 설정한다:INGRESS_HOST 와 INGRESS_PORT

$ kubectl get svc istio-ingressgateway -n istio-system
NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)                                      AGE
istio-ingressgateway   LoadBalancer   172.21.109.129   130.211.10.121  80:31380/TCP,443:31390/TCP,31400:31400/TCP   17h
어느정도 기다렸을때, EXTERNAL-IP 값이 설정되었다면 외부 로드밸런서 설정이 잘 된것이다. 만약 설정값이 기다려도 나타나지 않으면, 외부 ㄹ드밸런서가 없는 경우이다.

인그레스의 IP 와 Port 번호를 가져온다:

$ export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
$ export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].port}')
$ export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].port}')
AWS와 같은 특정한 환경에서는 IP address 대신 host명을 넘겨주는 경우가 있다.
이런 경우는 호스트명을 가져오도록 하는 명령으로 대치하여 설정한다:

$ export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
그런다음 다음의 환경변수에 Gateway URL을 생성할 수 있다: GATEWAY_URL:
    $ export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT
환경변수에 IP address 와 port 가 잘 설정되었는지 확인한다:
    $ echo "$GATEWAY_URL"
    192.168.99.100:32194
Verify external access
외부에서의 접속이 문제 없는지 확인한다.

다음의 명령을 입력하여 얻어진 url 로 브라우저를 접속하여 Bookinfo application이 잘 동작하는지 확인한다.
    $ echo "http://$GATEWAY_URL/productpage"
Bookinfo product page 를 여러번 리프래스 해본다.

동적 트래픽 제어 테스트
Review 서비스의 종류별 유입을 동적으로 변경하여 Canary 배포등의 시나리오에 적용하는 예시.

VirtualService 를 먼저 배포한다:

kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-80-20.yaml 
DestinationRule 을 배포한다:

kubectl apply -f samples/bookinfo/networking/destination-rule-reviews.yaml
80:20 의 확률로 v1과 v2가 선택됨을 확인할 수 있다.

virtual-service-reviews-80-20.yaml 의 내용을 아래와 같이 수정한다:

apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: reviews
spec:
  hosts:
    - reviews
  http:
  - route:
    - destination:
        host: reviews
        subset: v1
      weight: 0
    - destination:
        host: reviews
        subset: v2
      weight: 100

모든 트래픽 유입이 v2버전으로 전환됨을 확인한다.

View the dashboard
Istio 는 다른 텔레메트리 모니터링 툴과 같이 제공이 된다. 이 툴은 서비스 매시의 구조를 쉽게 들여다 볼 수 있도록 되어있어 서비스간 호출 구조와 핼쓰상태를 쉽게 이해할 수 있도록 제공된다.

이를 위해 다음을 설치한다: [Kiali and the other addons]({{< github_tree >}}/samples/addons). 설치가 완료될 때까지 기다린다.
    $ kubectl apply -f samples/addons
    $ kubectl rollout status deployment/kiali -n istio-system
    Waiting for deployment "kiali" rollout to finish: 0 of 1 updated replicas are available...
    deployment "kiali" successfully rolled out
중간에 오류가 생길 수 있으니, 그때는 다시 명령을 보내어 설치하면 된다.

Kiali dashboard 에 접속한다
Ingress setting for Istio Monitoring System
apiVersion: "extensions/v1beta1"
kind: "Ingress"
metadata: 
  name: "istio-ingress"
  namespace: "istio-system"
  annotations: 
    kubernetes.io/ingress.class: "nginx"
spec: 
  rules: 
    - host: "kiali.service.com"
      http: 
        paths: 
          - 
            path: /
            pathType: Prefix
            backend: 
              serviceName: kiali
              servicePort: 20001

    - host: "prom.service.com"
      http: 
        paths: 
          - 
            path: /
            pathType: Prefix
            backend: 
              serviceName: prometheus
              servicePort: 9090

    - host: "gra.service.com"
      http: 
        paths: 
          - 
            path: /
            pathType: Prefix
            backend: 
              serviceName: grafana
              servicePort: 3000
    - host: "jaeger.service.com"
      http: 
        paths: 
          - 
            path: /
            pathType: Prefix
            backend: 
              serviceName: tracing
              servicePort: 80

을 ingress.yaml 파일로 만들어 저장한후 생성한다

$ kubectl create -f ingress.yaml

생성된 ingress 의 상태를 확인한다:

$ kubectl get ingress -n istio-system -w

NAME               HOSTS   ADDRESS                                                                        PORTS   AGE
istio-ingress   *       ???   80      7m36s
아무리 기다려도 ADDRESS 부분에 값이 채워지지 않음을 알 수 있다. 원인은 내게 gateway provider 가 없기 때문이다. Ingress 는 Kubernetes 의 스펙일 뿐, 이를 실질적으로 지원하는 ingress controller 가 필요하기 때문이다. 다행히, 우리에겐 무료로 사용할 수 있는 nginx 인그레스 프로바이더를 사용할 수 있다.

Ingress Provider 설치하기
오픈소스 ingress provider 인 nginx ingress controller 를 설치하기 위해서는 하나 이상의 kubernetes 구성요소들을 설치해야 하기 때문에 이를 쉽게 Helm Chart 를 통해서 설치할 수 있다.

Helm으로 Ingress Controller 설치
Helm repo 설정
helm repo add stable https://charts.helm.sh/stable
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update
kubectl create namespace ingress-basic
nginx controller 설치
helm install nginx-ingress ingress-nginx/ingress-nginx --namespace=ingress-basic
이제, 자동으로 ingress 의 ADDRESS 부분의 설정이 채워지는 것을 확인한다:
$ kubectl get ingress -n istio-system
NAME               HOSTS   ADDRESS                                                                        PORTS   AGE
istio-ingress   *       acbdde7c8e29f451daee5605b8c7840c-1087513605.ap-northeast-2.elb.amazonaws.com   80      7m36s
아마존의 경우 획득한 주소가 도메인 네임이므로, IP Address 를 얻기위해서 ping 을 이용하여 address 를 얻는다:

ping acbdde7c8e29f451daee5605b8c7840c-1087513605.ap-northeast-2.elb.amazonaws.com

#  리턴되는 ip address 를 획득
가상호스트를 테스트하기 위해서 내의 hosts 파일에 아래를 추가한다:
(윈도우에서는 C:\Windows\System32\drivers\etc\hosts 에서 찾을 수 있고 리눅스와 맥은 /etc/hosts 파일을 수정하면 된다)

<획득한 ingress의 External IP>  kiali.service.com prom.service.com gra.service.com jaeger.service.com
파일 저장을 위하여 윈도우에서 메모장으로 열때 “관리자 권한으로 실행” 하여 메모장 애플리케이션을 열어야 하고, 리눅스와 맥에서는 “sudo vi /etc/hosts” 로 수정해야 한다.

이제 브라우저를 열고 kiali.service.com/kiali 에 접속해본다.

왼쪽 네비게이션 메뉴에서 Graph 를 선택한후 Namespace 드롭다운 메뉴에서, default 를 선택해준다.

The Kiali dashboard shows an overview of your mesh with the relationships
between the services in the Bookinfo sample application. It also provides
filters to visualize the traffic flow.

Kiali Dashboard

Uninstall
To delete the Bookinfo sample application and its configuration, see
Bookinfo cleanup.

The Istio uninstall deletes the RBAC permissions and all resources hierarchically
under the istio-system namespace. It is safe to ignore errors for non-existent
resources because they may have been deleted hierarchically.

$ kubectl delete -f samples/addons
$ istioctl manifest generate --set profile=demo | kubectl delete --ignore-not-found=true -f -
The istio-system namespace is not removed by default.
If no longer needed, use the following command to remove it:

$ kubectl delete namespace istio-system
Checkpoints
1. 모든 요구사항을 만족하는가



###

for i in $(seq 1 100); do curl -s -o /dev/null "http://$GATEWAY_URL/productpage"; done

### Traffic Mgmt & Canary 배포

root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# git clone https://github.com/redhat-developer-demos/istio-tutorial
Cloning into 'istio-tutorial'...
remote: Enumerating objects: 9264, done.
remote: Counting objects: 100% (449/449), done.
remote: Compressing objects: 100% (217/217), done.
remote: Total 9264 (delta 212), reused 407 (delta 200), pack-reused 8815
Receiving objects: 100% (9264/9264), 33.02 MiB | 5.91 MiB/s, done.
Resolving deltas: 100% (5369/5369), done.
Checking out files: 100% (407/407), done.
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# kubectl create namespace tutorial
namespace/tutorial created
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# ls
LICENSE  README.md  bin  istio-ingress.yaml  istio-tutorial  manifest.yaml  manifests  samples  tools
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3# cd istio-tutorial
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# ls
LICENSE        capture2.pcap  diferencia       generate_docs.sh      istiofiles  masterclassscripts  recommendation     site-workshop.yml
bin            customer       documentation    generate_workshop.sh  keycloak    preference          scripts            supplemental-ui
capture1.pcap  dev-site.yml   generate_dev.sh  install-app.sh        lib         readme.adoc         site-gh-pages.yml
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl apply -f <(istioctl kube-inject -f customer/kubernetes/Deployment.yml) -n tutorial  
<==== ISTIO 인젝션을 명렁어로처맇마.
serviceaccount/customer created
deployment.apps/customer created
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl get ns 
NAME              STATUS   AGE
db                Active   42h
default           Active   2d21h
hana              Active   71m
ingress-basic     Active   128m
istio-system      Active   108m
kube-node-lease   Active   2d21h
kube-public       Active   2d21h
kube-system       Active   2d21h
tutorial          Active   2m19s
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl get ns --show-label
Error: unknown flag: --show-label
See 'kubectl get --help' for usage.
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl get ns --show-labels
NAME              STATUS   AGE     LABELS
db                Active   42h     <none>
default           Active   2d21h   <none>
hana              Active   71m     istio-injection=enabled
ingress-basic     Active   128m    <none>
istio-system      Active   108m    <none>
kube-node-lease   Active   2d21h   <none>
kube-public       Active   2d21h   <none>
kube-system       Active   2d21h   <none>
tutorial          Active   2m29s   <none>
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# 



#######################


Traffic Mgmt & Canary 배포
Istio Tutorial 셋업
Git repository에서 Tutorial 리소스 가져오기
cd /home/project
git clone https://github.com/redhat-developer-demos/istio-tutorial
cd istio-tutorial
네임스페이스 생성
kubectl create namespace tutorial
Customer Service 배포 생성확인
kubectl apply -f <(istioctl kube-inject -f customer/kubernetes/Deployment.yml) -n tutorial
kubectl describe pod (Customer Pod) -n tutorial
kubectl create -f customer/kubernetes/Service.yml -n tutorial
Istio Gateway 설치 및 Customer 서비스 라우팅(VirtualService) 설정
cat customer/kubernetes/Gateway.yml
kubectl create -f customer/kubernetes/Gateway.yml -n tutorial
Istio-IngressGateway를 통한 Customer 서비스 확인
kubectl get service/istio-ingressgateway -n istio-system
해당 EXTERNAL-IP가 Istio Gateway 주소
Customer 서비스 호출
"http://(istio-ingressgateway IP)/customer"
Preference, Recommendation-v1 Service 배포
kubectl apply -f <(istioctl kube-inject -f preference/kubernetes/Deployment.yml) -n tutorial
kubectl create -f preference/kubernetes/Service.yml -n tutorial
kubectl apply -f <(istioctl kube-inject -f recommendation/kubernetes/Deployment.yml) -n tutorial
kubectl create -f recommendation/kubernetes/Service.yml -n tutorial
Istio - Traffic Routing
Simple Routing
pwd 로 현 위치가 /istio-tutorial/ 인지 확인
recommendation 서비스 추가 배포: v2
kubectl apply -f <(istioctl kube-inject -f recommendation/kubernetes/Deployment-v2.yml) -n tutorial
서비스 호출
브라우저에서 Customer 서비스(Externl-IP:8080 접속) 호출
F5(새로고침)를 10회 이상 클릭하여 다수의 요청 생성
Routing 결과 확인 - Kiali(Externl-IP:20001)
접속 서비스의 v2 의 replica 를 2로 설정

kubectl scale --replicas=2 deployment/recommendation-v2 -n tutorial
kubectl get po -n tutorial
Customer 서비스를 10회 이상
F5(새로고침)하여 서비스 호출
Routing 결과 확인 - Kiali(Externl-IP:20001) 접속

Advanced Routing
정책(VirtualService, DestinationRule) 설정 확인
kubectl get VirtualService -n tutorial -o yaml
kubectl get DestinationRule -n tutorial -o yaml
사용자 선호도에 따른 추천 서비스 라우팅 정책 설정
version-2로 100% 라우팅
kubectl create -f istiofiles/destination-rule-recommendation-v1-v2.yml -n tutorial
kubectl create -f istiofiles/virtual-service-recommendation-v2.yml -n tutorial
설정정책 확인
kubectl get VirtualService -n tutorial -o yaml
kubectl get DestinationRule -n tutorial -o yaml
서비스 확인
브라우저에서 Customer 서비스(Externl-IP:8080 접속)호출
Kiali(Externl-IP:20001), Jaeger(External-IP:80) 에서 모니터링
가중치 기반 스마트 라우팅
recommendation 서비스 v1의 가중치를 100으로 변경
kubectl replace -f istiofiles/virtual-service-recommendation-v1.yml -n tutorial
서비스 호출 및 Kiali(Externl-IP:20001)에서 모니터링
VirtualService 삭제 시, Round-Robin 방식으로 동작

kubectl delete -f istiofiles/virtual-service-recommendation-v1.yml -n tutorial
Canary 라우팅 비율별 배포 정책 예시

(90 : 10)
kubectl apply -f istiofiles/virtual-service-recommendation-v1_and_v2.yml -n tutorial
(75 : 25)
kubectl replace -f istiofiles/virtual-service-recommendation-v1_and_v2_75_25.yml -n tutorial
삭제
kubectl delete dr recommendation -n tutorial
kubectl delete vs recommendation -n tutorial
kubectl scale --replicas=1 deployment/recommendation-v2 -n tutorial
Header 정보기반 라우팅(브라우저 유형별 예시)
Firefox 브라우저로 접속 시, v2로 라우팅되도록 설정
kubectl apply -f istiofiles/destination-rule-recommendation-v1-v2.yml -n tutorial
kubectl apply -f istiofiles/virtual-service-firefox-recommendation-v2.yml -n tutorial
Firefox 브라우저와 다른 브라우저에서 접속 확인 Browser 환경이 지원되지 않을 경우
curl -A Safari Externl-IP:8080
curl -A Firefox Externl-IP:8080
삭제
kubectl delete dr recommendation -n tutorial
kubectl delete vs recommendation -n tutorial
Checkpoints
1. 모든 요구사항을 만족하는가



$###


root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl create -f customer/kubernetes/Gateway.yml -n tutorial
gateway.networking.istio.io/customer-gateway created
virtualservice.networking.istio.io/customer-gateway created
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl get service/istio-ingressgateway -n istio-system
NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP                                                                   PORT(S)                                                                      AGE
istio-ingressgateway   LoadBalancer   10.100.98.191   ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com   15021:32581/TCP,80:30326/TCP,443:32036/TCP,31400:31166/TCP,15443:32582/TCP   85m
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# "http://(istio-ingressgateway IP)/customer"
bash: http://(istio-ingressgateway IP)/customer: No such file or directory
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl apply -f <(istioctl kube-inject -f preference/kubernetes/Deployment.yml) -n tutorial
serviceaccount/preference created
deployment.apps/preference-v1 created
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl create -f preference/kubernetes/Service.yml -n tutorial
service/preference created
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl apply -f <(istioctl kube-inject -f recommendation/kubernetes/Deployment.yml) -n tutorial
serviceaccount/recommendation created
deployment.apps/recommendation-v1 created
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl create -f recommendation/kubernetes/Service.yml -n tutoriaroot@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl apply -f <(istioctl kube-inject -f preference/kubernetes/Deployment.yml) -n tutorial^C
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# echo $INGRESS_HOST

root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl get svc -n istio-system
NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP                                                                   PORT(S)                                                                      AGE
grafana                ClusterIP      10.100.12.143    <none>                                                                        3000/TCP                                                                     59m
istio-egressgateway    ClusterIP      10.100.59.211    <none>                                                                        80/TCP,443/TCP                                                               95m
istio-ingressgateway   LoadBalancer   10.100.98.191    ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com   15021:32581/TCP,80:30326/TCP,443:32036/TCP,31400:31166/TCP,15443:32582/TCP   95m
istiod                 ClusterIP      10.100.93.163    <none>                                                                        15010/TCP,15012/TCP,443/TCP,15014/TCP                                        95m
jaeger-collector       ClusterIP      10.100.88.187    <none>                                                                        14268/TCP,14250/TCP,9411/TCP                                                 59m
kiali                  ClusterIP      10.100.22.39     <none>                                                                        20001/TCP,9090/TCP                                                           59m
prometheus             ClusterIP      10.100.85.109    <none>                                                                        9090/TCP                                                                     59m
tracing                ClusterIP      10.100.98.27     <none>                                                                        80/TCP,16685/TCP                                                             59m
zipkin                 ClusterIP      10.100.105.134   <none>                                                                        9411/TCP                                                                     59m
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# ^C
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# ^C
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# echo $INGRESS_HOST=$
ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com=$
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# echo $INGRESS_HOST
ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].port}')
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# echo $INGRESS_PORT
80
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial#  export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].port}')
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# echo $SECURE_INGRESS_PORT
443
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# echo $GATEWAY_URL
ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com:80
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# for i in $(seq 1 100); do curl -s -o /dev/null "http://$GATEWAY_URL/productpage"; done
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl get ns
NAME              STATUS   AGE
db                Active   43h
default           Active   2d22h
hana              Active   91m
ingress-basic     Active   149m
istio-system      Active   128m
kube-node-lease   Active   2d22h
kube-public       Active   2d22h
kube-system       Active   2d22h
tutorial          Active   22m
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl get ns --show-labels
NAME              STATUS   AGE     LABELS
db                Active   43h     <none>
default           Active   2d22h   <none>
hana              Active   92m     istio-injection=enabled
ingress-basic     Active   149m    <none>
istio-system      Active   129m    <none>
kube-node-lease   Active   2d22h   <none>
kube-public       Active   2d22h   <none>
kube-system       Active   2d22h   <none>
tutorial          Active   23m     <none>
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl get pod -n default
NAME                              READY   STATUS             RESTARTS   AGE
busybox                           0/1     CrashLoopBackOff   520        44h
details-v1-79f774bdb9-zftj6       1/1     Running            0          95m
my-kafka-0                        1/1     Running            1          45h
my-kafka-zookeeper-0              1/1     Running            1          45h
netcom                            1/1     Running            46         46h
netcom-d598f8f9b-tgv8l            0/1     ImagePullBackOff   0          46h
order-7b66547f76-5l6ps            1/1     Running            0          134m
productpage-v1-6b746f74dc-l7czg   1/1     Running            0          95m
ratings-v1-b6994bb9-kbxbz         1/1     Running            0          95m
reviews-v1-545db77b95-8dnpp       1/1     Running            0          95m
reviews-v2-7bf8c9648f-pd8rw       1/1     Running            0          95m
reviews-v3-84779c7bbc-vhdj5       1/1     Running            0          95m
siege                             1/1     Running            0          2d19h
siege-55b7f7d5fc-vwqk6            1/1     Running            0          2d
user-data                         1/1     Running            0          43h
user02-order-5fd6ff986d-4q9q6     1/1     Running            0          2d20h
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl get pod -n hana
NAME                              READY   STATUS    RESTARTS   AGE
details-v1-79f774bdb9-m5tz7       2/2     Running   0          85m
productpage-v1-6b746f74dc-8f5lx   2/2     Running   0          85m
ratings-v1-b6994bb9-xscmb         2/2     Running   0          85m
reviews-v1-545db77b95-bjxrx       2/2     Running   0          85m
reviews-v2-7bf8c9648f-gbjsf       2/2     Running   0          85m
reviews-v3-84779c7bbc-rf5vv       2/2     Running   0          85m
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl describe details-v1-79f774bdb9-m5tz7
error: the server doesn't have a resource type "details-v1-79f774bdb9-m5tz7"
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl describe details-v1-79f774bdb9-m5tz7 -n hana
error: the server doesn't have a resource type "details-v1-79f774bdb9-m5tz7"
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl describe pod details-v1-79f774bdb9-m5tz7 -n hana
Name:         details-v1-79f774bdb9-m5tz7
Namespace:    hana
Priority:     0
Node:         ip-192-168-29-79.ap-northeast-1.compute.internal/192.168.29.79
Start Time:   Thu, 23 Jun 2022 01:24:13 +0000
Labels:       app=details
              pod-template-hash=79f774bdb9
              security.istio.io/tlsMode=istio
              service.istio.io/canonical-name=details
              service.istio.io/canonical-revision=v1
              version=v1
Annotations:  kubectl.kubernetes.io/default-container: details
              kubectl.kubernetes.io/default-logs-container: details
              kubernetes.io/psp: eks.privileged
              prometheus.io/path: /stats/prometheus
              prometheus.io/port: 15020
              prometheus.io/scrape: true
              sidecar.istio.io/status:
                {"initContainers":["istio-init"],"containers":["istio-proxy"],"volumes":["istio-envoy","istio-data","istio-podinfo","istio-token","istiod-...
Status:       Running
IP:           192.168.20.225
IPs:
  IP:           192.168.20.225
Controlled By:  ReplicaSet/details-v1-79f774bdb9
Init Containers:
  istio-init:
    Container ID:  docker://5ad903fce59ed10c2b3d4026a94031a70f1aef7c9dc7ddcc7dca1fab6c038b52
    Image:         docker.io/istio/proxyv2:1.11.3
    Image ID:      docker-pullable://istio/proxyv2@sha256:28513eb3706315b26610a53e0d66b29b09a334e3164393b9a0591f34fe47a6fd
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 23 Jun 2022 01:24:15 +0000
      Finished:     Thu, 23 Jun 2022 01:24:15 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     2
      memory:  1Gi
    Requests:
      cpu:        10m
      memory:     40Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from bookinfo-details-token-8b6lk (ro)
Containers:
  details:
    Container ID:   docker://59a08a3a1139ceb5d7ba2eb5760eca09f689c576c9f0526ed361f6b73fa38b57
    Image:          docker.io/istio/examples-bookinfo-details-v1:1.16.2
    Image ID:       docker-pullable://istio/examples-bookinfo-details-v1@sha256:18e54f81689035019e1ac78f6d2e6483fcf1d94072d047315ab193cb2ab89ae5
    Port:           9080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Thu, 23 Jun 2022 01:24:16 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from bookinfo-details-token-8b6lk (ro)
  istio-proxy:
    Container ID:  docker://3db386d6214bde297cd8ad6d6f38f9af3fb7f763793bfc1f7069923367155577
    Image:         docker.io/istio/proxyv2:1.11.3
    Image ID:      docker-pullable://istio/proxyv2@sha256:28513eb3706315b26610a53e0d66b29b09a334e3164393b9a0591f34fe47a6fd
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
      --concurrency
      2
    State:          Running
      Started:      Thu, 23 Jun 2022 01:24:16 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     2
      memory:  1Gi
    Requests:
      cpu:      10m
      memory:   40Mi
    Readiness:  http-get http://:15021/healthz/ready delay=1s timeout=3s period=2s #success=1 #failure=30
    Environment:
      JWT_POLICY:                    third-party-jwt
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      details-v1-79f774bdb9-m5tz7 (v1:metadata.name)
      POD_NAMESPACE:                 hana (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      PROXY_CONFIG:                  {}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"containerPort":9080,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     details
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      details-v1
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/hana/deployments/details-v1
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from bookinfo-details-token-8b6lk (ro)
      /var/run/secrets/tokens from istio-token (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  bookinfo-details-token-8b6lk:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  bookinfo-details-token-8b6lk
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:          <none>
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# ^C
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# ^C
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl get svc -n istio-system
NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP                                                                   PORT(S)                                                                      AGE
grafana                ClusterIP      10.100.12.143    <none>                                                                        3000/TCP                                                                     70m
istio-egressgateway    ClusterIP      10.100.59.211    <none>                                                                        80/TCP,443/TCP                                                               105m
istio-ingressgateway   LoadBalancer   10.100.98.191    ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com   15021:32581/TCP,80:30326/TCP,443:32036/TCP,31400:31166/TCP,15443:32582/TCP   105m
istiod                 ClusterIP      10.100.93.163    <none>                                                                        15010/TCP,15012/TCP,443/TCP,15014/TCP                                        105m
jaeger-collector       ClusterIP      10.100.88.187    <none>                                                                        14268/TCP,14250/TCP,9411/TCP                                                 70m
kiali                  ClusterIP      10.100.22.39     <none>                                                                        20001/TCP,9090/TCP                                                           70m
prometheus             ClusterIP      10.100.85.109    <none>                                                                        9090/TCP                                                                     70m
tracing                ClusterIP      10.100.98.27     <none>                                                                        80/TCP,16685/TCP                                                             70m
zipkin                 ClusterIP      10.100.105.134   <none>                                                                        9411/TCP                                                                     70m
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl get ing
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
No resources found in hana namespace.
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl get all -n tutorial
NAME                                     READY   STATUS    RESTARTS   AGE
pod/customer-64d5466459-p2rqt            2/2     Running   0          25m
pod/preference-v1-84cc947978-6dx56       2/2     Running   0          16m
pod/recommendation-v1-599b6cf575-49svv   2/2     Running   0          16m

NAME                     TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/customer         ClusterIP   10.100.49.210    <none>        8080/TCP   22m
service/preference       ClusterIP   10.100.1.75      <none>        8080/TCP   16m
service/recommendation   ClusterIP   10.100.120.127   <none>        8080/TCP   16m

NAME                                READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/customer            1/1     1            1           25m
deployment.apps/preference-v1       1/1     1            1           16m
deployment.apps/recommendation-v1   1/1     1            1           16m

NAME                                           DESIRED   CURRENT   READY   AGE
replicaset.apps/customer-64d5466459            1         1         1       25m
replicaset.apps/preference-v1-84cc947978       1         1         1       16m
replicaset.apps/recommendation-v1-599b6cf575   1         1         1       16m
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl get ing -n tutorial
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
No resources found in tutorial namespace.
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# ping ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com
PING ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com (54.248.142.8) 56(84) bytes of data.
^C
--- ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com ping statistics ---
14 packets transmitted, 0 received, 100% packet loss, time 13328ms

root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl get svc -n istio-system
NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP                                                                   PORT(S)                                                                      AGE
grafana                ClusterIP      10.100.12.143    <none>                                                                        3000/TCP                                                                     71m
istio-egressgateway    ClusterIP      10.100.59.211    <none>                                                                        80/TCP,443/TCP                                                               106m
istio-ingressgateway   LoadBalancer   10.100.98.191    ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com   15021:32581/TCP,80:30326/TCP,443:32036/TCP,31400:31166/TCP,15443:32582/TCP   106m
istiod                 ClusterIP      10.100.93.163    <none>                                                                        15010/TCP,15012/TCP,443/TCP,15014/TCP                                        106m
jaeger-collector       ClusterIP      10.100.88.187    <none>                                                                        14268/TCP,14250/TCP,9411/TCP                                                 71m
kiali                  ClusterIP      10.100.22.39     <none>                                                                        20001/TCP,9090/TCP                                                           71m
prometheus             ClusterIP      10.100.85.109    <none>                                                                        9090/TCP                                                                     71m
tracing                ClusterIP      10.100.98.27     <none>                                                                        80/TCP,16685/TCP                                                             71m
zipkin                 ClusterIP      10.100.105.134   <none>                                                                        9411/TCP                                                                     71m
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl get svc -n ingress-basic
NAME                                               TYPE           CLUSTER-IP      EXTERNAL-IP                                                                    PORT(S)                      AGE
nginx-ingress-ingress-nginx-controller             LoadBalancer   10.100.229.90   a577abeeec4b04f138fa14a11b9e4d54-1026617604.ap-northeast-1.elb.amazonaws.com   80:32170/TCP,443:31151/TCP   153m
nginx-ingress-ingress-nginx-controller-admission   ClusterIP      10.100.22.236   <none>                                                                         443/TCP                      153m
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# ping a577abeeec4b04f138fa14a11b9e4d54-1026617604.ap-northeast-1.elb.amazonaws.com
PING a577abeeec4b04f138fa14a11b9e4d54-1026617604.ap-northeast-1.elb.amazonaws.com (54.238.133.145) 56(84) bytes of data.
^C
--- a577abeeec4b04f138fa14a11b9e4d54-1026617604.ap-northeast-1.elb.amazonaws.com ping statistics ---
2 packets transmitted, 0 received, 100% packet loss, time 1049ms

root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# ping ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com
PING ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com (54.248.142.8) 56(84) bytes of data.
^C
--- ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com ping statistics ---
1 packets transmitted, 0 received, 100% packet loss, time 0ms

root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# ping ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com
PING ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com (52.192.109.203) 56(84) bytes of data.
^C
--- ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com ping statistics ---
2 packets transmitted, 0 received, 100% packet loss, time 1002ms


root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl get pod -n tutorial
NAME                                 READY   STATUS    RESTARTS   AGE
customer-64d5466459-p2rqt            2/2     Running   0          94m
preference-v1-84cc947978-6dx56       2/2     Running   0          85m
recommendation-v1-599b6cf575-49svv   2/2     Running   0          84m
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl apply -f <(istioctl kube-inject -f recommendation/kubernetes/Deployment-v2.yml) -n tutorial
deployment.apps/recommendation-v2 created
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# cat recommendation/kubernetes/Deployment-v2.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: recommendation
    version: v2
  name: recommendation-v2
spec:
  replicas: 1
  selector:
    matchLabels:
      app: recommendation
      version: v2
  template:
    metadata:
      labels:
        app: recommendation
        version: v2
      annotations:
        sidecar.istio.io/inject: "true"
    spec:
      containers:
      - env:
        - name: JAVA_OPTIONS
          value: -Xms15m -Xmx15m -Xmn15m
        name: recommendation          
        image: quay.io/rhdevelopers/istio-tutorial-recommendation:v2.2
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        - containerPort: 8778
          name: jolokia
          protocol: TCP
        - containerPort: 9779
          name: prometheus
          protocol: TCP
        resources:
          requests: 
            memory: "80Mi" 
            cpu: "200m" # 1/5 core
          limits:
            memory: "120Mi"
            cpu: "500m" 
        livenessProbe:
          exec:
            command:
            - curl
            - localhost:8080/health/live
          initialDelaySeconds: 5
          periodSeconds: 4
          timeoutSeconds: 1
        readinessProbe:
          exec:
            command:
            - curl
            - localhost:8080/health/ready
          initialDelaySeconds: 6
          periodSeconds: 5
          timeoutSeconds: 1
        securityContext:
          privileged: false
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl get VirtualService -n tutorial -o yaml
apiVersion: v1
items:
- apiVersion: networking.istio.io/v1beta1
  kind: VirtualService
  metadata:
    creationTimestamp: "2022-06-23T02:33:17Z"
    generation: 1
    managedFields:
    - apiVersion: networking.istio.io/v1alpha3
      fieldsType: FieldsV1
      fieldsV1:
        f:spec:
          .: {}
          f:gateways: {}
          f:hosts: {}
          f:http: {}
      manager: kubectl-create
      operation: Update
      time: "2022-06-23T02:33:17Z"
    name: customer-gateway
    namespace: tutorial
    resourceVersion: "736119"
    selfLink: /apis/networking.istio.io/v1beta1/namespaces/tutorial/virtualservices/customer-gateway
    uid: 6e38c5e0-e460-4385-ac34-2106cc3ee121
  spec:
    gateways:
    - customer-gateway
    hosts:
    - '*'
    http:
    - match:
      - uri:
          prefix: /customer
      rewrite:
        uri: /
      route:
      - destination:
          host: customer
          port:
            number: 8080
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl get DestinationRule -n tutorial -o yaml
apiVersion: v1
items: []
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# cat istiofiles/destination-rule-recommendation-v1-v2.yml
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: recommendation
spec:
  host: recommendation
  subsets:
  - labels:
      version: v1
    name: version-v1
  - labels:
      version: v2
    name: version-v2
---root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# cat istiofiles/virtual-service-recommendation-v2.yml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: recommendation
spec:
  hosts:
  - recommendation
  http:
  - route:
    - destination:
        host: recommendation
        subset: version-v2
      weight: 100
---root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl create -f istiofiles/destination-rule-recommendation-v1.yml -n tutorial
destinationrule.networking.istio.io/recommendation created
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl create -f istiofiles/virtual-service-recommendation-v2.yml -n tutorial
virtualservice.networking.istio.io/recommendation created
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl get VirtualService -n tutorial -o yaml
apiVersion: v1
items:
- apiVersion: networking.istio.io/v1beta1
  kind: VirtualService
  metadata:
    creationTimestamp: "2022-06-23T02:33:17Z"
    generation: 1
    managedFields:
    - apiVersion: networking.istio.io/v1alpha3
      fieldsType: FieldsV1
      fieldsV1:
        f:spec:
          .: {}
          f:gateways: {}
          f:hosts: {}
          f:http: {}
      manager: kubectl-create
      operation: Update
      time: "2022-06-23T02:33:17Z"
    name: customer-gateway
    namespace: tutorial
    resourceVersion: "736119"
    selfLink: /apis/networking.istio.io/v1beta1/namespaces/tutorial/virtualservices/customer-gateway
    uid: 6e38c5e0-e460-4385-ac34-2106cc3ee121
  spec:
    gateways:
    - customer-gateway
    hosts:
    - '*'
    http:
    - match:
      - uri:
          prefix: /customer
      rewrite:
        uri: /
      route:
      - destination:
          host: customer
          port:
            number: 8080
- apiVersion: networking.istio.io/v1beta1
  kind: VirtualService
  metadata:
    creationTimestamp: "2022-06-23T04:17:42Z"
    generation: 1
    managedFields:
    - apiVersion: networking.istio.io/v1alpha3
      fieldsType: FieldsV1
      fieldsV1:
        f:spec:
          .: {}
          f:hosts: {}
          f:http: {}
      manager: kubectl-create
      operation: Update
      time: "2022-06-23T04:17:42Z"
    name: recommendation
    namespace: tutorial
    resourceVersion: "757889"
    selfLink: /apis/networking.istio.io/v1beta1/namespaces/tutorial/virtualservices/recommendation
    uid: 7de3b411-ce9d-4858-8bd6-912e1d8284ae
  spec:
    hosts:
    - recommendation
    http:
    - route:
      - destination:
          host: recommendation
          subset: version-v2
        weight: 100
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl get DestinationRule -n tutorial -o yaml
apiVersion: v1
items:
- apiVersion: networking.istio.io/v1beta1
  kind: DestinationRule
  metadata:
    creationTimestamp: "2022-06-23T04:17:36Z"
    generation: 1
    managedFields:
    - apiVersion: networking.istio.io/v1alpha3
      fieldsType: FieldsV1
      fieldsV1:
        f:spec:
          .: {}
          f:host: {}
          f:subsets: {}
      manager: kubectl-create
      operation: Update
      time: "2022-06-23T04:17:36Z"
    name: recommendation
    namespace: tutorial
    resourceVersion: "757868"
    selfLink: /apis/networking.istio.io/v1beta1/namespaces/tutorial/destinationrules/recommendation
    uid: 7ba6d232-af69-449a-b1ca-df5323a10b3c
  spec:
    host: recommendation
    subsets:
    - labels:
        version: v1
      name: version-v1
    - labels:
        version: v2
      name: version-v2
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl get svc -n istio-system
NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP                                                                   PORT(S)                                                                      AGE
grafana                ClusterIP      10.100.12.143    <none>                                                                        3000/TCP                                                                     154m
istio-egressgateway    ClusterIP      10.100.59.211    <none>                                                                        80/TCP,443/TCP                                                               3h10m
istio-ingressgateway   LoadBalancer   10.100.98.191    ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com   15021:32581/TCP,80:30326/TCP,443:32036/TCP,31400:31166/TCP,15443:32582/TCP   3h10m
istiod                 ClusterIP      10.100.93.163    <none>                                                                        15010/TCP,15012/TCP,443/TCP,15014/TCP                                        3h10m
jaeger-collector       ClusterIP      10.100.88.187    <none>                                                                        14268/TCP,14250/TCP,9411/TCP                                                 154m
kiali                  ClusterIP      10.100.22.39     <none>                                                                        20001/TCP,9090/TCP                                                           154m
prometheus             ClusterIP      10.100.85.109    <none>                                                                        9090/TCP                                                                     154m
tracing                ClusterIP      10.100.98.27     <none>                                                                        80/TCP,16685/TCP                                                             154m
zipkin                 ClusterIP      10.100.105.134   <none>                                                                        9411/TCP                                                                     154m
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# ping ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com
PING ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com (52.192.138.14) 56(84) bytes of data.


^C
--- ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com ping statistics ---
84 packets transmitted, 0 received, 100% packet loss, time 84988ms

root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# cat istiofiles/virtual-service-recommendation-v1.yml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: recommendation
spec:
  hosts:
  - recommendation
  http:
  - route:
    - destination:
        host: recommendation
        subset: version-v1
      weight: 100
---root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl replace -f istiofiles/virtual-service-recommendation-v1l -n tutorial
virtualservice.networking.istio.io/recommendation replaced
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# cat istiofiles/virtual-service-recommendation-v1.yml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: recommendation
spec:
  hosts:
  - recommendation
  http:
  - route:
    - destination:
        host: recommendation
        subset: version-v1
      weight: 100
---root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl delete -f istiofiles/virtual-service-recommendation-v1. -n tutorial
virtualservice.networking.istio.io "recommendation" deleted
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# cat istiofiles/virtual-service-recommendation-v1_and_v2.yml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: recommendation
spec:
  hosts:
  - recommendation
  http:
  - route:
    - destination:
        host: recommendation
        subset: version-v1
      weight: 90
    - destination:
        host: recommendation
        subset: version-v2
      weight: 10
---root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl apply -f istiofiles/virtual-service-recommendation-v1_av2.yml -n tutorial
virtualservice.networking.istio.io/recommendation created
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# cat deployment/recommendation-v2
cat: deployment/recommendation-v2: No such file or directory
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# cat istiofiles/destination-rule-recommendation-v1-v2.yml
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: recommendation
spec:
  host: recommendation
  subsets:
  - labels:
      version: v1
    name: version-v1
  - labels:
      version: v2
    name: version-v2
---root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# cat istiofiles/virtual-service-firefox-recommendation-v2.yml 
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: recommendation
spec:
  hosts:
  - recommendation
  http:
  - match:
    - headers:
        baggage-user-agent:
          regex: .*Firefox.*
    route:
    - destination:
        host: recommendation
        subset: version-v2
  - route:
    - destination:
        host: recommendation
        subset: version-v1
---root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# cat istiofiles/destination-rule-recommendation-v1-v2.yml^C
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# cat istiofiles/destination-rule-recommendation-v1-v2.yml
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: recommendation
spec:
  host: recommendation
  subsets:
  - labels:
      version: v1
    name: version-v1
  - labels:
      version: v2
    name: version-v2
---root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# cat istiofiles/virtual-service-firefox-recommendation-v2.yml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: recommendation
spec:
  hosts:
  - recommendation
  http:
  - match:
    - headers:
        baggage-user-agent:
          regex: .*Firefox.*
    route:
    - destination:
        host: recommendation
        subset: version-v2
  - route:
    - destination:
        host: recommendation
        subset: version-v1
---root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl apply -f istiofiles/destination-rule-recommendation-v1-yml -n tutorial
Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply
destinationrule.networking.istio.io/recommendation configured
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl apply -f istiofiles/virtual-service-firefox-recommendation-v2.yml -n tutorial
virtualservice.networking.istio.io/recommendation configured
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# curl -A Safari http://52.192.138.14:8080
^C
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# curl -A Safari 52.192.138.14:8080
^C
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# curl -A Firefox 52.192.138.14:8080
^C
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl get svc -n istio-system
NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP                                                                   PORT(S)                                                                      AGE
grafana                ClusterIP      10.100.12.143    <none>                                                                        3000/TCP                                                                     162m
istio-egressgateway    ClusterIP      10.100.59.211    <none>                                                                        80/TCP,443/TCP                                                               3h18m
istio-ingressgateway   LoadBalancer   10.100.98.191    ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com   15021:32581/TCP,80:30326/TCP,443:32036/TCP,31400:31166/TCP,15443:32582/TCP   3h18m
istiod                 ClusterIP      10.100.93.163    <none>                                                                        15010/TCP,15012/TCP,443/TCP,15014/TCP                                        3h18m
jaeger-collector       ClusterIP      10.100.88.187    <none>                                                                        14268/TCP,14250/TCP,9411/TCP                                                 162m
kiali                  ClusterIP      10.100.22.39     <none>                                                                        20001/TCP,9090/TCP                                                           162m
prometheus             ClusterIP      10.100.85.109    <none>                                                                        9090/TCP                                                                     162m
tracing                ClusterIP      10.100.98.27     <none>                                                                        80/TCP,16685/TCP                                                             162m
zipkin                 ClusterIP      10.100.105.134   <none>                                                                        9411/TCP                                                                     162m
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# ping ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com
PING ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com (52.192.138.14) 56(84) bytes of data.
^C
--- ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com ping statistics ---
8 packets transmitted, 0 received, 100% packet loss, time 7196ms

root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# curl -A Firefox 52.192.138.14:8080
^C
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# curl -A Firefox 52.192.138.14
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# curl -A Safari 52.192.138.14

root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl delete dr recommendation -n tutorial

destinationrule.networking.istio.io "recommendation" deleted
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# 
root@labs--311032102:/home/project/ops-autoscale/istio-1.11.3/istio-tutorial# kubectl delete vs recommendation -n tutorial
virtualservice.networking.istio.io "recommendation" deleted


## 참고 
kubectl delete dr recommendation -n tutorial
kubectl delete vs recommendation -n tutorial
kubectl scale --replicas=1 deployment/recommendation-v2 -n tutorial

### Istio Timeout & Retry

root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get svc -n istio-system
NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP                                                                   PORT(S)                                                                      AGE
grafana                ClusterIP      10.100.12.143    <none>                                                                        3000/TCP                                                                     167m
istio-egressgateway    ClusterIP      10.100.59.211    <none>                                                                        80/TCP,443/TCP                                                               3h23m
istio-ingressgateway   LoadBalancer   10.100.98.191    ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com   15021:32581/TCP,80:30326/TCP,443:32036/TCP,31400:31166/TCP,15443:32582/TCP   3h23m
istiod                 ClusterIP      10.100.93.163    <none>                                                                        15010/TCP,15012/TCP,443/TCP,15014/TCP                                        3h23m
jaeger-collector       ClusterIP      10.100.88.187    <none>                                                                        14268/TCP,14250/TCP,9411/TCP                                                 167m
kiali                  ClusterIP      10.100.22.39     <none>                                                                        20001/TCP,9090/TCP                                                           167m
prometheus             ClusterIP      10.100.85.109    <none>                                                                        9090/TCP                                                                     167m
tracing                ClusterIP      10.100.98.27     <none>                                                                        80/TCP,16685/TCP                                                             167m
zipkin                 ClusterIP      10.100.105.134   <none>                                                                        9411/TCP                                                                     167m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl label namespace tutorial istio-injection=enabled --overwrite
namespace/tutorial labeled
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get svc -n istio-system
NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP                                                                   PORT(S)                                                                      AGE
grafana                ClusterIP      10.100.12.143    <none>                                                                        3000/TCP                                                                     168m
istio-egressgateway    ClusterIP      10.100.59.211    <none>                                                                        80/TCP,443/TCP                                                               3h24m
istio-ingressgateway   LoadBalancer   10.100.98.191    ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com   15021:32581/TCP,80:30326/TCP,443:32036/TCP,31400:31166/TCP,15443:32582/TCP   3h24m
istiod                 ClusterIP      10.100.93.163    <none>                                                                        15010/TCP,15012/TCP,443/TCP,15014/TCP                                        3h24m
jaeger-collector       ClusterIP      10.100.88.187    <none>                                                                        14268/TCP,14250/TCP,9411/TCP                                                 168m
kiali                  ClusterIP      10.100.22.39     <none>                                                                        20001/TCP,9090/TCP                                                           168m
prometheus             ClusterIP      10.100.85.109    <none>                                                                        9090/TCP                                                                     168m
tracing                ClusterIP      10.100.98.27     <none>                                                                        80/TCP,16685/TCP                                                             168m
zipkin                 ClusterIP      10.100.105.134   <none>                                                                        9411/TCP                                                                     168m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ns --show-labels
NAME              STATUS   AGE     LABELS
db                Active   44h     <none>
default           Active   2d23h   <none>
hana              Active   3h15m   istio-injection=enabled
ingress-basic     Active   4h12m   <none>
istio-system      Active   3h52m   <none>
kube-node-lease   Active   3d      <none>
kube-public       Active   3d      <none>
kube-system       Active   3d      <none>
tutorial          Active   126m    istio-injection=enabled
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ingress -n istion-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
No resources found in istion-system namespace.
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ingress -n istio-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME            CLASS    HOSTS                                                                ADDRESS                                                                        PORTS   AGE
istio-ingress   <none>   tracing.service.com,kiali.service.com,prom.service.com + 1 more...   a577abeeec4b04f138fa14a11b9e4d54-1026617604.ap-northeast-1.elb.amazonaws.com   80      163m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl edit ingress -n istio-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
Edit cancelled, no changes made.
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl edit ingress -n istio-system -o yaml > tracing.yaml
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
Vim: Warning: Output is not to a terminal

[1]+  Stopped                 kubectl edit ingress -n istio-system -o yaml > tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# ls
delivery  gateway  order  tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# vi traing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# ls -lrt
total 16
drwxr-xr-x 6 root root 4096 Jun 23 04:29 delivery
drwxr-xr-x 5 root root 4096 Jun 23 04:29 gateway
drwxr-xr-x 6 root root 4096 Jun 23 04:29 order
-rw-r--r-- 1 root root 1549 Jun 23 04:36 tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# vi tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# ls
delivery  gateway  order  tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# rm tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl edit ingress -n istio-system -o yaml > tracing.yaml
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
Vim: Warning: Output is not to a terminal

[2]+  Stopped                 kubectl edit ingress -n istio-system -o yaml > tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# s
bash: s: command not found
root@labs--311032102:/home/project/istio-resiliency-part1# ls
delivery  gateway  order  tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# rm tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# vi tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# vi tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl apply -f tracing.yaml
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply
Error from server (Conflict): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"extensions/v1beta1\",\"kind\":\"Ingress\",\"metadata\":{\"annotations\":{\"kubernetes.io/ingress.class\":\"nginx\"},\"creationTimestamp\":\"2022-06-23T01:48:53Z\",\"generation\":1,\"managedFields\":[{\"apiVersion\":\"extensions/v1beta1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:annotations\":{\".\":{},\"f:kubernetes.io/ingress.class\":{}}},\"f:spec\":{\"f:rules\":{}}},\"manager\":\"kubectl-create\",\"operation\":\"Update\",\"time\":\"2022-06-23T01:48:53Z\"},{\"apiVersion\":\"networking.k8s.io/v1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:loadBalancer\":{\"f:ingress\":{}}}},\"manager\":\"nginx-ingress-controller\",\"operation\":\"Update\",\"time\":\"2022-06-23T01:49:49Z\"}],\"name\":\"istio-ingress\",\"namespace\":\"istio-system\",\"resourceVersion\":\"728111\",\"selfLink\":\"/apis/extensions/v1beta1/namespaces/istio-system/ingresses/istio-ingress\",\"uid\":\"29ab901e-639a-43ca-b6a8-922442ac911f\"},\"spec\":{\"rules\":[{\"host\":\"kiali.service.com\",\"http\":{\"paths\":[{\"backend\":{\"serviceName\":\"kiali\",\"servicePort\":20001},\"path\":\"/\",\"pathType\":\"Prefix\"}]}},{\"host\":\"tracing.service.com\",\"http\":{\"paths\":[{\"backend\":{\"serviceName\":\"tracing\",\"servicePort\":80},\"path\":\"/\",\"pathType\":\"Prefix\"}]}},{\"host\":\"prom.service.com\",\"http\":{\"paths\":[{\"backend\":{\"serviceName\":\"prometheus\",\"servicePort\":9090},\"path\":\"/\",\"pathType\":\"Prefix\"}]}},{\"host\":\"gra.service.com\",\"http\":{\"paths\":[{\"backend\":{\"serviceName\":\"grafana\",\"servicePort\":3000},\"path\":\"/\",\"pathType\":\"Prefix\"}]}}]}}\n"},"creationTimestamp":"2022-06-23T01:48:53Z","managedFields":[{"apiVersion":"extensions/v1beta1","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:kubernetes.io/ingress.class":{}}},"f:spec":{"f:rules":{}}},"manager":"kubectl-create","operation":"Update","time":"2022-06-23T01:48:53Z"},{"apiVersion":"networking.k8s.io/v1","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:loadBalancer":{"f:ingress":{}}}},"manager":"nginx-ingress-controller","operation":"Update","time":"2022-06-23T01:49:49Z"}],"resourceVersion":"728111","uid":"29ab901e-639a-43ca-b6a8-922442ac911f"},"spec":{"rules":[{"host":"kiali.service.com","http":{"paths":[{"backend":{"serviceName":"kiali","servicePort":20001},"path":"/","pathType":"Prefix"}]}},{"host":"tracing.service.com","http":{"paths":[{"backend":{"serviceName":"tracing","servicePort":80},"path":"/","pathType":"Prefix"}]}},{"host":"prom.service.com","http":{"paths":[{"backend":{"serviceName":"prometheus","servicePort":9090},"path":"/","pathType":"Prefix"}]}},{"host":"gra.service.com","http":{"paths":[{"backend":{"serviceName":"grafana","servicePort":3000},"path":"/","pathType":"Prefix"}]}}]}}
to:
Resource: "extensions/v1beta1, Resource=ingresses", GroupVersionKind: "extensions/v1beta1, Kind=Ingress"
Name: "istio-ingress", Namespace: "istio-system"
for: "tracing.yaml": Operation cannot be fulfilled on ingresses.extensions "istio-ingress": the object has been modified; please apply your changes to the latest version and try again
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl apply -f tracing.yaml^C
root@labs--311032102:/home/project/istio-resiliency-part1# vi tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl apply -f tracing.yaml^C
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl apply -f tracing.yaml^C
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl describe ingress -n istio-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
Name:             istio-ingress
Namespace:        istio-system
Address:          a577abeeec4b04f138fa14a11b9e4d54-1026617604.ap-northeast-1.elb.amazonaws.com
Default backend:  default-http-backend:80 (<error: endpoints "default-http-backend" not found>)
Rules:
  Host                 Path  Backends
  ----                 ----  --------
  tracing.service.com  
                       /   tracing:80   192.168.43.46:16686)
  kiali.service.com    
                       /   kiali:20001   192.168.84.16:20001)
  prom.service.com     
                       /   prometheus:9090   192.168.22.220:9090)
  gra.service.com      
                       /   grafana:3000   192.168.6.36:3000)
Annotations:           kubernetes.io/ingress.class: nginx
Events:                <none>
root@labs--311032102:/home/project/istio-resiliency-part1# 
Display all 1060 possibilities? (y or n)^C
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ingress -n istio-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME            CLASS    HOSTS                                                                ADDRESS                                                                        PORTS   AGE
istio-ingress   <none>   tracing.service.com,kiali.service.com,prom.service.com + 1 more...   a577abeeec4b04f138fa14a11b9e4d54-1026617604.ap-northeast-1.elb.amazonaws.com   80      175m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl delete ingress istio-ingress -n insto-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
Error from server (NotFound): ingresses.extensions "istio-ingress" not found
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl delete ingress istio-ingress -n istio-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
ingress.extensions "istio-ingress" deleted
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ingress -n istio-system^C
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ingress -n istio-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
No resources found in istio-system namespace.
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ingress -n istio-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
No resources found in istio-system namespace.
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# ls
delivery  gateway  order  tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# vi tra*
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ns --show-labels
NAME              STATUS   AGE     LABELS
db                Active   45h     <none>
default           Active   3d      <none>
hana              Active   3h32m   istio-injection=enabled
ingress-basic     Active   4h30m   <none>
istio-system      Active   4h10m   <none>
kube-node-lease   Active   3d      <none>
kube-public       Active   3d      <none>
kube-system       Active   3d      <none>
tutorial          Active   143m    istio-injection=enabled
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl label namespace tutorial istio-injection=enabled --overwrite
namespace/tutorial not labeled
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl apply -f - <<EOF
>   apiVersion: apps/v1
>   kind: Deployment
>   metadata:
>     name: order
>     namespace: tutorial
>     labels:
>       app: order
>   spec:
>     replicas: 1
>     selector:
>       matchLabels:
>         app: order
>     template:
>       metadata:
>         labels:
>           app: order
>       spec:
>         containers:
>           - name: order
>             image: jinyoung/order:timeout
>             ports:
>               - containerPort: 8080
>             resources:
>               limits:
>                 cpu: 500m
>               requests:
>                 cpu: 200m
> EOF
deployment.apps/order created
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get pod
NAME                              READY   STATUS    RESTARTS   AGE
details-v1-79f774bdb9-m5tz7       2/2     Running   0          3h27m
productpage-v1-6b746f74dc-8f5lx   2/2     Running   0          3h27m
ratings-v1-b6994bb9-xscmb         2/2     Running   0          3h27m
reviews-v1-545db77b95-bjxrx       2/2     Running   0          3h27m
reviews-v2-7bf8c9648f-gbjsf       2/2     Running   0          3h27m
reviews-v3-84779c7bbc-rf5vv       2/2     Running   0          3h27m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get pod -n tutorial
NAME                                 READY   STATUS    RESTARTS   AGE
customer-64d5466459-p2rqt            2/2     Running   0          143m
order-6659564bbb-59xdx               2/2     Running   0          32s
preference-v1-84cc947978-6dx56       2/2     Running   0          134m
recommendation-v1-599b6cf575-49svv   2/2     Running   0          134m
recommendation-v2-6c956bc676-6xvg5   2/2     Running   0          46m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get svc -n tutorial
NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
customer         ClusterIP   10.100.49.210    <none>        8080/TCP   141m
preference       ClusterIP   10.100.1.75      <none>        8080/TCP   135m
recommendation   ClusterIP   10.100.120.127   <none>        8080/TCP   134m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl expose deploy order --port=8080 -n tutorial
service/order exposed
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get svc -n tutorial
NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
customer         ClusterIP   10.100.49.210    <none>        8080/TCP   141m
order            ClusterIP   10.100.136.253   <none>        8080/TCP   3s
preference       ClusterIP   10.100.1.75      <none>        8080/TCP   135m
recommendation   ClusterIP   10.100.120.127   <none>        8080/TCP   135m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get vs -ntutorial
NAME               GATEWAYS               HOSTS   AGE
customer-gateway   ["customer-gateway"]   ["*"]   140m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get vs -n tutorial
NAME               GATEWAYS               HOSTS   AGE
customer-gateway   ["customer-gateway"]   ["*"]   140m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get dr -n tutorial
No resources found in tutorial namespace.
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl delete vs customer-gateway
Error from server (NotFound): virtualservices.networking.istio.io "customer-gateway" not found
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl delete vs customer-gateway -n tutorial
virtualservice.networking.istio.io "customer-gateway" deleted
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl apply -f - <<EOF
>     apiVersion: networking.istio.io/v1alpha3
>     kind: VirtualService
>     metadata:
>       name: vs-order-network-rule
>       namespace: tutorial
>     spec:
>       hosts:
>       - order
>       http:
>       - route:
>         - destination:
>             host: order
>         timeout: 3s
> EOF
virtualservice.networking.istio.io/vs-order-network-rule created
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get vs -n tutorial
NAME                    GATEWAYS   HOSTS       AGE
vs-order-network-rule              ["order"]   8s
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl set image deploy/order order='jinyoung/monolith:v20121121' -n tutorial^C
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get deploy
NAME             READY   UP-TO-DATE   AVAILABLE   AGE
details-v1       1/1     1            1           3h38m
productpage-v1   1/1     1            1           3h38m
ratings-v1       1/1     1            1           3h38m
reviews-v1       1/1     1            1           3h38m
reviews-v2       1/1     1            1           3h38m
reviews-v3       1/1     1            1           3h38m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get deploy -n tutorial
NAME                READY   UP-TO-DATE   AVAILABLE   AGE
customer            1/1     1            1           154m
order               1/1     1            1           11m
preference-v1       1/1     1            1           145m
recommendation-v1   1/1     1            1           145m
recommendation-v2   1/1     1            1           57m
siege               1/1     1            1           6m31s
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get deploy^C
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl set image deploy/order order='jinyoung/monolith:v20121121' -n tutorial
deployment.apps/order image updated
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl apply -f - <<EOF
>   apiVersion: networking.istio.io/v1alpha3
>   kind: VirtualService
>   metadata:
>     name: vs-order-network-rule
>     namespace: tutorial
>   spec:
>     hosts:
>     - order
>     http:
>     - route:
>       - destination:
>           host: order
>       timeout: 3s
>       retries:
>         attempts: 3
>         perTryTimeout: 2s
>         retryOn: 5xx,retriable-4xx,gateway-error,connect-failure,refused-stream
> EOF
virtualservice.networking.istio.io/vs-order-network-rule configured
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl exec -it [siege-Pod-Instance] -c siege-nginx -n tutorial -- /bin/bash
Error from server (NotFound): pods "[siege-Pod-Instance]" not found
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get pod -n tutorial
NAME                                 READY   STATUS    RESTARTS   AGE
customer-64d5466459-p2rqt            2/2     Running   0          155m
order-6496759475-7pbkj               2/2     Running   0          38s
preference-v1-84cc947978-6dx56       2/2     Running   0          146m
recommendation-v1-599b6cf575-49svv   2/2     Running   0          146m
recommendation-v2-6c956bc676-6xvg5   2/2     Running   0          58m
siege-88f7fdd8d-x62fn                2/2     Running   0          7m27s
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl exec -it order-6496759475-7pbkj -c siege-nginx -n tutorial -- /bin/bash
Error from server (BadRequest): container siege-nginx is not valid for pod order-6496759475-7pbkj
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl exec -it order-6496759475-7pbkj -c siege-nginx -n tutorial -- /bin/bash
Error from server (BadRequest): container siege-nginx is not valid for pod order-6496759475-7pbkj
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl exec -it order-6496759475-7pbkj ^C siege-nginx -n tutorial -- /bin/bash
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get pod -n tutorial
NAME                                 READY   STATUS    RESTARTS   AGE
customer-64d5466459-p2rqt            2/2     Running   0          156m
order-6496759475-7pbkj               2/2     Running   0          115s
preference-v1-84cc947978-6dx56       2/2     Running   0          147m
recommendation-v1-599b6cf575-49svv   2/2     Running   0          147m
recommendation-v2-6c956bc676-6xvg5   2/2     Running   0          59m
siege-88f7fdd8d-x62fn                2/2     Running   0          8m44s
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl exec -it order-6496759475-7pbkj -c siege-88f7fdd8d-x62fn -n tutorial -- /bin/bash
Error from server (BadRequest): container siege-88f7fdd8d-x62fn is not valid for pod order-6496759475-7pbkj
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl exec -it siege-88f7fdd8d-x62fn -n tutorial -- /bin/bash
Defaulting container name to siege-nginx.
Use 'kubectl describe pod/siege-88f7fdd8d-x62fn -n tutorial' to see all of the containers in this pod.
root@siege-88f7fdd8d-x62fn:/# http http://order:8080/orders/ qty=5
HTTP/1.1 500 Internal Server Error
content-type: application/json;charset=UTF-8
date: Thu, 23 Jun 2022 05:06:38 GMT
server: envoy
transfer-encoding: chunked
x-envoy-upstream-service-time: 1820

{
    "error": "Internal Server Error",
    "message": "No message available",
    "path": "/orders/",
    "status": 500,
    "timestamp": "2022-06-23T05:06:38.691+0000"
}

root@siege-88f7fdd8d-x62fn:/# http http://order:8080/orders/ qty=5
HTTP/1.1 500 Internal Server Error
content-type: application/json;charset=UTF-8
date: Thu, 23 Jun 2022 05:07:31 GMT
server: envoy
transfer-encoding: chunked
x-envoy-upstream-service-time: 272

{
    "error": "Internal Server Error",
    "message": "No message available",
    "path": "/orders/",
    "status": 500,
    "timestamp": "2022-06-23T05:07:32.081+0000"
}

root@siege-88f7fdd8d-x62fn:/# http http://order:8080/orders/ qty=5
HTTP/1.1 500 Internal Server Error
content-type: application/json;charset=UTF-8
date: Thu, 23 Jun 2022 05:07:32 GMT
server: envoy
transfer-encoding: chunked
x-envoy-upstream-service-time: 279

{
    "error": "Internal Server Error",
    "message": "No message available",
    "path": "/orders/",
    "status": 500,
    "timestamp": "2022-06-23T05:07:33.100+0000"
}

root@siege-88f7fdd8d-x62fn:/# quit
bash: quit: command not found
root@siege-88f7fdd8d-x62fn:/# exit
exit
command terminated with exit code 127
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl -n default exec -it siege -- bash
root@siege:/# http http://order.tutorial:8080/orders
HTTP/1.1 200 OK
content-type: application/hal+json;charset=UTF-8
date: Thu, 23 Jun 2022 05:12:07 GMT
server: istio-envoy
transfer-encoding: chunked
x-envoy-decorator-operation: order.tutorial.svc.cluster.local:8080/*
x-envoy-upstream-service-time: 1137

{
    "_embedded": {
        "orders": []
    },
    "_links": {
        "profile": {
            "href": "http://order.tutorial:8080/profile/orders"
        },
        "search": {
            "href": "http://order.tutorial:8080/orders/search"
        },
        "self": {
            "href": "http://order.tutorial:8080/orders{?page,size,sort}",
            "templated": true
        }
    },
    "page": {
        "number": 0,
        "size": 20,
        "totalElements": 0,
        "totalPages": 0
    }
}

root@siege:/# http http://order.tutorial:8080/orders qty=5
HTTP/1.1 500 Internal Server Error
content-type: application/json;charset=UTF-8
date: Thu, 23 Jun 2022 05:12:16 GMT
server: istio-envoy
transfer-encoding: chunked
x-envoy-decorator-operation: order.tutorial.svc.cluster.local:8080/*
x-envoy-upstream-service-time: 16

{
    "error": "Internal Server Error",
    "message": "No message available",
    "path": "/orders",
    "status": 500,
    "timestamp": "2022-06-23T05:12:16.897+0000"
}

root@siege:/# exit
exit
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl config get-contex
error: unknown command "get-contex"
See 'kubectl config -h' for help and examples
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl config get-contex -n tutorial
error: unknown command "get-contex"
See 'kubectl config -h' for help and examples
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl config get-context -n tutorial
error: unknown command "get-context"
See 'kubectl config -h' for help and examples
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl config get-context
error: unknown command "get-context"
See 'kubectl config -h' for help and examples
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl config get-context
error: unknown command "get-context"
See 'kubectl config -h' for help and examples
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl config get-context
error: unknown command "get-context"
See 'kubectl config -h' for help and examples
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl config -h
Modify kubeconfig files using subcommands like "kubectl config set current-context my-context"

 The loading order follows these rules:

  1.  If the --kubeconfig flag is set, then only that file is loaded. The flag may only be set once and no merging takes
place.
  2.  If $KUBECONFIG environment variable is set, then it is used as a list of paths (normal path delimiting rules for
your system). These paths are merged. When a value is modified, it is modified in the file that defines the stanza. When
a value is created, it is created in the first file that exists. If no files in the chain exist, then it creates the
last file in the list.
  3.  Otherwise, ${HOME}/.kube/config is used and no merging takes place.

Available Commands:
  current-context Displays the current-context
  delete-cluster  Delete the specified cluster from the kubeconfig
  delete-context  Delete the specified context from the kubeconfig
  get-clusters    Display clusters defined in the kubeconfig
  get-contexts    Describe one or many contexts
  rename-context  Renames a context from the kubeconfig file.
  set             Sets an individual value in a kubeconfig file
  set-cluster     Sets a cluster entry in kubeconfig
  set-context     Sets a context entry in kubeconfig
  set-credentials Sets a user entry in kubeconfig
  unset           Unsets an individual value in a kubeconfig file
  use-context     Sets the current-context in a kubeconfig file
  view            Display merged kubeconfig settings or a specified kubeconfig file

Usage:
  kubectl config SUBCOMMAND [options]

Use "kubectl <command> --help" for more information about a given command.
Use "kubectl options" for a list of global command-line options (applies to all commands).
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl config get-contexts
CURRENT   NAME                                                         CLUSTER                                                      AUTHINFO                                                     NAMESPACE
*         arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks   arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks   arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks   hana
          kcb-test2.k8s.local                                          kcb-test2.k8s.local                                          labs--311032102                                              labs--311032102
          user02@user02-eks.ap-northeast-1.eksctl.io                   user02-eks.ap-northeast-1.eksctl.io                          user02@user02-eks.ap-northeast-1.eksctl.io                   
root@labs--311032102:/home/project/istio-resiliency-part1# ls -lrt
total 16
drwxr-xr-x 6 root root 4096 Jun 23 04:29 delivery
drwxr-xr-x 5 root root 4096 Jun 23 04:29 gateway
drwxr-xr-x 6 root root 4096 Jun 23 04:29 order
-rw-r--r-- 1 root root  843 Jun 23 04:48 tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# vi *.y
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get svc -n istio-system
NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP                                                                   PORT(S)                                                                      AGE
grafana                ClusterIP      10.100.12.143    <none>                                                                        3000/TCP                                                                     3h34m
istio-egressgateway    ClusterIP      10.100.59.211    <none>                                                                        80/TCP,443/TCP                                                               4h9m
istio-ingressgateway   LoadBalancer   10.100.98.191    ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com   15021:32581/TCP,80:30326/TCP,443:32036/TCP,31400:31166/TCP,15443:32582/TCP   4h9m
istiod                 ClusterIP      10.100.93.163    <none>                                                                        15010/TCP,15012/TCP,443/TCP,15014/TCP                                        4h10m
jaeger-collector       ClusterIP      10.100.88.187    <none>                                                                        14268/TCP,14250/TCP,9411/TCP                                                 3h34m
kiali                  ClusterIP      10.100.22.39     <none>                                                                        20001/TCP,9090/TCP                                                           3h34m
prometheus             ClusterIP      10.100.85.109    <none>                                                                        9090/TCP                                                                     3h34m
tracing                ClusterIP      10.100.98.27     <none>                                                                        80/TCP,16685/TCP                                                             3h34m
zipkin                 ClusterIP      10.100.105.134   <none>                                                                        9411/TCP                                                                     3h34m
root@labs--311032102:/home/project/istio-resiliency-part1# ping ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com
PING ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com (54.248.142.8) 56(84) bytes of data.
^C
--- ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com ping statistics ---
2 packets transmitted, 0 received, 100% packet loss, time 1053ms

root@labs--311032102:/home/project/istio-resiliency-part1# 


root@labs--311032102:/home/project/istio-resiliency-part1# kubectl create deploy siege --image=apexacme/siege-nginx -n tutorial
deployment.apps/siege created
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl exec -it [siege-Pod-Instance] -c siege-nginx -n tutorial -- /bin/bash
Error from server (NotFound): pods "[siege-Pod-Instance]" not found
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get pod -n tutorial
NAME                                 READY   STATUS    RESTARTS   AGE
customer-64d5466459-p2rqt            2/2     Running   0          148m
order-6659564bbb-59xdx               2/2     Running   0          5m5s
preference-v1-84cc947978-6dx56       2/2     Running   0          139m
recommendation-v1-599b6cf575-49svv   2/2     Running   0          139m
recommendation-v2-6c956bc676-6xvg5   2/2     Running   0          51m
siege-88f7fdd8d-x62fn                2/2     Running   0          24s
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl exec -it siege-88f7fdd8d-x62fn  -c siege-nginx -n tutorial -- /bin/bash
root@siege-88f7fdd8d-x62fn:/# siege -c30 -t20S -v --content-type "application/json" 'http://order:8080/orders POST {"productId": "1001", "qty":5}'
** SIEGE 4.0.4
** Preparing 30 concurrent users for battle.
The server is now under siege...
HTTP/1.1 504     3.05 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.06 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.06 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.05 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.06 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.06 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.06 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.04 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.06 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.06 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.05 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.05 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.06 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.06 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.06 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.04 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.06 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.04 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.08 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.11 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.12 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.10 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.12 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.12 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.10 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.12 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.12 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.12 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.12 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.11 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.01 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.04 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.04 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.04 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.04 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.04 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.04 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.04 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.04 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.04 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.04 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.04 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.04 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.04 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.04 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.04 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.04 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.00 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.05 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.01 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.01 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.02 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.06 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.02 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.02 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.02 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.02 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.02 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.02 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.02 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     0.75 secs:     212 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     0.88 secs:     212 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.63 secs:     212 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.77 secs:     212 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.83 secs:     212 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.86 secs:     212 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.99 secs:     212 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.02 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.02 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.02 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.02 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.02 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.02 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.04 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.04 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.02 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.02 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.03 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.03 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.01 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.04 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.01 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.01 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.01 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.01 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.01 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.01 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.01 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.01 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.00 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     0.70 secs:     212 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     0.54 secs:     212 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.00 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.00 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.81 secs:     212 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.07 secs:     212 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.90 secs:     212 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.90 secs:     212 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.83 secs:     212 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.97 secs:     212 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.07 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.12 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.33 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.38 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.36 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.42 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.46 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     0.88 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.63 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.62 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.64 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.78 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.84 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.86 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.85 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.92 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.01 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.01 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.01 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.00 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.00 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.81 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.90 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.74 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.62 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.90 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.82 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.90 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.90 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.89 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.89 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.91 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.68 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.72 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.77 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.71 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.82 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.79 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.80 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.01 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.90 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.67 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.68 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.84 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.78 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.85 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.90 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.93 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.05 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.03 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     0.59 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.79 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.26 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.81 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.01 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.88 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.88 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.90 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.09 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.01 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.10 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.71 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.88 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.89 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.90 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.89 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.88 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.90 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.81 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.00 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.03 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     3.01 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.90 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.71 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.01 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.81 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     0.79 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.71 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.09 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.02 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.92 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.02 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.89 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.90 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.99 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     0.69 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.82 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.71 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.01 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.80 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.80 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.80 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     0.70 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.72 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.81 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.90 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     2.00 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.81 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.71 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.80 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.80 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.80 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.89 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.79 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.79 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.80 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 504     3.01 secs:      24 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.91 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.70 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.69 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.69 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.78 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.89 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.80 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.90 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.90 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.89 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.61 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.69 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.68 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.71 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.70 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.88 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.70 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.89 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.69 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.90 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.70 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.69 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.99 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.68 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.81 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.71 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.79 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.79 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.68 secs:     214 bytes ==> POST http://order:8080/orders
HTTP/1.1 201     1.80 secs:     214 bytes ==> POST http://order:8080/orders

Lifting the server siege...
Transactions:                    145 hits
Availability:                  61.18 %
Elapsed time:                  19.46 secs
Data transferred:               0.03 MB
Response time:                  3.85 secs
Transaction rate:               7.45 trans/sec
Throughput:                     0.00 MB/sec
Concurrency:                   28.66
Successful transactions:         145
Failed transactions:              92
Longest transaction:            3.12
Shortest transaction:           0.54
 
root@siege-88f7fdd8d-x62fn:/# ^C
root@siege-88f7fdd8d-x62fn:/# exit
exit
command terminated with exit code 130
root@labs--311032102:/home/project/istio-resiliency-part1# 


################################################


apiVersion: apps/v1
kind: Deployment
metadata:
  name: order
  namespace: tutorial
  labels:
    app: order                          // 디플로이되는 영역명
spec:
  replicas: 1
  selector:
    matchLabels:
      app: order                         // 레프리카 영역명

  template:                              // POD 영역
    metadata:
      labels:   
        app: order                      // POD부분에 대한 명
    spec:
      containers:
        - name: order                             //컨테이너 2개는 각각의 이미지를 물고 올라간다. (컨테이너1 = POD1개임)
          image: jinyoung/order:timeout
          ports:
            - containerPort: 8080
          resources:
            limits:
              cpu: 500m
            requests:
              cpu: 200m
        - name: ordertwo
          image: jinyoung/order:timeout
          ports:
            - containerPort: 8081
          resources:
            limits:
              cpu: 500m
            requests:
              cpu: 200m

####

root@labs--311032102:/home/project/istio-resiliency-part1# kubectl apply -f - <<EOF
>   apiVersion: apps/v1
>   kind: Deployment
>   metadata:
>     name: order
>     namespace: tutorial
>     labels:
>       app: order
>   spec:
>     replicas: 1
>     selector:
>       matchLabels:
>         app: order
>     template:
>       metadata:
>         labels:
>           app: order
>       spec:
>         containers:
>           - name: order
>             image: jinyoung/order:timeout
>             ports:
>               - containerPort: 8080
>             resources:
>               limits:
>                 cpu: 500m
>               requests:
>                 cpu: 200m
> EOF
deployment.apps/order configured
root@labs--311032102:/home/project/istio-resiliency-part1# ^C
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl apply -f test.yaml 
deployment.apps/order-test created
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get po -n tutorial
NAME                                 READY   STATUS    RESTARTS   AGE
customer-64d5466459-p2rqt            2/2     Running   0          3h1m
order-6659564bbb-2kzhr               2/2     Running   0          2m35s
order-test-6c6d6b7b79-mzrq8          2/3     Running   0          9s
preference-v1-84cc947978-6dx56       2/2     Running   0          172m
recommendation-v1-599b6cf575-49svv   2/2     Running   0          171m
recommendation-v2-6c956bc676-6xvg5   2/2     Running   0          83m
siege-88f7fdd8d-x62fn                2/2     Running   0          33m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl d^C
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get rs
NAME                        DESIRED   CURRENT   READY   AGE
details-v1-79f774bdb9       1         1         1       4h8m
productpage-v1-6b746f74dc   1         1         1       4h8m
ratings-v1-b6994bb9         1         1         1       4h8m
reviews-v1-545db77b95       1         1         1       4h8m
reviews-v2-7bf8c9648f       1         1         1       4h8m
reviews-v3-84779c7bbc       1         1         1       4h8m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get deployment
NAME             READY   UP-TO-DATE   AVAILABLE   AGE
details-v1       1/1     1            1           4h8m
productpage-v1   1/1     1            1           4h8m
ratings-v1       1/1     1            1           4h8m
reviews-v1       1/1     1            1           4h8m
reviews-v2       1/1     1            1           4h8m
reviews-v3       1/1     1            1           4h8m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get rs
NAME                        DESIRED   CURRENT   READY   AGE
details-v1-79f774bdb9       1         1         1       4h8m
productpage-v1-6b746f74dc   1         1         1       4h8m
ratings-v1-b6994bb9         1         1         1       4h8m
reviews-v1-545db77b95       1         1         1       4h8m
reviews-v2-7bf8c9648f       1         1         1       4h8m
reviews-v3-84779c7bbc       1         1         1       4h8m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get pod
NAME                              READY   STATUS    RESTARTS   AGE
details-v1-79f774bdb9-m5tz7       2/2     Running   0          4h8m
productpage-v1-6b746f74dc-8f5lx   2/2     Running   0          4h8m
ratings-v1-b6994bb9-xscmb         2/2     Running   0          4h8m
reviews-v1-545db77b95-bjxrx       2/2     Running   0          4h8m
reviews-v2-7bf8c9648f-gbjsf       2/2     Running   0          4h8m
reviews-v3-84779c7bbc-rf5vv       2/2     Running   0          4h8m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get replicaset
NAME                        DESIRED   CURRENT   READY   AGE
details-v1-79f774bdb9       1         1         1       4h9m
productpage-v1-6b746f74dc   1         1         1       4h9m
ratings-v1-b6994bb9         1         1         1       4h9m
reviews-v1-545db77b95       1         1         1       4h9m
reviews-v2-7bf8c9648f       1         1         1       4h9m
reviews-v3-84779c7bbc       1         1         1       4h9m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl edit rs details-v1-79f774bdb9
Edit cancelled, no changes made.
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get rs -n tutorial
NAME                           DESIRED   CURRENT   READY   AGE
customer-64d5466459            1         1         1       3h8m
order-6496759475               0         0         0       33m
order-6659564bbb               1         1         1       45m
order-test-6c6d6b7b79          1         1         1       7m41s
preference-v1-84cc947978       1         1         1       179m
recommendation-v1-599b6cf575   1         1         1       179m
recommendation-v2-6c956bc676   1         1         1       91m
siege-88f7fdd8d                1         1         1       40m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl edit rs order-6659564bbb -n tutorial
Edit cancelled, no changes made.
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl expose deployment order -n tutorial
Error from server (AlreadyExists): services "order" already exists
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl edit svc order
Error from server (NotFound): services "order" not found
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl edit svc order -n tutorial
Edit cancelled, no changes made.
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl edit svc order -n tutorial^C
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get po --show-labels
NAME                              READY   STATUS    RESTARTS   AGE     LABELS
details-v1-79f774bdb9-m5tz7       2/2     Running   0          4h16m   app=details,pod-template-hash=79f774bdb9,security.istio.io/tlsMode=istio,service.istio.io/canonical-name=details,service.istio.io/canonical-revision=v1,version=v1
productpage-v1-6b746f74dc-8f5lx   2/2     Running   0          4h15m   app=productpage,pod-template-hash=6b746f74dc,security.istio.io/tlsMode=istio,service.istio.io/canonical-name=productpage,service.istio.io/canonical-revision=v1,version=v1
ratings-v1-b6994bb9-xscmb         2/2     Running   0          4h16m   app=ratings,pod-template-hash=b6994bb9,security.istio.io/tlsMode=istio,service.istio.io/canonical-name=ratings,service.istio.io/canonical-revision=v1,version=v1
reviews-v1-545db77b95-bjxrx       2/2     Running   0          4h15m   app=reviews,pod-template-hash=545db77b95,security.istio.io/tlsMode=istio,service.istio.io/canonical-name=reviews,service.istio.io/canonical-revision=v1,version=v1
reviews-v2-7bf8c9648f-gbjsf       2/2     Running   0          4h15m   app=reviews,pod-template-hash=7bf8c9648f,security.istio.io/tlsMode=istio,service.istio.io/canonical-name=reviews,service.istio.io/canonical-revision=v2,version=v2
reviews-v3-84779c7bbc-rf5vv       2/2     Running   0          4h15m   app=reviews,pod-template-hash=84779c7bbc,security.istio.io/tlsMode=istio,service.istio.io/canonical-name=reviews,service.istio.io/canonical-revision=v3,version=v3
root@labs--311032102:/home/project/istio-resiliency-part1# 



## Istio Circuit Breaker

root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get svc -n istio-system
NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP                                                                   PORT(S)                                                                      AGE
grafana                ClusterIP      10.100.12.143    <none>                                                                        3000/TCP                                                                     167m
istio-egressgateway    ClusterIP      10.100.59.211    <none>                                                                        80/TCP,443/TCP                                                               3h23m
istio-ingressgateway   LoadBalancer   10.100.98.191    ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com   15021:32581/TCP,80:30326/TCP,443:32036/TCP,31400:31166/TCP,15443:32582/TCP   3h23m
istiod                 ClusterIP      10.100.93.163    <none>                                                                        15010/TCP,15012/TCP,443/TCP,15014/TCP                                        3h23m
jaeger-collector       ClusterIP      10.100.88.187    <none>                                                                        14268/TCP,14250/TCP,9411/TCP                                                 167m
kiali                  ClusterIP      10.100.22.39     <none>                                                                        20001/TCP,9090/TCP                                                           167m
prometheus             ClusterIP      10.100.85.109    <none>                                                                        9090/TCP                                                                     167m
tracing                ClusterIP      10.100.98.27     <none>                                                                        80/TCP,16685/TCP                                                             167m
zipkin                 ClusterIP      10.100.105.134   <none>                                                                        9411/TCP                                                                     167m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl label namespace tutorial istio-injection=enabled --overwrite
namespace/tutorial labeled
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get svc -n istio-system
NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP                                                                   PORT(S)                                                                      AGE
grafana                ClusterIP      10.100.12.143    <none>                                                                        3000/TCP                                                                     168m
istio-egressgateway    ClusterIP      10.100.59.211    <none>                                                                        80/TCP,443/TCP                                                               3h24m
istio-ingressgateway   LoadBalancer   10.100.98.191    ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com   15021:32581/TCP,80:30326/TCP,443:32036/TCP,31400:31166/TCP,15443:32582/TCP   3h24m
istiod                 ClusterIP      10.100.93.163    <none>                                                                        15010/TCP,15012/TCP,443/TCP,15014/TCP                                        3h24m
jaeger-collector       ClusterIP      10.100.88.187    <none>                                                                        14268/TCP,14250/TCP,9411/TCP                                                 168m
kiali                  ClusterIP      10.100.22.39     <none>                                                                        20001/TCP,9090/TCP                                                           168m
prometheus             ClusterIP      10.100.85.109    <none>                                                                        9090/TCP                                                                     168m
tracing                ClusterIP      10.100.98.27     <none>                                                                        80/TCP,16685/TCP                                                             168m
zipkin                 ClusterIP      10.100.105.134   <none>                                                                        9411/TCP                                                                     168m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ns --show-labels
NAME              STATUS   AGE     LABELS
db                Active   44h     <none>
default           Active   2d23h   <none>
hana              Active   3h15m   istio-injection=enabled
ingress-basic     Active   4h12m   <none>
istio-system      Active   3h52m   <none>
kube-node-lease   Active   3d      <none>
kube-public       Active   3d      <none>
kube-system       Active   3d      <none>
tutorial          Active   126m    istio-injection=enabled
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ingress -n istion-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
No resources found in istion-system namespace.
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ingress -n istio-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME            CLASS    HOSTS                                                                ADDRESS                                                                        PORTS   AGE
istio-ingress   <none>   tracing.service.com,kiali.service.com,prom.service.com + 1 more...   a577abeeec4b04f138fa14a11b9e4d54-1026617604.ap-northeast-1.elb.amazonaws.com   80      163m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl edit ingress -n istio-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
Edit cancelled, no changes made.
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl edit ingress -n istio-system -o yaml > tracing.yaml
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
Vim: Warning: Output is not to a terminal

[1]+  Stopped                 kubectl edit ingress -n istio-system -o yaml > tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# ls
delivery  gateway  order  tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# vi traing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# ls -lrt
total 16
drwxr-xr-x 6 root root 4096 Jun 23 04:29 delivery
drwxr-xr-x 5 root root 4096 Jun 23 04:29 gateway
drwxr-xr-x 6 root root 4096 Jun 23 04:29 order
-rw-r--r-- 1 root root 1549 Jun 23 04:36 tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# vi tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# ls
delivery  gateway  order  tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# rm tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl edit ingress -n istio-system -o yaml > tracing.yaml
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
Vim: Warning: Output is not to a terminal

[2]+  Stopped                 kubectl edit ingress -n istio-system -o yaml > tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# s
bash: s: command not found
root@labs--311032102:/home/project/istio-resiliency-part1# ls
delivery  gateway  order  tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# rm tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# vi tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# vi tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl apply -f tracing.yaml
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply
Error from server (Conflict): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"extensions/v1beta1\",\"kind\":\"Ingress\",\"metadata\":{\"annotations\":{\"kubernetes.io/ingress.class\":\"nginx\"},\"creationTimestamp\":\"2022-06-23T01:48:53Z\",\"generation\":1,\"managedFields\":[{\"apiVersion\":\"extensions/v1beta1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:annotations\":{\".\":{},\"f:kubernetes.io/ingress.class\":{}}},\"f:spec\":{\"f:rules\":{}}},\"manager\":\"kubectl-create\",\"operation\":\"Update\",\"time\":\"2022-06-23T01:48:53Z\"},{\"apiVersion\":\"networking.k8s.io/v1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:loadBalancer\":{\"f:ingress\":{}}}},\"manager\":\"nginx-ingress-controller\",\"operation\":\"Update\",\"time\":\"2022-06-23T01:49:49Z\"}],\"name\":\"istio-ingress\",\"namespace\":\"istio-system\",\"resourceVersion\":\"728111\",\"selfLink\":\"/apis/extensions/v1beta1/namespaces/istio-system/ingresses/istio-ingress\",\"uid\":\"29ab901e-639a-43ca-b6a8-922442ac911f\"},\"spec\":{\"rules\":[{\"host\":\"kiali.service.com\",\"http\":{\"paths\":[{\"backend\":{\"serviceName\":\"kiali\",\"servicePort\":20001},\"path\":\"/\",\"pathType\":\"Prefix\"}]}},{\"host\":\"tracing.service.com\",\"http\":{\"paths\":[{\"backend\":{\"serviceName\":\"tracing\",\"servicePort\":80},\"path\":\"/\",\"pathType\":\"Prefix\"}]}},{\"host\":\"prom.service.com\",\"http\":{\"paths\":[{\"backend\":{\"serviceName\":\"prometheus\",\"servicePort\":9090},\"path\":\"/\",\"pathType\":\"Prefix\"}]}},{\"host\":\"gra.service.com\",\"http\":{\"paths\":[{\"backend\":{\"serviceName\":\"grafana\",\"servicePort\":3000},\"path\":\"/\",\"pathType\":\"Prefix\"}]}}]}}\n"},"creationTimestamp":"2022-06-23T01:48:53Z","managedFields":[{"apiVersion":"extensions/v1beta1","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:kubernetes.io/ingress.class":{}}},"f:spec":{"f:rules":{}}},"manager":"kubectl-create","operation":"Update","time":"2022-06-23T01:48:53Z"},{"apiVersion":"networking.k8s.io/v1","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:loadBalancer":{"f:ingress":{}}}},"manager":"nginx-ingress-controller","operation":"Update","time":"2022-06-23T01:49:49Z"}],"resourceVersion":"728111","uid":"29ab901e-639a-43ca-b6a8-922442ac911f"},"spec":{"rules":[{"host":"kiali.service.com","http":{"paths":[{"backend":{"serviceName":"kiali","servicePort":20001},"path":"/","pathType":"Prefix"}]}},{"host":"tracing.service.com","http":{"paths":[{"backend":{"serviceName":"tracing","servicePort":80},"path":"/","pathType":"Prefix"}]}},{"host":"prom.service.com","http":{"paths":[{"backend":{"serviceName":"prometheus","servicePort":9090},"path":"/","pathType":"Prefix"}]}},{"host":"gra.service.com","http":{"paths":[{"backend":{"serviceName":"grafana","servicePort":3000},"path":"/","pathType":"Prefix"}]}}]}}
to:
Resource: "extensions/v1beta1, Resource=ingresses", GroupVersionKind: "extensions/v1beta1, Kind=Ingress"
Name: "istio-ingress", Namespace: "istio-system"
for: "tracing.yaml": Operation cannot be fulfilled on ingresses.extensions "istio-ingress": the object has been modified; please apply your changes to the latest version and try again
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl apply -f tracing.yaml^C
root@labs--311032102:/home/project/istio-resiliency-part1# vi tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl apply -f tracing.yaml^C
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl apply -f tracing.yaml^C
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl describe ingress -n istio-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
Name:             istio-ingress
Namespace:        istio-system
Address:          a577abeeec4b04f138fa14a11b9e4d54-1026617604.ap-northeast-1.elb.amazonaws.com
Default backend:  default-http-backend:80 (<error: endpoints "default-http-backend" not found>)
Rules:
  Host                 Path  Backends
  ----                 ----  --------
  tracing.service.com  
                       /   tracing:80   192.168.43.46:16686)
  kiali.service.com    
                       /   kiali:20001   192.168.84.16:20001)
  prom.service.com     
                       /   prometheus:9090   192.168.22.220:9090)
  gra.service.com      
                       /   grafana:3000   192.168.6.36:3000)
Annotations:           kubernetes.io/ingress.class: nginx
Events:                <none>
root@labs--311032102:/home/project/istio-resiliency-part1# 
Display all 1060 possibilities? (y or n)^C
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ingress -n istio-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME            CLASS    HOSTS                                                                ADDRESS                                                                        PORTS   AGE
istio-ingress   <none>   tracing.service.com,kiali.service.com,prom.service.com + 1 more...   a577abeeec4b04f138fa14a11b9e4d54-1026617604.ap-northeast-1.elb.amazonaws.com   80      175m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl delete ingress istio-ingress -n insto-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
Error from server (NotFound): ingresses.extensions "istio-ingress" not found
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl delete ingress istio-ingress -n istio-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
ingress.extensions "istio-ingress" deleted
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ingress -n istio-system^C
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ingress -n istio-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
No resources found in istio-system namespace.
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ingress -n istio-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
No resources found in istio-system namespace.
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# ls
delivery  gateway  order  tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# vi tra*
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ns --show-labels
NAME              STATUS   AGE     LABELS
db                Active   45h     <none>
default           Active   3d      <none>
hana              Active   3h32m   istio-injection=enabled
ingress-basic     Active   4h30m   <none>
istio-system      Active   4h10m   <none>
kube-node-lease   Active   3d      <none>
kube-public       Active   3d      <none>
kube-system       Active   3d      <none>
tutorial          Active   143m    istio-injection=enabled
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl label namespace tutorial istio-injection=enabled --overwrite
namespace/tutorial not labeled
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl apply -f - <<EOF
>   apiVersion: apps/v1
>   kind: Deployment
>   metadata:
>     name: order
>     namespace: tutorial
>     labels:
>       app: order
>   spec:
>     replicas: 1
>     selector:
>       matchLabels:
>         app: order
>     template:
>       metadata:
>         labels:
>           app: order
>       spec:
>         containers:
>           - name: order
>             image: jinyoung/order:timeout
>             ports:
>               - containerPort: 8080
>             resources:
>               limits:
>                 cpu: 500m
>               requests:
>                 cpu: 200m
> EOF
deployment.apps/order created
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get pod
NAME                              READY   STATUS    RESTARTS   AGE
details-v1-79f774bdb9-m5tz7       2/2     Running   0          3h27m
productpage-v1-6b746f74dc-8f5lx   2/2     Running   0          3h27m
ratings-v1-b6994bb9-xscmb         2/2     Running   0          3h27m
reviews-v1-545db77b95-bjxrx       2/2     Running   0          3h27m
reviews-v2-7bf8c9648f-gbjsf       2/2     Running   0          3h27m
reviews-v3-84779c7bbc-rf5vv       2/2     Running   0          3h27m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get pod -n tutorial
NAME                                 READY   STATUS    RESTARTS   AGE
customer-64d5466459-p2rqt            2/2     Running   0          143m
order-6659564bbb-59xdx               2/2     Running   0          32s
preference-v1-84cc947978-6dx56       2/2     Running   0          134m
recommendation-v1-599b6cf575-49svv   2/2     Running   0          134m
recommendation-v2-6c956bc676-6xvg5   2/2     Running   0          46m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get svc -n tutorial
NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
customer         ClusterIP   10.100.49.210    <none>        8080/TCP   141m
preference       ClusterIP   10.100.1.75      <none>        8080/TCP   135m
recommendation   ClusterIP   10.100.120.127   <none>        8080/TCP   134m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl expose deploy order --port=8080 -n tutorial
service/order exposed
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get svc -n tutorial
NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
customer         ClusterIP   10.100.49.210    <none>        8080/TCP   141m
order            ClusterIP   10.100.136.253   <none>        8080/TCP   3s
preference       ClusterIP   10.100.1.75      <none>        8080/TCP   135m
recommendation   ClusterIP   10.100.120.127   <none>        8080/TCP   135m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get vs -ntutorial
NAME               GATEWAYS               HOSTS   AGE
customer-gateway   ["customer-gateway"]   ["*"]   140m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get vs -n tutorial
NAME               GATEWAYS               HOSTS   AGE
customer-gateway   ["customer-gateway"]   ["*"]   140m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get dr -n tutorial
No resources found in tutorial namespace.
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl delete vs customer-gateway
Error from server (NotFound): virtualservices.networking.istio.io "customer-gateway" not found
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl delete vs customer-gateway -n tutorial
virtualservice.networking.istio.io "customer-gateway" deleted
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl apply -f - <<EOF
>     apiVersion: networking.istio.io/v1alpha3
>     kind: VirtualService
>     metadata:
>       name: vs-order-network-rule
>       namespace: tutorial
>     spec:
>       hosts:
>       - order
>       http:
>       - route:
>         - destination:
>             host: order
>         timeout: 3s
> EOF
virtualservice.networking.istio.io/vs-order-network-rule created
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl apply -f - <<EOF
    apiVersion: networking.istio.io/v1alpha3
    kind: VirtualService
    metadata:
      name: vs-order-network-rule
      namespace: tutorial
    spec:
      hosts:
      - order
      http:
      - route:
        - destination:
            host: order                                            delete vs customer-gateway -n tutorial











                                                                   get vs -n tutorial
NAME                    GATEWAYS   HOSTS       AGE
vs-order-network-rule              ["order"]   8s
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl set image deploy/order order='jinyoung/monolith:v20121121' -n tutorial^C
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get deploy
NAME             READY   UP-TO-DATE   AVAILABLE   AGE
details-v1       1/1     1            1           3h38m
productpage-v1   1/1     1            1           3h38m
ratings-v1       1/1     1            1           3h38m
reviews-v1       1/1     1            1           3h38m
reviews-v2       1/1     1            1           3h38m
reviews-v3       1/1     1            1           3h38m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get deploy -n tutorial
NAME                READY   UP-TO-DATE   AVAILABLE   AGE
customer            1/1     1            1           154m
order               1/1     1            1           11m
preference-v1       1/1     1            1           145m
recommendation-v1   1/1     1            1           145m
recommendation-v2   1/1     1            1           57m
siege               1/1     1            1           6m31s
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get deploy^C
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl set image deploy/order order='jinyoung/monolith:v20121121' -n tutorial
deployment.apps/order image updated
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl apply -f - <<EOF
>   apiVersion: networking.istio.io/v1alpha3
>   kind: VirtualService
>   metadata:
>     name: vs-order-network-rule
>     namespace: tutorial
>   spec:
>     hosts:
>     - order
>     http:
>     - route:
>       - destination:
>           host: order
>       timeout: 3s
>       retries:
>         attempts: 3
>         perTryTimeout: 2s
>         retryOn: 5xx,retriable-4xx,gateway-error,connect-failure,refused-stream
> EOF
virtualservice.networking.istio.io/vs-order-network-rule configured
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl exec -it [siege-Pod-Instance] -c siege-nginx -n tutorial -- /bin/bash
Error from server (NotFound): pods "[siege-Pod-Instance]" not found
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get pod -n tutorial
NAME                                 READY   STATUS    RESTARTS   AGE
customer-64d5466459-p2rqt            2/2     Running   0          155m
order-6496759475-7pbkj               2/2     Running   0          38s
preference-v1-84cc947978-6dx56       2/2     Running   0          146m
recommendation-v1-599b6cf575-49svv   2/2     Running   0          146m
recommendation-v2-6c956bc676-6xvg5   2/2     Running   0          58m
siege-88f7fdd8d-x62fn                2/2     Running   0          7m27s
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl exec -it order-6496759475-7pbkj -c siege-nginx -n tutorial -- /bin/bash
Error from server (BadRequest): container siege-nginx is not valid for pod order-6496759475-7pbkj
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl exec -it order-6496759475-7pbkj -c siege-nginx -n tutorial -- /bin/bash
Error from server (BadRequest): container siege-nginx is not valid for pod order-6496759475-7pbkj
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl exec -it order-6496759475-7pbkj ^C siege-nginx -n tutorial -- /bin/bash
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get pod -n tutorial
NAME                                 READY   STATUS    RESTARTS   AGE
customer-64d5466459-p2rqt            2/2     Running   0          156m
order-6496759475-7pbkj               2/2     Running   0          115s
preference-v1-84cc947978-6dx56       2/2     Running   0          147m
recommendation-v1-599b6cf575-49svv   2/2     Running   0          147m
recommendation-v2-6c956bc676-6xvg5   2/2     Running   0          59m
siege-88f7fdd8d-x62fn                2/2     Running   0          8m44s
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl exec -it order-6496759475-7pbkj -c siege-88f7fdd8d-x62fn -n tutorial -- /bin/bash
Error from server (BadRequest): container siege-88f7fdd8d-x62fn is not valid for pod order-6496759475-7pbkj
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl exec -it siege-88f7fdd8d-x62fn -n tutorial -- /bin/bash
Defaulting container name to siege-nginx.
Use 'kubectl describe pod/siege-88f7fdd8d-x62fn -n tutorial' to see all of the containers in this pod.
root@siege-88f7fdd8d-x62fn:/# http http://order:8080/orders/ qty=5
HTTP/1.1 500 Internal Server Error
content-type: application/json;charset=UTF-8
date: Thu, 23 Jun 2022 05:06:38 GMT
server: envoy
transfer-encoding: chunked
x-envoy-upstream-service-time: 1820

{
    "error": "Internal Server Error",
    "message": "No message available",
    "path": "/orders/",
    "status": 500,
    "timestamp": "2022-06-23T05:06:38.691+0000"
}

root@siege-88f7fdd8d-x62fn:/# http http://order:8080/orders/ qty=5
HTTP/1.1 500 Internal Server Error
content-type: application/json;charset=UTF-8
date: Thu, 23 Jun 2022 05:07:31 GMT
server: envoy
transfer-encoding: chunked
x-envoy-upstream-service-time: 272

{
    "error": "Internal Server Error",
    "message": "No message available",
    "path": "/orders/",
    "status": 500,
    "timestamp": "2022-06-23T05:07:32.081+0000"
}

root@siege-88f7fdd8d-x62fn:/# http http://order:8080/orders/ qty=5
HTTP/1.1 500 Internal Server Error
content-type: application/json;charset=UTF-8
date: Thu, 23 Jun 2022 05:07:32 GMT
server: envoy
transfer-encoding: chunked
x-envoy-upstream-service-time: 279

{
    "error": "Internal Server Error",
    "message": "No message available",
    "path": "/orders/",
    "status": 500,
    "timestamp": "2022-06-23T05:07:33.100+0000"
}

root@siege-88f7fdd8d-x62fn:/# quit
bash: quit: command not found
root@siege-88f7fdd8d-x62fn:/# exit
exit
command terminated with exit code 127
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl -n default exec -it siege -- bash
root@siege:/# http http://order.tutorial:8080/orders
HTTP/1.1 200 OK
content-type: application/hal+json;charset=UTF-8
date: Thu, 23 Jun 2022 05:12:07 GMT
server: istio-envoy
transfer-encoding: chunked
x-envoy-decorator-operation: order.tutorial.svc.cluster.local:8080/*
x-envoy-upstream-service-time: 1137

{
    "_embedded": {
        "orders": []
    },
    "_links": {
        "profile": {
            "href": "http://order.tutorial:8080/profile/orders"
        },
        "search": {
            "href": "http://order.tutorial:8080/orders/search"
        },
        "self": {
            "href": "http://order.tutorial:8080/orders{?page,size,sort}",
            "templated": true
        }
    },
    "page": {
        "number": 0,
        "size": 20,
        "totalElements": 0,
        "totalPages": 0
    }
}

root@siege:/# http http://order.tutorial:8080/orders qty=5
HTTP/1.1 500 Internal Server Error
content-type: application/json;charset=UTF-8
date: Thu, 23 Jun 2022 05:12:16 GMT
server: istio-envoy
transfer-encoding: chunked
x-envoy-decorator-operation: order.tutorial.svc.cluster.local:8080/*
x-envoy-upstream-service-time: 16

{
    "error": "Internal Server Error",
    "message": "No message available",
    "path": "/orders",
    "status": 500,
    "timestamp": "2022-06-23T05:12:16.897+0000"
}

root@siege:/# exit
exit
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl config get-contex
error: unknown command "get-contex"
See 'kubectl config -h' for help and examples
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl config get-contex -n tutorial
error: unknown command "get-contex"
See 'kubectl config -h' for help and examples
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl config get-context -n tutorial
error: unknown command "get-context"
See 'kubectl config -h' for help and examples
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl config get-context
error: unknown command "get-context"
See 'kubectl config -h' for help and examples
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl config get-context
error: unknown command "get-context"
See 'kubectl config -h' for help and examples
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl config get-context
error: unknown command "get-context"
See 'kubectl config -h' for help and examples
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl config -h
Modify kubeconfig files using subcommands like "kubectl config set current-context my-context"

 The loading order follows these rules:

  1.  If the --kubeconfig flag is set, then only that file is loaded. The flag may only be set once and no merging takes
place.
  2.  If $KUBECONFIG environment variable is set, then it is used as a list of paths (normal path delimiting rules for
your system). These paths are merged. When a value is modified, it is modified in the file that defines the stanza. When
a value is created, it is created in the first file that exists. If no files in the chain exist, then it creates the
last file in the list.
  3.  Otherwise, ${HOME}/.kube/config is used and no merging takes place.

Available Commands:
  current-context Displays the current-context
  delete-cluster  Delete the specified cluster from the kubeconfig
  delete-context  Delete the specified context from the kubeconfig
  get-clusters    Display clusters defined in the kubeconfig
  get-contexts    Describe one or many contexts
  rename-context  Renames a context from the kubeconfig file.
  set             Sets an individual value in a kubeconfig file
  set-cluster     Sets a cluster entry in kubeconfig
  set-context     Sets a context entry in kubeconfig
  set-credentials Sets a user entry in kubeconfig
  unset           Unsets an individual value in a kubeconfig file
  use-context     Sets the current-context in a kubeconfig file
  view            Display merged kubeconfig settings or a specified kubeconfig file

Usage:
  kubectl config SUBCOMMAND [options]

Use "kubectl <command> --help" for more information about a given command.
Use "kubectl options" for a list of global command-line options (applies to all commands).
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl config get-contexts
CURRENT   NAME                                                         CLUSTER                                                      AUTHINFO                                                     NAMESPACE
*         arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks   arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks   arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks   hana
          kcb-test2.k8s.local                                          kcb-test2.k8s.local                                          labs--311032102                                              labs--311032102
          user02@user02-eks.ap-northeast-1.eksctl.io                   user02-eks.ap-northeast-1.eksctl.io                          user02@user02-eks.ap-northeast-1.eksctl.io                   
root@labs--311032102:/home/project/istio-resiliency-part1# ls -lrt
total 16
drwxr-xr-x 6 root root 4096 Jun 23 04:29 delivery
drwxr-xr-x 5 root root 4096 Jun 23 04:29 gateway
drwxr-xr-x 6 root root 4096 Jun 23 04:29 order
-rw-r--r-- 1 root root  843 Jun 23 04:48 tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# vi *.y
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get svc -n istio-system
NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP                                                                   PORT(S)                                                                      AGE
grafana                ClusterIP      10.100.12.143    <none>                                                                        3000/TCP                                                                     3h34m
istio-egressgateway    ClusterIP      10.100.59.211    <none>                                                                        80/TCP,443/TCP                                                               4h9m
istio-ingressgateway   LoadBalancer   10.100.98.191    ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com   15021:32581/TCP,80:30326/TCP,443:32036/TCP,31400:31166/TCP,15443:32582/TCP   4h9m
istiod                 ClusterIP      10.100.93.163    <none>                                                                        15010/TCP,15012/TCP,443/TCP,15014/TCP                                        4h10m
jaeger-collector       ClusterIP      10.100.88.187    <none>                                                                        14268/TCP,14250/TCP,9411/TCP                                                 3h34m
kiali                  ClusterIP      10.100.22.39     <none>                                                                        20001/TCP,9090/TCP                                                           3h34m
prometheus             ClusterIP      10.100.85.109    <none>                                                                        9090/TCP                                                                     3h34m
tracing                ClusterIP      10.100.98.27     <none>                                                                        80/TCP,16685/TCP                                                             3h34m
zipkin                 ClusterIP      10.100.105.134   <none>                                                                        9411/TCP                                                                     3h34m
root@labs--311032102:/home/project/istio-resiliency-part1# ping ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com
PING ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com (54.248.142.8) 56(84) bytes of data.
^C
--- ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com ping statistics ---
2 packets transmitted, 0 received, 100% packet loss, time 1053ms

root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get svc -n istio-system
NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP                                                                   PORT(S)                                                                      AGE
grafana                ClusterIP      10.100.12.143    <none>                                                                        3000/TCP                                                                     3h38m
istio-egressgateway    ClusterIP      10.100.59.211    <none>                                                                        80/TCP,443/TCP                                                               4h14m
istio-ingressgateway   LoadBalancer   10.100.98.191    ab3b9f62ddc764d6db0034c5fb0f58ac-501828164.ap-northeast-1.elb.amazonaws.com   15021:32581/TCP,80:30326/TCP,443:32036/TCP,31400:31166/TCP,15443:32582/TCP   4h14m
istiod                 ClusterIP      10.100.93.163    <none>                                                                        15010/TCP,15012/TCP,443/TCP,15014/TCP                                        4h14m
jaeger-collector       ClusterIP      10.100.88.187    <none>                                                                        14268/TCP,14250/TCP,9411/TCP                                                 3h38m
kiali                  ClusterIP      10.100.22.39     <none>                                                                        20001/TCP,9090/TCP                                                           3h38m
prometheus             ClusterIP      10.100.85.109    <none>                                                                        9090/TCP                                                                     3h38m
tracing                ClusterIP      10.100.98.27     <none>                                                                        80/TCP,16685/TCP                                                             3h38m
zipkin                 ClusterIP      10.100.105.134   <none>                                                                        9411/TCP                                                                     3h38m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ing -n istio-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
No resources found in istio-system namespace.
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ing
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
No resources found in hana namespace.
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ing -n tutorial
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
No resources found in tutorial namespace.
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ing
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
No resources found in hana namespace.
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ing -n istio-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
No resources found in istio-system namespace.
root@labs--311032102:/home/project/istio-resiliency-part1# ls
delivery  gateway  order  tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl apply -f tracing.yaml 
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
ingress.extensions/istio-ingress created
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get pod
NAME                              READY   STATUS    RESTARTS   AGE
details-v1-79f774bdb9-m5tz7       2/2     Running   0          3h59m
productpage-v1-6b746f74dc-8f5lx   2/2     Running   0          3h59m
ratings-v1-b6994bb9-xscmb         2/2     Running   0          3h59m
reviews-v1-545db77b95-bjxrx       2/2     Running   0          3h59m
reviews-v2-7bf8c9648f-gbjsf       2/2     Running   0          3h59m
reviews-v3-84779c7bbc-rf5vv       2/2     Running   0          3h59m
root@labs--311032102:/home/project/istio-resiliency-part1# ^C
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ing -n istio-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME            CLASS    HOSTS                                                                ADDRESS   PORTS   AGE
istio-ingress   <none>   kiali.service.com,tracing.service.com,prom.service.com + 1 more...             80      24s
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ing -n istio-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME            CLASS    HOSTS                                                                ADDRESS   PORTS   AGE
istio-ingress   <none>   kiali.service.com,tracing.service.com,prom.service.com + 1 more...             80      28s
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ing -n istio-system -w
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME            CLASS    HOSTS                                                                ADDRESS   PORTS   AGE
istio-ingress   <none>   kiali.service.com,tracing.service.com,prom.service.com + 1 more...             80      30s
^Croot@labs--311032102:/home/project/istio-resiliency-part1# kubectl apply -f tracing.yaml 
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
ingress.extensions/istio-ingress configured
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ing -n istio-system -w
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME            CLASS    HOSTS                                                                ADDRESS   PORTS   AGE
istio-ingress   <none>   kiali.service.com,tracing.service.com,prom.service.com + 1 more...             80      60s
istio-ingress   <none>   kiali.service.com,tracing.service.com,prom.service.com + 1 more...   a577abeeec4b04f138fa14a11b9e4d54-1026617604.ap-northeast-1.elb.amazonaws.com   80      68s
^Croot@labs--311032102:/home/project/istio-resiliency-part1# kubectl get^C
root@labs--311032102:/home/project/istio-resiliency-part1# ping a577abeeec4b04f138fa14a11b9e4d54-1026617604.ap-northeast-1.elb.amazonaws.com
PING a577abeeec4b04f138fa14a11b9e4d54-1026617604.ap-northeast-1.elb.amazonaws.com (54.199.76.144) 56(84) bytes of data.
^C
--- a577abeeec4b04f138fa14a11b9e4d54-1026617604.ap-northeast-1.elb.amazonaws.com ping statistics ---
4 packets transmitted, 0 received, 100% packet loss, time 3071ms

root@labs--311032102:/home/project/istio-resiliency-part1# ls
delivery  gateway  order  tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# ls -lrt
total 16
drwxr-xr-x 6 root root 4096 Jun 23 04:29 delivery
drwxr-xr-x 5 root root 4096 Jun 23 04:29 gateway
drwxr-xr-x 6 root root 4096 Jun 23 04:29 order
-rw-r--r-- 1 root root  843 Jun 23 05:24 tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# vi ^C
root@labs--311032102:/home/project/istio-resiliency-part1# ^C
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl apply -f - <<EOF
>   apiVersion: apps/v1
>   kind: Deployment
>   metadata:
>     name: order
>     namespace: tutorial
>     labels:
>       app: order
>   spec:
>     replicas: 1
>     selector:
>       matchLabels:
>         app: order
>     template:
>       metadata:
>         labels:
>           app: order
>       spec:
>         containers:
>           - name: order
>             image: jinyoung/order:timeout
>             ports:
>               - containerPort: 8080
>             resources:
>               limits:
>                 cpu: 500m
>               requests:
>                 cpu: 200m
> EOF
deployment.apps/order configured
root@labs--311032102:/home/project/istio-resiliency-part1# ^C
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl apply -f test.yaml 
deployment.apps/order-test created
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get po -n tutorial
NAME                                 READY   STATUS    RESTARTS   AGE
customer-64d5466459-p2rqt            2/2     Running   0          3h1m
order-6659564bbb-2kzhr               2/2     Running   0          2m35s
order-test-6c6d6b7b79-mzrq8          2/3     Running   0          9s
preference-v1-84cc947978-6dx56       2/2     Running   0          172m
recommendation-v1-599b6cf575-49svv   2/2     Running   0          171m
recommendation-v2-6c956bc676-6xvg5   2/2     Running   0          83m
siege-88f7fdd8d-x62fn                2/2     Running   0          33m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl d^C
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get rs
NAME                        DESIRED   CURRENT   READY   AGE
details-v1-79f774bdb9       1         1         1       4h8m
productpage-v1-6b746f74dc   1         1         1       4h8m
ratings-v1-b6994bb9         1         1         1       4h8m
reviews-v1-545db77b95       1         1         1       4h8m
reviews-v2-7bf8c9648f       1         1         1       4h8m
reviews-v3-84779c7bbc       1         1         1       4h8m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get deployment
NAME             READY   UP-TO-DATE   AVAILABLE   AGE
details-v1       1/1     1            1           4h8m
productpage-v1   1/1     1            1           4h8m
ratings-v1       1/1     1            1           4h8m
reviews-v1       1/1     1            1           4h8m
reviews-v2       1/1     1            1           4h8m
reviews-v3       1/1     1            1           4h8m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get rs
NAME                        DESIRED   CURRENT   READY   AGE
details-v1-79f774bdb9       1         1         1       4h8m
productpage-v1-6b746f74dc   1         1         1       4h8m
ratings-v1-b6994bb9         1         1         1       4h8m
reviews-v1-545db77b95       1         1         1       4h8m
reviews-v2-7bf8c9648f       1         1         1       4h8m
reviews-v3-84779c7bbc       1         1         1       4h8m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get pod
NAME                              READY   STATUS    RESTARTS   AGE
details-v1-79f774bdb9-m5tz7       2/2     Running   0          4h8m
productpage-v1-6b746f74dc-8f5lx   2/2     Running   0          4h8m
ratings-v1-b6994bb9-xscmb         2/2     Running   0          4h8m
reviews-v1-545db77b95-bjxrx       2/2     Running   0          4h8m
reviews-v2-7bf8c9648f-gbjsf       2/2     Running   0          4h8m
reviews-v3-84779c7bbc-rf5vv       2/2     Running   0          4h8m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get replicaset
NAME                        DESIRED   CURRENT   READY   AGE
details-v1-79f774bdb9       1         1         1       4h9m
productpage-v1-6b746f74dc   1         1         1       4h9m
ratings-v1-b6994bb9         1         1         1       4h9m
reviews-v1-545db77b95       1         1         1       4h9m
reviews-v2-7bf8c9648f       1         1         1       4h9m
reviews-v3-84779c7bbc       1         1         1       4h9m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl edit rs details-v1-79f774bdb9
Edit cancelled, no changes made.
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get rs -n tutorial
NAME                           DESIRED   CURRENT   READY   AGE
customer-64d5466459            1         1         1       3h8m
order-6496759475               0         0         0       33m
order-6659564bbb               1         1         1       45m
order-test-6c6d6b7b79          1         1         1       7m41s
preference-v1-84cc947978       1         1         1       179m
recommendation-v1-599b6cf575   1         1         1       179m
recommendation-v2-6c956bc676   1         1         1       91m
siege-88f7fdd8d                1         1         1       40m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl edit rs order-6659564bbb -n tutorial
Edit cancelled, no changes made.
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl expose deployment order -n tutorial
Error from server (AlreadyExists): services "order" already exists
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl edit svc order
Error from server (NotFound): services "order" not found
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl edit svc order -n tutorial
Edit cancelled, no changes made.
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl edit svc order -n tutorial^C
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get po --show-labels
NAME                              READY   STATUS    RESTARTS   AGE     LABELS
details-v1-79f774bdb9-m5tz7       2/2     Running   0          4h16m   app=details,pod-template-hash=79f774bdb9,security.istio.io/tlsMode=istio,service.istio.io/canonical-name=details,service.istio.io/canonical-revision=v1,version=v1
productpage-v1-6b746f74dc-8f5lx   2/2     Running   0          4h15m   app=productpage,pod-template-hash=6b746f74dc,security.istio.io/tlsMode=istio,service.istio.io/canonical-name=productpage,service.istio.io/canonical-revision=v1,version=v1
ratings-v1-b6994bb9-xscmb         2/2     Running   0          4h16m   app=ratings,pod-template-hash=b6994bb9,security.istio.io/tlsMode=istio,service.istio.io/canonical-name=ratings,service.istio.io/canonical-revision=v1,version=v1
reviews-v1-545db77b95-bjxrx       2/2     Running   0          4h15m   app=reviews,pod-template-hash=545db77b95,security.istio.io/tlsMode=istio,service.istio.io/canonical-name=reviews,service.istio.io/canonical-revision=v1,version=v1
reviews-v2-7bf8c9648f-gbjsf       2/2     Running   0          4h15m   app=reviews,pod-template-hash=7bf8c9648f,security.istio.io/tlsMode=istio,service.istio.io/canonical-name=reviews,service.istio.io/canonical-revision=v2,version=v2
reviews-v3-84779c7bbc-rf5vv       2/2     Running   0          4h15m   app=reviews,pod-template-hash=84779c7bbc,security.istio.io/tlsMode=istio,service.istio.io/canonical-name=reviews,service.istio.io/canonical-revision=v3,version=v3
root@labs--311032102:/home/project/istio-resiliency-part1# 2R10;rgb:cccc/cccc/cccc11;rgb:1e1e/1e1e/1e1e2R10;rgb:cccc/cccc/cccc11;rgb:1e1e/1e1e/1e1e2R10;rgb:cccc/cccc/cccc11;rgb:1e1e/1e1e/1e1e2R10;rgb:cccc/cccc/cccc11;rgb:1e1e/1e1e/1e1e2R10;rgb:cccc/cccc/cccc11;rgb:1e1e/1e1e/1e1e2R10;rgb:cccc/cccc/cccc11;rgb:1e1e/1e1e/1e1e2R10;rgb:cccc/cccc/cccc11;rgb:1e1e/1e1e/1e1e2R10;rgb:cccc/cccc/cccc11;rgb:1e1e/1e1e/1e1e2R10;rgb:cccc/cccc/cccc11;rgb:1e1e/1e1e/1e1e2R10;rgb:cccc/cccc/cccc11;rgb:1e1e/1e1e/1e1e2R10;rgb:cccc/cccc/cccc11;rgb:1e1e/1e1e/1e1e^C
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ingress
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
No resources found in hana namespace.
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ingress -n tutorial
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
No resources found in tutorial namespace.
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ingress -n istio-system
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME            CLASS    HOSTS                                                                ADDRESS                                                                        PORTS   AGE
istio-ingress   <none>   kiali.service.com,tracing.service.com,prom.service.com + 1 more...   a577abeeec4b04f138fa14a11b9e4d54-1026617604.ap-northeast-1.elb.amazonaws.com   80      22m
root@labs--311032102:/home/project/istio-resiliency-part1# ls
delivery  gateway  order  test.yaml  tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# cat tracing.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: istio-ingress
  namespace: istio-system
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: kiali.service.com
    http:
      paths:
      - backend:
          serviceName: kiali
          servicePort: 20001
        path: /
        pathType: Prefix
  - host: tracing.service.com
    http:
      paths:
      - backend:
          serviceName: tracing
          servicePort: 80
        path: /
        pathType: Prefix
  - host: prom.service.com
    http:
      paths:
      - backend:
          serviceName: prometheus
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl create deploy delivery --image=ghcr.io/acmexii/delivery:istio-v1 -n tutorial
deployment.apps/delivery created
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl expose deploy delivery --port=8080 -n tutorial
service/delivery exposed
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get deploy
NAME             READY   UP-TO-DATE   AVAILABLE   AGE
details-v1       1/1     1            1           4h23m
productpage-v1   1/1     1            1           4h23m
ratings-v1       1/1     1            1           4h23m
reviews-v1       1/1     1            1           4h23m
reviews-v2       1/1     1            1           4h23m
reviews-v3       1/1     1            1           4h23m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get deploy -n tutorial
NAME                READY   UP-TO-DATE   AVAILABLE   AGE
customer            1/1     1            1           3h20m
delivery            1/1     1            1           41s
order               1/1     1            1           56m
order-test          0/1     1            0           19m
preference-v1       1/1     1            1           3h11m
recommendation-v1   1/1     1            1           3h10m
recommendation-v2   1/1     1            1           102m
siege               1/1     1            1           52m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl apply -f - << EOF
>   apiVersion: networking.istio.io/v1alpha3
>   kind: DestinationRule
>   metadata:
>     name: dr-delivery
>     namespace: tutorial
>   spec:
>     host: delivery
>     trafficPolicy:
>       loadBalancer:
>         simple: ROUND_ROBIN
>       outlierDetection:
>         consecutive5xxErrors: 1
>         interval: 1s
>         baseEjectionTime: 3m
>         maxEjectionPercent: 100
> EOF
destinationrule.networking.istio.io/dr-delivery created
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl apply -f - << EOF
  apiVersion: networking.istio.io/v1alpha3
  kind: DestinationRule
  metadata:
    name: dr-delivery
    namespace: tutorial
  spec:
    host: delivery
    trafficPolicy:
      loadBalancer:
        simple: ROUND_ROBIN
      outlierDetection:
        consecutive5xxErrors: 1
        interval: 1s
        baseEjectionTime: 3m
        maxEjectionPercent: 100
EOF
^C
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get rs
NAME                        DESIRED   CURRENT   READY   AGE
details-v1-79f774bdb9       1         1         1       4h25m
productpage-v1-6b746f74dc   1         1         1       4h25m
ratings-v1-b6994bb9         1         1         1       4h25m
reviews-v1-545db77b95       1         1         1       4h25m
reviews-v2-7bf8c9648f       1         1         1       4h25m
reviews-v3-84779c7bbc       1         1         1       4h25m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl scale deploy delivery --replicas=3 -n tutorial
deployment.apps/delivery scaled
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get rs
NAME                        DESIRED   CURRENT   READY   AGE
details-v1-79f774bdb9       1         1         1       4h25m
productpage-v1-6b746f74dc   1         1         1       4h25m
ratings-v1-b6994bb9         1         1         1       4h25m
reviews-v1-545db77b95       1         1         1       4h25m
reviews-v2-7bf8c9648f       1         1         1       4h25m
reviews-v3-84779c7bbc       1         1         1       4h25m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get rs
NAME                        DESIRED   CURRENT   READY   AGE
details-v1-79f774bdb9       1         1         1       4h25m
productpage-v1-6b746f74dc   1         1         1       4h25m
ratings-v1-b6994bb9         1         1         1       4h25m
reviews-v1-545db77b95       1         1         1       4h25m
reviews-v2-7bf8c9648f       1         1         1       4h25m
reviews-v3-84779c7bbc       1         1         1       4h25m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get deloy
^C
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get deploy
NAME             READY   UP-TO-DATE   AVAILABLE   AGE
details-v1       1/1     1            1           4h26m
productpage-v1   1/1     1            1           4h26m
ratings-v1       1/1     1            1           4h26m
reviews-v1       1/1     1            1           4h26m
reviews-v2       1/1     1            1           4h26m
reviews-v3       1/1     1            1           4h26m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get pod
NAME                              READY   STATUS    RESTARTS   AGE
details-v1-79f774bdb9-m5tz7       2/2     Running   0          4h28m
productpage-v1-6b746f74dc-8f5lx   2/2     Running   0          4h28m
ratings-v1-b6994bb9-xscmb         2/2     Running   0          4h28m
reviews-v1-545db77b95-bjxrx       2/2     Running   0          4h28m
reviews-v2-7bf8c9648f-gbjsf       2/2     Running   0          4h28m
reviews-v3-84779c7bbc-rf5vv       2/2     Running   0          4h28m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get pod -n tutorial
NAME                                 READY   STATUS             RESTARTS   AGE
customer-64d5466459-p2rqt            2/2     Running            0          3h24m
delivery-67ff6476bb-p6mp9            2/2     Running            0          2m44s
delivery-67ff6476bb-qm8cv            2/2     Running            0          4m57s
delivery-67ff6476bb-tgw5d            2/2     Running            0          2m44s
order-6659564bbb-2kzhr               2/2     Running            0          25m
order-test-6c6d6b7b79-mzrq8          2/3     CrashLoopBackOff   7          23m
preference-v1-84cc947978-6dx56       2/2     Running            0          3h15m
recommendation-v1-599b6cf575-49svv   2/2     Running            0          3h15m
recommendation-v2-6c956bc676-6xvg5   2/2     Running            0          107m
siege-88f7fdd8d-x62fn                2/2     Running            0          56m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get pod -n rs
No resources found in rs namespace.
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get rs -n tutorial
NAME                           DESIRED   CURRENT   READY   AGE
customer-64d5466459            1         1         1       3h24m
delivery-67ff6476bb            3         3         3       5m13s
order-6496759475               0         0         0       49m
order-6659564bbb               1         1         1       61m
order-test-6c6d6b7b79          1         1         0       23m
preference-v1-84cc947978       1         1         1       3h15m
recommendation-v1-599b6cf575   1         1         1       3h15m
recommendation-v2-6c956bc676   1         1         1       107m
siege-88f7fdd8d                1         1         1       56m
root@labs--311032102:/home/project/istio-resiliency-part1# 


root@labs--311032102:/home/project/istio-resiliency-part1# kubectl exec -it siege-88f7fdd8d-x62fn  -c siege-nginx -n tutorial -- /bin/bash
root@siege-88f7fdd8d-x62fn:/# http http://delivery:8080/actuator/echo
HTTP/1.1 200 OK
content-length: 40
content-type: text/plain;charset=UTF-8
date: Thu, 23 Jun 2022 05:51:02 GMT
server: envoy
x-envoy-upstream-service-time: 347

delivery-67ff6476bb-tgw5d/192.168.41.144

root@siege-88f7fdd8d-x62fn:/# http PUT http://delivery:8080/actuator/down
HTTP/1.1 200 OK
content-type: application/json;charset=UTF-8
date: Thu, 23 Jun 2022 05:51:13 GMT
server: envoy
transfer-encoding: chunked
x-envoy-upstream-service-time: 534

{
    "status": "DOWN"
}

root@siege-88f7fdd8d-x62fn:/# http GET http://delivery:8080/actuator/health
HTTP/1.1 200 OK
content-type: application/vnd.spring-boot.actuator.v2+json;charset=UTF-8
date: Thu, 23 Jun 2022 05:51:21 GMT
server: envoy
transfer-encoding: chunked
x-envoy-upstream-service-time: 388

{
    "status": "UP"
}

root@siege-88f7fdd8d-x62fn:/# http http://delivery:8080/actuator/echo
HTTP/1.1 200 OK
content-length: 40
content-type: text/plain;charset=UTF-8
date: Thu, 23 Jun 2022 05:51:30 GMT
server: envoy
x-envoy-upstream-service-time: 61

delivery-67ff6476bb-tgw5d/192.168.41.144

root@siege-88f7fdd8d-x62fn:/# http http://delivery:8080/actuator/echo
HTTP/1.1 200 OK
content-length: 40
content-type: text/plain;charset=UTF-8
date: Thu, 23 Jun 2022 05:51:36 GMT
server: envoy
x-envoy-upstream-service-time: 74

delivery-67ff6476bb-p6mp9/192.168.75.139

root@siege-88f7fdd8d-x62fn:/# http http://delivery:8080/actuator/echo^C
root@siege-88f7fdd8d-x62fn:/# http http://delivery:8080/actuator/echo
HTTP/1.1 200 OK
content-length: 39
content-type: text/plain;charset=UTF-8
date: Thu, 23 Jun 2022 05:53:59 GMT
server: envoy
x-envoy-upstream-service-time: 51

delivery-67ff6476bb-qm8cv/192.168.6.134

root@siege-88f7fdd8d-x62fn:/# http http://delivery:8080/actuator/echo
HTTP/1.1 200 OK
content-length: 40
content-type: text/plain;charset=UTF-8
date: Thu, 23 Jun 2022 05:54:09 GMT
server: envoy
x-envoy-upstream-service-time: 15

delivery-67ff6476bb-tgw5d/192.168.41.144

root@siege-88f7fdd8d-x62fn:/# 
root@siege-88f7fdd8d-x62fn:/# http http://delivery:8080/actuator/echo
HTTP/1.1 200 OK
content-length: 39
content-type: text/plain;charset=UTF-8
date: Thu, 23 Jun 2022 05:54:15 GMT
server: envoy
x-envoy-upstream-service-time: 19

delivery-67ff6476bb-qm8cv/192.168.6.134

root@siege-88f7fdd8d-x62fn:/# http http://delivery:8080/actuator/echo
HTTP/1.1 200 OK
content-length: 40
content-type: text/plain;charset=UTF-8
date: Thu, 23 Jun 2022 05:54:20 GMT
server: envoy
x-envoy-upstream-service-time: 11

delivery-67ff6476bb-tgw5d/192.168.41.144

root@siege-88f7fdd8d-x62fn:/# http http://delivery:8080/actuator/echo
HTTP/1.1 200 OK
content-length: 40
content-type: text/plain;charset=UTF-8
date: Thu, 23 Jun 2022 05:54:22 GMT
server: envoy
x-envoy-upstream-service-time: 9

delivery-67ff6476bb-p6mp9/192.168.75.139

root@siege-88f7fdd8d-x62fn:/# http PUT http://delivery:8080/actuator/down
HTTP/1.1 200 OK
content-type: application/json;charset=UTF-8
date: Thu, 23 Jun 2022 05:54:39 GMT
server: envoy
transfer-encoding: chunked
x-envoy-upstream-service-time: 8

{
    "status": "DOWN"
}

root@siege-88f7fdd8d-x62fn:/# http http://delivery:8080/actuator/echo
HTTP/1.1 200 OK
content-length: 40
content-type: text/plain;charset=UTF-8
date: Thu, 23 Jun 2022 05:54:58 GMT
server: envoy
x-envoy-upstream-service-time: 25

delivery-67ff6476bb-p6mp9/192.168.75.139

root@siege-88f7fdd8d-x62fn:/# http http://delivery:8080/actuator/echo
HTTP/1.1 200 OK
content-length: 39
content-type: text/plain;charset=UTF-8
date: Thu, 23 Jun 2022 05:55:02 GMT
server: envoy
x-envoy-upstream-service-time: 16

delivery-67ff6476bb-qm8cv/192.168.6.134

root@siege-88f7fdd8d-x62fn:/# http http://delivery:8080/actuator/echo
HTTP/1.1 200 OK
content-length: 40
content-type: text/plain;charset=UTF-8
date: Thu, 23 Jun 2022 05:55:03 GMT
server: envoy
x-envoy-upstream-service-time: 20

delivery-67ff6476bb-tgw5d/192.168.41.144

root@siege-88f7fdd8d-x62fn:/# http http://delivery:8080/actuator/echo
HTTP/1.1 200 OK
content-length: 40
content-type: text/plain;charset=UTF-8
date: Thu, 23 Jun 2022 05:55:05 GMT
server: envoy
x-envoy-upstream-service-time: 15

delivery-67ff6476bb-p6mp9/192.168.75.139

root@siege-88f7fdd8d-x62fn:/# http http://delivery:8080/actuator/echo
HTTP/1.1 200 OK
content-length: 39
content-type: text/plain;charset=UTF-8
date: Thu, 23 Jun 2022 05:55:06 GMT
server: envoy
x-envoy-upstream-service-time: 10

delivery-67ff6476bb-qm8cv/192.168.6.134

root@siege-88f7fdd8d-x62fn:/# 



### DATABASE 설정
주문 서비스에 Database 설정의 변경
데이터베이스 설정이 이루어진 order project 를 다운로드 받는다

cd ~
git clone https://github.com/event-storming/monolith

cd monolith
Dockerfile-prod 를 Dockerfile 로 바꾼다.
기존 Dockerfile 삭제.

rm Dockerfile
mv Dockerfile-prod Dockerfile
빌드하여 order:database 라는 이미지명으로 레지스트리에 등록한다
e.g. (도커허브경우)

mvn package -B 

# docker hub 경우
docker build -t jinyoung/order:database .

# azure 의 경우
az acr build --registry user27 --image user27.azurecr.io/order:database .
왼쪽 에디터에서 생성된 프로젝트를 열기위해서는, Explorer 의 바의 우측 … 버튼을 클릭한후, Add folder to workspace 를 클릭한후, 다운받은 monolith 를 선택한다.

order 서비스의 Database 설정을 아래와 같이 변경하므로써, 외부의 데이터베이스에 접근 가능하게 된다:

application.yml (or application-prod.yml)

spring:
  jpa:
    hibernate:
      naming:
        physical-strategy: org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl
      ddl-auto: update
    properties:
      hibernate:
        show_sql: true
        format_sql: true
        dialect: org.hibernate.dialect.MySQL57Dialect
  datasource:
    url: jdbc:mysql://${_DATASOURCE_ADDRESS:35.221.110.118:3306}/${_DATASOURCE_TABLESPACE:my-database}
    username: ${_DATASOURCE_USERNAME:root1}
    password: ${_DATASOURCE_PASSWORD:secretpassword}
    driverClassName: com.mysql.cj.jdbc.Driver
변경한 정보를 환경변수에서 얻어오도록 설정하였고, Deployment 에서 위의 값이 전달되도록 주입할 수 있다:

apiVersion: "apps/v1"
kind: "Deployment"
metadata: 
  name: "order"
  labels: 
    app: "order"
spec: 
  selector: 
    matchLabels: 
      app: "order"
  replicas: 1
  template: 
    metadata: 
      labels: 
        app: "order"
    spec: 
      containers: 
        - 
          name: "order"
          image: "jinyoung/order:database"
          ports: 
            - 
              containerPort: 80
          env:
            - name: superuser.userId
              value: some_value					
            - name: _DATASOURCE_ADDRESS
              value: mysql
            - name: _DATASOURCE_TABLESPACE
              value: orderdb
            - name: _DATASOURCE_USERNAME
              value: root
            - name: _DATASOURCE_PASSWORD
              value: admin

설정후에 kubectl logs 로 로그를 확인하면 다음과 같은 오류를 발견할 수 있다:

kubectl get po # pod 명 확인
kubectl logs <pod 명>
로그 내용중:

Caused by: java.net.UnknownHostException: mysql: Name does not resolve
우리가 제공한 DB server 의 주소를 환경변수로 잘 받아왔고 (mysql), 그 주소로 접근을 시도했으나 서비스가 올라있지 않으므로 발생하는 오류이다.

값을 위와 같이 Deployment 설정에 직접 입력하는것은 개발자와 운영자사이에 역할이 혼재되므로, 운영자가 해당 설정 부분만을 관리할 수 있도록 별도의 Configuration 을 위한 쿠버네티스 객체인 ConfigMap (혹은 Secret)을 선언하여 연결할 수 있다. 여기서는 패스워드가 노출되면 안되므로 PASSWORD 에 대해서만 Secret 을 이용하여 분리해준다:

apiVersion: v1
kind: Secret
metadata:
  name: mysql-pass
type: Opaque
data:
  password: YWRtaW4=     
"YWRtaW4="는 ‘admin’ 문자열의 BASE64 인코딩된 문자열이다. “echo -n ‘admin’ | base64” 명령을 통해 생성가능하다.

Secret 객체의 내용을 기존 deployment.yaml 에 추가하고:

$ kubectl apply -f deployment.yaml

secret/mysql-pass created
생성된 secret 을 확인한다:

$ kubectl get secrets

NAME                  TYPE                                  DATA   AGE
default-token-l7t7b   kubernetes.io/service-account-token   3      4h24m
mysql-pass            Opaque                                1      1m
해당 Secret 을 Order Deployment 에 설정:

          env:
            - name: superuser.userId
              value: userId
            - name: _DATASOURCE_ADDRESS
              value: mysql
            - name: _DATASOURCE_TABLESPACE
              value: orderdb
            - name: _DATASOURCE_USERNAME
              value: root
            - name: _DATASOURCE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mysql-pass
                  key: password

Database 서비스의 생성
주문 서비스를 위한 데이터베이스로 MySQL 을 사용하기로 한다. MySQL 이미지명으로 간단하게 Pod 하나를 생성한다.

MySQL 을 위한 Pod 설치:

apiVersion: v1
kind: Pod
metadata:
  name: mysql
  labels:
    name: lbl-k8s-mysql
spec:
  containers:
  - name: mysql
    image: mysql:latest
    env:
    - name: MYSQL_ROOT_PASSWORD
      valueFrom:
        secretKeyRef:
          name: mysql-pass
          key: password
    ports:
    - name: mysql
      containerPort: 3306
      protocol: TCP
    volumeMounts:
    - name: k8s-mysql-storage
      mountPath: /var/lib/mysql
  volumes:
  - name: k8s-mysql-storage
    emptyDir: {}
생성된 yaml 을 deployment.yaml 에 추가한후:

$ kubectl apply -f deployment.yaml

pod/k8s-mysql created
Pod 실행을 확인한다:

$ kubectl get pod

NAME        READY   STATUS    RESTARTS   AGE
k8s-mysql   1/1     Running   0          30s
Now, we can connect to the k8s-mysql pod:
Pod 에 접속하여 orderdb 데이터베이스 공간을 만들어주고 데이터베이스가 잘 동작하는지 확인한다:

$ kubectl exec mysql -it -- bash

# echo $MYSQL_ROOT_PASSWORD
admin

# mysql --user=root --password=$MYSQL_ROOT_PASSWORD

mysql> create database orderdb;
    -> ;
Query OK, 1 row affected (0.01 sec)

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| orderdb            |
| performance_schema |
| sys                |
+--------------------+
5 rows in set (0.01 sec)

mysql> exit
주문 마이크로 서비스를 쿠버네티스 DNS 체계내에서 접근가능하게 하기 위해 ClusterIP 로 서비스를 생성해준다. 주문 서비스에서 mysql 접근을 위하여 "mysql"이라는 도메인명으로 접근하고 있으므로, 같은 이름으로 서비스를 만들어준다:

apiVersion: v1
kind: Service
metadata:
  labels:
    name: lbl-k8s-mysql
  name: mysql
  namespace: default
spec:
  ports:
  - port: 3306
    protocol: TCP
    targetPort: 3306
  selector:
    name: lbl-k8s-mysql
  type: ClusterIP
마찬가지 위의 내용을 deployment.yaml 에 추가하고 kubectl apply -f deployment.yaml
앞으로 계속 마찬가지!

주문 마이크로 서비스만을 새로 재기동 시키기 위해서는 아래와 같이 po 를 삭제해주면 deployment 에 의해서 알아서 재시작된다:

kubectl get po -l app=order
kubectl delete po [order-po-name]
이렇게 잘 접속이 된다면 다음의 로그를 확인할 수 있다:

Hibernate: 
    insert 
    into
        ProductOption
        (description, name, optionName, PRODUCT_ID) 
    values
        (?, ?, ?, ?)
Hibernate: 
    insert 
    into
        Product
        (imageUrl, name, price, stock) 
    values
        (?, ?, ?, ?)
Hibernate: 
    insert 
    into
        ProductOption
        (description, name, optionName, PRODUCT_ID) 
    values
        (?, ?, ?, ?)
Hibernate: 
    insert 
    into
        ProductOption
        (description, name, optionName, PRODUCT_ID) 
    values
        (?, ?, ?, ?)
주문 걸어보기
httpie pod 에 들어가서 주문을 걸어준다:

kubectl exec -it httpie /bin/bash 

root@httpie:/# http order:8080/orders productId=1 customerId="jjy"
위의 ‘order’ 도메인 주소로 접근이 되도록 하려면 order 를 위한 Service (이름 order) 객체가 꼭 만들어져 있어야 한다.

apiVersion: "v1"
kind: "Service"
metadata: 
  name: "order"
  labels: 
    app: "order"
spec: 
  ports: 
    - 
      port: 8080
      targetPort: 8080
  selector: 
    app: "order"
  type: "ClusterIP"

order 라는 도메인 주소로 호출이 되려면 order 와 같은 클러스터내부에서 호출해야 한다. 그러려면 httpie pod 를 만들어서 order 와 같은 네임스페이스 (default) 내에 httpie pod 가 생성되어있어야 한다.
httpie pod 를 만들기:

cat <<EOF | kubectl apply -f -
apiVersion: "v1"
kind: "Pod"
metadata: 
  name: httpie
  labels: 
    name: httpie
spec: 
  containers: 
    - 
      name: httpie
      image: clue/httpie
      command:
        - sleep
        - "36000"
EOF

httpie pod 를 만들기 귀찮다면… kubectl port-forward deploy/order 8085:8080 를 해놓은 후, localhost:8085 번으로 호출해도 된다.

주문 마이크로 서비스의 데이터가 설치한 MySQL을 통하여 보존되는 것을 확인한다.

mysql> use orderdb
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql> show tables;
+-------------------+
| Tables_in_orderdb |
+-------------------+
| Delivery          |
| Product           |
| ProductOption     |
| order_table       |
+-------------------+
4 rows in set (0.00 sec)

mysql> select * from order_table

+----+--------------+------------+--------------+-------+-----------+-------------+----------+-------------+-------------+
| id | customerAddr | customerId | customerName | price | productId | productName | quantity | state       | product_idx |
+----+--------------+------------+--------------+-------+-----------+-------------+----------+-------------+-------------+
|  1 | NULL         | jjy        | NULL         | 10000 |         1 | TV          |        0 | OrderPlaced |           1 |
+----+--------------+------------+--------------+-------+-----------+-------------+----------+-------------+-------------+
1 row in set (0.00 sec)

주문 마이크로 서비스를 내렸다가 올려도 주문한 내역이 그대로 존재함을 확인할 수 있어야 한다.

PersistenceVolume 을 통한 데이터베이스 데이터 보존
먼저, MySQL 서비스의 설치를 제거했다가 다시 기동시켜본다:

kubectl delete pod mysql
kubectl apply -f deployment.yaml
애플리케이션 데이터가 소실됨을 확인할 수 있다.

이는, MySQL 자체가 사용하는 볼륨이 해당 Pod 에 기본 부착된 파일시스템이기 때문이다. 이를 해결하기 위하여 PersistenceVolume 으로 된 파일시스템에 연결하도록 설정한다:

spec:
  containers:
    volumeMounts:
    - name: k8s-mysql-storage
      mountPath: /var/lib/mysql
  volumes:
  - name: k8s-mysql-storage
    persistentVolumeClaim:
      claimName: "fs"
변경후 apply 를 해주고 기다려보면 해당 Pod 는 Pending 상태에 잠기게 된다. 이유는 해당 Pod 를 위한 fs 라는 PVC가 생성되지 않았기 때문이다.

우리는 저 “fs” 라고 하는 PVC 를 플랫폼(애져 or AWS등)의 파일 서비스에서 만들어와야 한다.

PVC 생성
PersistentVolumeClaim - PVC 를 생성하는 방법은 간단하다. 기본으로 장착된 gp2라고 하는 StorageClass 를 통해서 얻을 수 있기 때문이다.
아래와 같은 yaml 을 설정한 후, kubectl get pvc 를 통해 PVC가 생성되는 것을 확인한다.

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: fs
  labels:
    app: test-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Mi
PVC가 생성되면, mysql pod 의 pending 이 해제될 것이다.

Volume mount 에 대한 설정변경은 그냥 apply 만 해서는 반영이 될 수 없다는 오류가 발견될 것이다. 이를 해결하기 위해서는 완전히 기존 mysql 설정을 삭제한 후 다시 기동해주어야 한다:

kubectl delete -f deployment.yaml
kubectl apply -f deployment.yaml
다시 기동후에는 orderdb 테이블 스페이스를 다시 만들어주어야 한다.

그런후에 PVC 상태를 확인:

# kubectl get pvc
NAME            STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
aws-ebs         Bound     pvc-225ebe47-cc67-4985-8f94-c0d4d795dede   1Gi        RWO            gp2            9m47s
이제, MySQL 이 소실된다하더라도, PersistenceVolume 에 실제 연결된 클라우드 파일 시스템에 데이터가 보존됨을 확인할 수 있다.

주문 한건을 걸어본 후,

mysql pod 를 삭제하고,

kubectl delete pod mysql
kubectl apply -f deployment.yaml
그 후에도, 해당 데이터가 존재함을 확인한다


## Argo CD 를 통한 배포

Argo CD 의 홈페이지를 방문한다:
https://argo-cd.readthedocs.io/en/stable/

Getting Started 메뉴를 접속하여, argo cd 를 설치한다:

kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
Argo CD UI 를 접속하기 위하여 LoadBalancer 로 전환한다:

kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}'

Argo CD UI 의 External IP 주소를 획득한다

kubectl get svc argocd-server -n argocd
접속한다.

Argo CD 는 기본 https 로 UI 서비스가 열리므로, 인증서가 없이 서비스를 열었으므로, 이를 그냥 접속하기 위해서 해당 페이지에서 허공에 대고 “thisisunsafe” 를 입력하면 다음과 같은 페이지로 넘어간다 ㅡㅡ;

##
Argo CD 의 홈페이지를 방문한다:
https://argo-cd.readthedocs.io/en/stable/

Getting Started 메뉴를 접속하여, argo cd 를 설치한다:

kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
Argo CD UI 를 접속하기 위하여 LoadBalancer 로 전환한다:

kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}'

Argo CD UI 의 External IP 주소를 획득한다

kubectl get svc argocd-server -n argocd
접속한다.

Argo CD 는 기본 https 로 UI 서비스가 열리므로, 인증서가 없이 서비스를 열었으므로, 이를 그냥 접속하기 위해서 해당 페이지에서 허공에 대고 “thisisunsafe” 를 입력하면 다음과 같은 페이지로 넘어간다 ㅡㅡ;



접속 user id 는 admin 이고 password 는 다음과 같이 Secret 에서 얻어내어야 한다 (무슨 CD 툴이 왠 보안에 엄청 신경을):

kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
왼쪽 메뉴에서 New App 을 클릭하여 Git 주소가 포함된 Application 정보를 등록한다:



Guestbook Application 을 등록한다:



https://github.com/argoproj/argocd-example-apps.git 를 접속한 후, 이를 Fork 한다.

내 계정으로 복제된 guest book application 의 git 주소를 argo 에 등록한다.



배포될 타겟 클러스터를 지정한다



kubernetes.default.svc 가 내가 포함된 서비스의 기본 접속 주소이다.
namespace 를 “guestbook” 으로 줘본다.

git 에 변화를 주고, 이를 동기화 시켜서 반영이 되는지 확인한다:



### 앱에 MYSQL 붙히는거.
1. 도커 3306 -> MYSQL 3306 으로 이동 
docker run --name mysql -e MYSQL_ROOT_PASSWORD=admin -p 3306:3306 -d mysql

docker stop mysql
docker rm mysql
docker ps -a
2.

			mysql
			mysql-connector-java
			runtime

3
jpa:
    hibernate:
      naming:
        physical-strategy: org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl
      ddl-auto: update
    properties:
      hibernate:
        show_sql: true
        format_sql: true
        dialect: org.hibernate.dialect.MySQL57Dialect
  # datasource:
  #   url: "jdbc:mysql://70.1.3.64:3306/dbname?serverTimezone=UTC&characterEncoding=UTF-8"
  #   username: db 유저네임
  #   password: db 비밀번호
  #   driverClassName: com.mysql.cj.jdbc.Driver

4. use orderdb;
5. show tables;



1. MYSQL 설치 (로컬)
2. 앱 수정(로컬접속후테스트)
4. 앱->도커로이동


##

server:
  port: 8080
---

spring:
  profiles: default
  jpa:
    hibernate:
      naming:
        physical-strategy: org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl
      ddl-auto: update
    properties:
      hibernate:
        show_sql: true
        format_sql: true
        dialect: org.hibernate.dialect.MySQL57Dialect
  datasource:
    url: "jdbc:mysql://localhost:3306/orderdb?serverTimezone=UTC&characterEncoding=UTF-8"
    username: root
    password: admin
    driverClassName: com.mysql.cj.jdbc.Driver
  cloud:
    stream:
      kafka:
        binder:
          brokers: localhost:9092
        streams:
          binder:
            configuration:
              default:
                key:
                  serde: org.apache.kafka.common.serialization.Serdes$StringSerde
                value:
                  serde: org.apache.kafka.common.serialization.Serdes$StringSerde
      bindings:
        event-in:
          group: order
          destination: mall
          contentType: application/json
        event-out:
          destination: mall
          contentType: application/json

logging:
  level:
    org.hibernate.type: trace
    org.springframework.cloud: debug
server:
  port: 8082
api:
  delivery:
    url: http://localhost:8083
---

spring:
  profiles: docker
  cloud:
    stream:
      kafka:
        binder:
          brokers: my-kafka.kafka.svc.cluster.local:9092
        streams:
          binder:
            configuration:
              default:
                key:
                  serde: org.apache.kafka.common.serialization.Serdes$StringSerde
                value:
                  serde: org.apache.kafka.common.serialization.Serdes$StringSerde
      bindings:
        event-in:
          group: order
          destination: mall
          contentType: application/json
        event-out:
          destination: mall
          contentType: application/json
api:
  delivery:
    url: http://delivery:8080



		<dependency>
			<groupId>mysql</groupId>
			<artifactId>mysql-connector-java</artifactId>
			<scope>runtime</scope>
		</dependency>
###

root@labs--311032102:/home/project/istio-resiliency-part1# docker exec -it mysql -- bash
OCI runtime exec failed: exec failed: container_linux.go:348: starting container process caused "exec: \"--\": executable file not found in $PATH": unknown
root@labs--311032102:/home/project/istio-resiliency-part1# docker exec -it mysql  bash
root@2b61b7ce2477:/# mysql -u root -p
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 35
Server version: 8.0.29 MySQL Community Server - GPL

Copyright (c) 2000, 2022, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> use orderdb;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql> select * from tables;
ERROR 1146 (42S02): Table 'orderdb.tables' doesn't exist
mysql> select * from table;
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'table' at line 1
mysql> show tables;
+--------------------+
| Tables_in_orderdb  |
+--------------------+
| Order_table        |
| hibernate_sequence |
+--------------------+
2 rows in set (0.00 sec)

mysql> select * from Order_table;
+----+-----------+------+--------+
| id | productId | qty  | status |
+----+-----------+------+--------+
|  1 | 1         |    1 | NULL   |
+----+-----------+------+--------+
1 row in set (0.00 sec)

mysql> 


## 로컬검증하고 (default) --> profile docker 를 세팅함.
server:
  port: 8080
---

spring:
  profiles: default
  jpa:
    hibernate:
      naming:
        physical-strategy: org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl
      ddl-auto: update
    properties:
      hibernate:
        show_sql: true
        format_sql: true
        dialect: org.hibernate.dialect.MySQL57Dialect
  datasource:
    url: "jdbc:mysql://localhost:3306/orderdb?serverTimezone=UTC&characterEncoding=UTF-8"
    username: root
    password: admin
    driverClassName: com.mysql.cj.jdbc.Driver
  cloud:
    stream:
      kafka:
        binder:
          brokers: localhost:9092
        streams:
          binder:
            configuration:
              default:
                key:
                  serde: org.apache.kafka.common.serialization.Serdes$StringSerde
                value:
                  serde: org.apache.kafka.common.serialization.Serdes$StringSerde
      bindings:
        event-in:
          group: order
          destination: mall
          contentType: application/json
        event-out:
          destination: mall
          contentType: application/json

logging:
  level:
    org.hibernate.type: trace
    org.springframework.cloud: debug
server:
  port: 8082
api:
  delivery:
    url: http://localhost:8083
---

spring:
  profiles: docker
  jpa:
    hibernate:
      naming:
        physical-strategy: org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl
      ddl-auto: update
    properties:
      hibernate:
        show_sql: true
        format_sql: true
        dialect: org.hibernate.dialect.MySQL57Dialect
  datasource:
    url: "${DB_URL}"
    username: "${DB_USER}"
    password: "${DB_PASSWORD}"
    driverClassName: com.mysql.cj.jdbc.Driver  
  cloud:
    stream:
      kafka:
        binder:
          brokers: ${KAFKA_BROKER}
        streams:
          binder:
            configuration:
              default:
                key:
                  serde: org.apache.kafka.common.serialization.Serdes$StringSerde
                value:
                  serde: org.apache.kafka.common.serialization.Serdes$StringSerde
      bindings:
        event-in:
          group: order
          destination: mall
          contentType: application/json
        event-out:
          destination: mall
          contentType: application/json
api:
  delivery:
    url: ${DELIVERY_SERVER}



===================================


root@labs--311032102:/home/project/istio-resiliency-part1# docker ps -a
CONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS                    PORTS                               NAMES
2b61b7ce2477        mysql                 "docker-entrypoint..."   19 minutes ago      Up 18 minutes             0.0.0.0:3306->3306/tcp, 33060/tcp   mysql
0d806e5b83f6        gorapadd/welcome:v1   "/docker-entrypoin..."   3 days ago          Up 3 days                 0.0.0.0:8091->80/tcp                naughty_mahavira
e2cd835b1b21        gorapadd/welcome:v1   "/docker-entrypoin..."   3 days ago          Created                                                       silly_pascal
c68ef9383f6c        gorapadd/welcome:v1   "/docker-entrypoin..."   3 days ago          Created                                                       nifty_raman
a41922a1add2        nginx                 "/docker-entrypoin..."   3 days ago          Exited (127) 3 days ago                                       agitated_dewdney
8b55e1bad221        httpd                 "httpd-foreground"       3 days ago          Up 3 days                 0.0.0.0:8080->80/tcp                my-running-app
root@labs--311032102:/home/project/istio-resiliency-part1# http :8082/orders
HTTP/1.1 200 
Content-Type: application/hal+json;charset=UTF-8
Date: Thu, 23 Jun 2022 07:41:44 GMT
Transfer-Encoding: chunked

{
    "_embedded": {
        "orders": []
    },
    "_links": {
        "profile": {
            "href": "http://localhost:8082/profile/orders"
        },
        "self": {
            "href": "http://localhost:8082/orders{?page,size,sort}",
            "templated": true
        }
    },
    "page": {
        "number": 0,
        "size": 20,
        "totalElements": 0,
        "totalPages": 0
    }
}

root@labs--311032102:/home/project/istio-resiliency-part1# http :8082/orders productId=1 qty=1
HTTP/1.1 201 
Content-Type: application/json;charset=UTF-8
Date: Thu, 23 Jun 2022 07:41:58 GMT
Location: http://localhost:8082/orders/1
Transfer-Encoding: chunked

{
    "_links": {
        "order": {
            "href": "http://localhost:8082/orders/1"
        },
        "self": {
            "href": "http://localhost:8082/orders/1"
        }
    },
    "productId": "1",
    "qty": 1,
    "status": null
}

root@labs--311032102:/home/project/istio-resiliency-part1# http :8082/orders
HTTP/1.1 200 
Content-Type: application/hal+json;charset=UTF-8
Date: Thu, 23 Jun 2022 07:42:31 GMT
Transfer-Encoding: chunked

{
    "_embedded": {
        "orders": [
            {
                "_links": {
                    "order": {
                        "href": "http://localhost:8082/orders/1"
                    },
                    "self": {
                        "href": "http://localhost:8082/orders/1"
                    }
                },
                "productId": "1",
                "qty": 1,
                "status": null
            }
        ]
    },
    "_links": {
        "profile": {
            "href": "http://localhost:8082/profile/orders"
        },
        "self": {
            "href": "http://localhost:8082/orders{?page,size,sort}",
            "templated": true
        }
    },
    "page": {
        "number": 0,
        "size": 20,
        "totalElements": 1,
        "totalPages": 1
    }
}

root@labs--311032102:/home/project/istio-resiliency-part1# http :8082/orders
HTTP/1.1 200 
Content-Type: application/hal+json;charset=UTF-8
Date: Thu, 23 Jun 2022 07:42:34 GMT
Transfer-Encoding: chunked

{
    "_embedded": {
        "orders": [
            {
                "_links": {
                    "order": {
                        "href": "http://localhost:8082/orders/1"
                    },
                    "self": {
                        "href": "http://localhost:8082/orders/1"
                    }
                },
                "productId": "1",
                "qty": 1,
                "status": null
            }
        ]
    },
    "_links": {
        "profile": {
            "href": "http://localhost:8082/profile/orders"
        },
        "self": {
            "href": "http://localhost:8082/orders{?page,size,sort}",
            "templated": true
        }
    },
    "page": {
        "number": 0,
        "size": 20,
        "totalElements": 1,
        "totalPages": 1
    }
}

root@labs--311032102:/home/project/istio-resiliency-part1# http :8082/orders
HTTP/1.1 200 
Content-Type: application/hal+json;charset=UTF-8
Date: Thu, 23 Jun 2022 07:42:35 GMT
Transfer-Encoding: chunked

{
    "_embedded": {
        "orders": [
            {
                "_links": {
                    "order": {
                        "href": "http://localhost:8082/orders/1"
                    },
                    "self": {
                        "href": "http://localhost:8082/orders/1"
                    }
                },
                "productId": "1",
                "qty": 1,
                "status": null
            }
        ]
    },
    "_links": {
        "profile": {
            "href": "http://localhost:8082/profile/orders"
        },
        "self": {
            "href": "http://localhost:8082/orders{?page,size,sort}",
            "templated": true
        }
    },
    "page": {
        "number": 0,
        "size": 20,
        "totalElements": 1,
        "totalPages": 1
    }
}

root@labs--311032102:/home/project/istio-resiliency-part1# docker exec -it mysql -- bash
OCI runtime exec failed: exec failed: container_linux.go:348: starting container process caused "exec: \"--\": executable file not found in $PATH": unknown
root@labs--311032102:/home/project/istio-resiliency-part1# docker exec -it mysql  bash
root@2b61b7ce2477:/# mysql -u root -p
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 35
Server version: 8.0.29 MySQL Community Server - GPL

Copyright (c) 2000, 2022, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> use orderdb;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql> select * from tables;
ERROR 1146 (42S02): Table 'orderdb.tables' doesn't exist
mysql> select * from table;
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'table' at line 1
mysql> show tables;
+--------------------+
| Tables_in_orderdb  |
+--------------------+
| Order_table        |
| hibernate_sequence |
+--------------------+
2 rows in set (0.00 sec)

mysql> select * from Order_table;
+----+-----------+------+--------+
| id | productId | qty  | status |
+----+-----------+------+--------+
|  1 | 1         |    1 | NULL   |
+----+-----------+------+--------+
1 row in set (0.00 sec)

mysql> quit
Bye
root@2b61b7ce2477:/# 
root@2b61b7ce2477:/# 
root@2b61b7ce2477:/# 
root@2b61b7ce2477:/# exit
exit
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# docker stop mysql
mysql
root@labs--311032102:/home/project/istio-resiliency-part1# docker rm mysql
mysql
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get ns
NAME              STATUS   AGE
argocd            Active   82m
default           Active   3d3h
kube-node-lease   Active   3d3h
kube-public       Active   3d3h
kube-system       Active   3d3h
tutorial          Active   5h45m
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl^Cd$get ns
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl delete argocd tutorial
error: the server doesn't have a resource type "argocd"
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl delete ns argocd tutorial
namespace "argocd" deleted
namespace "tutorial" deleted
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl config set-context --current --namespace=default
Context "arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks" modified.
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl config get-context 
error: unknown command "get-context"
See 'kubectl config -h' for help and examples
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl config get-context s
error: unknown command "get-context s"
See 'kubectl config -h' for help and examples
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl config get-contexts
CURRENT   NAME                                                         CLUSTER                                                      AUTHINFO                                                     NAMESPACE
*         arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks   arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks   arn:aws:eks:ap-northeast-1:979050235289:cluster/user02-eks   default
          kcb-test2.k8s.local                                          kcb-test2.k8s.local                                          labs--311032102                                              labs--311032102
          user02@user02-eks.ap-northeast-1.eksctl.io                   user02-eks.ap-northeast-1.eksctl.io                          user02@user02-eks.ap-northeast-1.eksctl.io                   
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl top nodes
NAME                                               CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
ip-192-168-29-79.ap-northeast-1.compute.internal   74m          3%     1424Mi          42%       
ip-192-168-45-77.ap-northeast-1.compute.internal   83m          4%     1887Mi          56%       
ip-192-168-91-26.ap-northeast-1.compute.internal   69m          3%     1313Mi          39%       
root@labs--311032102:/home/project/istio-resiliency-part1# helm repo list
NAME            URL                                       
bitnami         https://charts.bitnami.com/bitnami        
stable          https://charts.helm.sh/stable             
ingress-nginx   https://kubernetes.github.io/ingress-nginx
root@labs--311032102:/home/project/istio-resiliency-part1# helm install mysql bitnami/mysql
NAME: mysql
LAST DEPLOYED: Thu Jun 23 08:16:25 2022
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: mysql
CHART VERSION: 9.1.8
APP VERSION: 8.0.29

** Please be patient while the chart is being deployed **

Tip:

  Watch the deployment status using the command: kubectl get pods -w --namespace default

Services:

  echo Primary: mysql.default.svc.cluster.local:3306

Execute the following to get the administrator credentials:

  echo Username: root
  MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace default mysql -o jsonpath="{.data.mysql-root-password}" | base64 -d)

To connect to your database:

  1. Run a pod that you can use as a client:

      kubectl run mysql-client --rm --tty -i --restart='Never' --image  docker.io/bitnami/mysql:8.0.29-debian-11-r3 --namespace default --env MYSQL_ROOT_PASSWORD=$MYSQL_ROOT_PASSWORD --command -- bash

  2. To connect to primary service (read/write):

      mysql -h mysql.default.svc.cluster.local -uroot -p"$MYSQL_ROOT_PASSWORD"
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get secret
NAME                               TYPE                                  DATA   AGE
bookinfo-details-token-fprxf       kubernetes.io/service-account-token   3      7h4m
bookinfo-productpage-token-rnx9n   kubernetes.io/service-account-token   3      7h4m
bookinfo-ratings-token-ls557       kubernetes.io/service-account-token   3      7h4m
bookinfo-reviews-token-zbbf4       kubernetes.io/service-account-token   3      7h4m
default-token-zkb74                kubernetes.io/service-account-token   3      3d3h
my-kafka-token-p4lrr               kubernetes.io/service-account-token   3      2d2h
mysql                              Opaque                                2      2m27s
mysql-token-8kpj6                  kubernetes.io/service-account-token   3      2m27s
sh.helm.release.v1.my-kafka.v1     helm.sh/release.v1                    1      2d2h
sh.helm.release.v1.mysql.v1        helm.sh/release.v1                    1      2m27s
root@labs--311032102:/home/project/istio-resiliency-part1# $(kubectl get secret --namespace default mysql -o jsonpath="{.data.mysql-root-password}" | base64 -d)
bash: yiMK80ZKcV: command not found
root@labs--311032102:/home/project/istio-resiliency-part1# MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace default mysql -o jsonpath="{.data.mysql-root-password}" | base64 -d)
root@labs--311032102:/home/project/istio-resiliency-part1# echo $MYSQL_ROOT_PASSWORD
yiMK80ZKcV
root@labs--311032102:/home/project/istio-resiliency-part1# kubectl get svc
NAME                          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
kubernetes                    ClusterIP   10.100.0.1       <none>        443/TCP                      3d3h
my-kafka                      ClusterIP   10.100.147.203   <none>        9092/TCP                     2d3h
my-kafka-headless             ClusterIP   None             <none>        9092/TCP,9093/TCP            2d3h
my-kafka-zookeeper            ClusterIP   10.100.124.20    <none>        2181/TCP,2888/TCP,3888/TCP   2d3h
my-kafka-zookeeper-headless   ClusterIP   None             <none>        2181/TCP,2888/TCP,3888/TCP   2d3h
mysql                         ClusterIP   10.100.133.156   <none>        3306/TCP                     10m
mysql-headless                ClusterIP   None             <none>        3306/TCP                     10m
root@labs--311032102:/home/project/istio-resiliency-part1# 
root@labs--311032102:/home/project/istio-resiliency-part1# cd order
root@labs--311032102:/home/project/istio-resiliency-part1/order# ls
Dockerfile  azure-pipelines.yml  cloudbuild.yaml  kubernetes  oder_config.yaml  order.iml  pom.xml  src  target
root@labs--311032102:/home/project/istio-resiliency-part1/order# kubectl create configmap order-cm --from-literal=DB_USER=root --dry-run=client -o yaml > order-cm.yaml
root@labs--311032102:/home/project/istio-resiliency-part1/order# 
root@labs--311032102:/home/project/istio-resiliency-part1/order# 
root@labs--311032102:/home/project/istio-resiliency-part1/order# kubectl get cm
NAME                         DATA   AGE
istio-ca-root-cert           1      7h26m
my-config                    2      27h
my-kafka-scripts             1      2d3h
my-kafka-zookeeper-scripts   2      2d3h
mysql                        1      18m
root@labs--311032102:/home/project/istio-resiliency-part1/order# ls -lrt
total 60
-rw-r--r-- 1 root root 17614 May  9  2021 order.iml
-rw-r--r-- 1 root root  2939 May  9  2021 cloudbuild.yaml
-rw-r--r-- 1 root root  4694 May  9  2021 azure-pipelines.yml
-rw-r--r-- 1 root root   205 May  9  2021 Dockerfile
drwxr-xr-x 2 root root  4096 Jun 23 04:29 kubernetes
drwxr-xr-x 3 root root  4096 Jun 23 04:29 src
-rw-r--r-- 1 root root  2742 Jun 23 07:20 pom.xml
drwxr-xr-x 5 root root  4096 Jun 23 07:26 target
-rw-r--r-- 1 root root   465 Jun 23 08:32 oder_config.yaml
-rw-r--r-- 1 root root   106 Jun 23 08:34 order-cm.yaml
root@labs--311032102:/home/project/istio-resiliency-part1/order# kubectl create configmap order-cm --from-literal=DB_USER=root --dry-run=client -o yaml > order-cm.yaml^C
root@labs--311032102:/home/project/istio-resiliency-part1/order# kubectl create configmap order-cm --from-literal=DB_USER=root --dry-run=client -o yaml > order-cm.yaml
root@labs--311032102:/home/project/istio-resiliency-part1/order# ls -lrt
total 60
-rw-r--r-- 1 root root 17614 May  9  2021 order.iml
-rw-r--r-- 1 root root  2939 May  9  2021 cloudbuild.yaml
-rw-r--r-- 1 root root  4694 May  9  2021 azure-pipelines.yml
-rw-r--r-- 1 root root   205 May  9  2021 Dockerfile
drwxr-xr-x 2 root root  4096 Jun 23 04:29 kubernetes
drwxr-xr-x 3 root root  4096 Jun 23 04:29 src
-rw-r--r-- 1 root root  2742 Jun 23 07:20 pom.xml
drwxr-xr-x 5 root root  4096 Jun 23 07:26 target
-rw-r--r-- 1 root root   465 Jun 23 08:32 oder_config.yaml
-rw-r--r-- 1 root root   106 Jun 23 08:35 order-cm.yaml
root@labs--311032102:/home/project/istio-resiliency-part1/order# vi order-cm.yaml
root@labs--311032102:/home/project/istio-resiliency-part1/order# pwd
/home/project/istio-resiliency-part1/order
root@labs--311032102:/home/project/istio-resiliency-part1/order# kubectl create -f order-cm.yaml
configmap/order-cm created
root@labs--311032102:/home/project/istio-resiliency-part1/order# kubectl get cm
NAME                         DATA   AGE
istio-ca-root-cert           1      7h31m
my-config                    2      27h
my-kafka-scripts             1      2d3h
my-kafka-zookeeper-scripts   2      2d3h
mysql                        1      22m
order-cm                     5      7s
root@labs--311032102:/home/project/istio-resiliency-part1/order# kubectl describe cm order-cm
Name:         order-cm
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
DB_USER:
----
root
DELIVERY_SERVER:
----
delivery.default:8080
KAFKA_BROKER:
----
my-kafka.default:9092
DB_PASSWORD:
----
yiMK80ZKcV
DB_URL:
----
jdbc:mysql://mysql.default:3306/orderdb?serverTimezone=UTC&characterEncoding=UTF-8
Events:  <none>
root@labs--311032102:/home/project/istio-resiliency-part1/order# 
root@labs--311032102:/home/project/istio-resiliency-part1/order# 
root@labs--311032102:/home/project/istio-resiliency-part1/order# 


root@labs--311032102:/home/project/istio-resiliency-part1/order# mvn clean
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$1 (file:/usr/share/maven/lib/guice.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$1
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[INFO] Scanning for projects...
[INFO] 
[INFO] -----------------------------< mall:order >-----------------------------
[INFO] Building order 0.0.1-SNAPSHOT
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:3.1.0:clean (default-clean) @ order ---
[INFO] Deleting /home/project/istio-resiliency-part1/order/target
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  1.191 s
[INFO] Finished at: 2022-06-23T08:40:47Z
[INFO] ------------------------------------------------------------------------
root@labs--311032102:/home/project/istio-resiliency-part1/order# mvn package -B
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$1 (file:/usr/share/maven/lib/guice.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$1
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[INFO] Scanning for projects...
[INFO] 
[INFO] -----------------------------< mall:order >-----------------------------
[INFO] Building order 0.0.1-SNAPSHOT
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:resources (default-resources) @ order ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 0 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ order ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 15 source files to /home/project/istio-resiliency-part1/order/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:testResources (default-testResources) @ order ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/project/istio-resiliency-part1/order/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ order ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.22.2:test (default-test) @ order ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-jar-plugin:3.1.2:jar (default-jar) @ order ---
[INFO] Building jar: /home/project/istio-resiliency-part1/order/target/order-0.0.1-SNAPSHOT.jar
[INFO] 
[INFO] --- spring-boot-maven-plugin:2.1.9.RELEASE:repackage (repackage) @ order ---
[INFO] Replacing main artifact with repackaged archive
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  4.978 s
[INFO] Finished at: 2022-06-23T08:41:01Z
[INFO] ------------------------------------------------------------------------
root@labs--311032102:/home/project/istio-resiliency-part1/order# ls
Dockerfile  azure-pipelines.yml  cloudbuild.yaml  kubernetes  oder_config.yaml  order-cm.yaml  order.iml  pom.xml  src  target
root@labs--311032102:/home/project/istio-resiliency-part1/order# cd targer
bash: cd: targer: No such file or directory
root@labs--311032102:/home/project/istio-resiliency-part1/order# ls
Dockerfile  azure-pipelines.yml  cloudbuild.yaml  kubernetes  oder_config.yaml  order-cm.yaml  order.iml  pom.xml  src  target
root@labs--311032102:/home/project/istio-resiliency-part1/order# cd targer
bash: cd: targer: No such file or directory
root@labs--311032102:/home/project/istio-resiliency-part1/order# cd target
root@labs--311032102:/home/project/istio-resiliency-part1/order/target# ls
classes  generated-sources  maven-archiver  maven-status  order-0.0.1-SNAPSHOT.jar  order-0.0.1-SNAPSHOT.jar.original
root@labs--311032102:/home/project/istio-resiliency-part1/order/target# cd ..
root@labs--311032102:/home/project/istio-resiliency-part1/order# ls
Dockerfile  azure-pipelines.yml  cloudbuild.yaml  kubernetes  oder_config.yaml  order-cm.yaml  order.iml  pom.xml  src  target
root@labs--311032102:/home/project/istio-resiliency-part1/order# docker build -t gorapadd/order:cm .
Sending build context to Docker daemon 59.97 MB
Step 1/4 : FROM openjdk:8u212-jdk-alpine
 ---> a3562aa0b991
Step 2/4 : COPY target/*SNAPSHOT.jar app.jar
 ---> 0d33f597cc6d
Step 3/4 : EXPOSE 8080
 ---> Running in 41f1d84becd3
Removing intermediate container 41f1d84becd3
 ---> f9ab9bdeac87
Step 4/4 : ENTRYPOINT ["java","-Xmx400M","-Djava.security.egd=file:/dev/./urandom","-jar","/app.jar","--spring.profiles.active=docker"]
 ---> Running in b2a60aabc7a7
Removing intermediate container b2a60aabc7a7
 ---> 4b5fad34e7d8
Successfully built 4b5fad34e7d8
Successfully tagged gorapadd/order:cm
root@labs--311032102:/home/project/istio-resiliency-part1/order# cat Dockerfile
FROM openjdk:8u212-jdk-alpine
COPY target/*SNAPSHOT.jar app.jar
EXPOSE 8080
ENTRYPOINT ["java","-Xmx400M","-Djava.security.egd=file:/dev/./urandom","-jar","/app.jar","--spring.profiles.active=docker"]
root@labs--311032102:/home/project/istio-resiliency-part1/order# docker images
REPOSITORY                                                       TAG                 IMAGE ID            CREATED             SIZE
gorapadd/order                                                   cm                  4b5fad34e7d8        31 seconds ago      165 MB
mysql                                                            latest              968083d5be36        4 hours ago         524 MB
gorapadd/order                                                   db                  bd7e9385851b        27 hours ago        146 MB
gorapadd/order                                                   memleak             b730ec011f1c        31 hours ago        165 MB
gorapadd/userapp                                                 1                   f13057de45d7        2 days ago          415 MB
gorapadd/userapp                                                 1.0                 f13057de45d7        2 days ago          415 MB
979050235289.dkr.ecr.ap-northeast-1.amazonaws.com/user02-order   v1                  04836ea56ba2        3 days ago          146 MB
gorapadd/welcome                                                 v1                  a6f8064d8aeb        3 days ago          142 MB
gorapadd/order                                                   2022061401          83818081755e        9 days ago          165 MB
jinyoung/order                                                   2022061401          1d349b76aa1e        9 days ago          165 MB
httpd                                                            latest              b260a49eebf9        9 days ago          145 MB
nginx                                                            latest              0e901e68141f        3 weeks ago         142 MB
openjdk                                                          15-jdk-alpine       f02adfce91a2        23 months ago       343 MB
openjdk                                                          8u212-jdk-alpine    a3562aa0b991        3 years ago         105 MB
ghcr.io/gkedu/openjdk                                            8u212-jdk-alpine    a3562aa0b991        3 years ago         105 MB
root@labs--311032102:/home/project/istio-resiliency-part1/order# docker push gorapadd/order:cm 
The push refers to repository [docker.io/gorapadd/order]
09c153aca66c: Pushed 
ceaf9e1ebef5: Layer already exists 
9b9b7f3d56a0: Layer already exists 
f1b5933fe4b5: Layer already exists 
cm: digest: sha256:6493158e89262b04eef2e76a74f1715b3dca330cf8b271c0b148ad8de1a93441 size: 1159
root@labs--311032102:/home/project/istio-resiliency-part1/order# 



#####################

1. oder -> kubernetes deploy파일 수정 

          envFrom:
            - configMapRef:
              name: order-cm

apiVersion: apps/v1
kind: Deployment
metadata:
  name: order
  labels:
    app: order
spec:
  replicas: 1
  selector:
    matchLabels:
      app: order
  template:
    metadata:
      labels:
        app: order
    spec:
      containers:
        - name: order
          image: gorapadd/order:cm
          ports:
            - containerPort: 8080
          envFrom:
            - configMapRef:
                name: order-cm
          readinessProbe:
            httpGet:
              path: '/actuator/health'
              port: 8080
            initialDelaySeconds: 10
            timeoutSeconds: 2
            periodSeconds: 5
            failureThreshold: 10
          livenessProbe:
            httpGet:
              path: '/actuator/health'
              port: 8080
            initialDelaySeconds: 120
            timeoutSeconds: 2
            periodSeconds: 5
            failureThreshold: 5

2. delivery ->kubernetes deploy파일 수정 


apiVersion: apps/v1
kind: Deployment
metadata:
  name: delivery
  labels:
    app: delivery
spec:
  replicas: 1
  selector:
    matchLabels:
      app: delivery
  template:
    metadata:
      labels:
        app: delivery
    spec:
      containers:
        - name: delivery
          image: gorapadd/delivery:cm
          ports:
            - containerPort: 8080
          env:
            - name: KAFKA_BROKER
              valueFrom:
                configMapKeyRef:
                  name: order-cm
                  key: KAFKA_BROKER
          readinessProbe:
            httpGet:
              path: '/actuator/health'
              port: 8080
            initialDelaySeconds: 10
            timeoutSeconds: 2
            periodSeconds: 5
            failureThreshold: 10
          livenessProbe:
            httpGet:
              path: '/actuator/health'
              port: 8080
            initialDelaySeconds: 120
            timeoutSeconds: 2
            periodSeconds: 5
            failureThreshold: 5


root@labs--311032102:/home/project/istio-resiliency-part1/delivery# docker build -t gorapadd/delivery:cm .
Sending build context to Docker daemon 59.79 MB
Step 1/4 : FROM openjdk:8u212-jdk-alpine
 ---> a3562aa0b991
Step 2/4 : COPY target/*SNAPSHOT.jar app.jar
 ---> cdbde178db97
Step 3/4 : EXPOSE 8080
 ---> Running in 9184cad7a980
Removing intermediate container 9184cad7a980
 ---> 67fd51781ea6
Step 4/4 : ENTRYPOINT ["java","-Xmx400M","-Djava.security.egd=file:/dev/./urandom","-jar","/app.jar","--spring.profiles.active=docker"]
 ---> Running in 15fbf8de11b3
Removing intermediate container 15fbf8de11b3
 ---> 43713ef310d4
Successfully built 43713ef310d4
Successfully tagged gorapadd/delivery:cm
root@labs--311032102:/home/project/istio-resiliency-part1/delivery# 
root@labs--311032102:/home/project/istio-resiliency-part1/delivery# docker push  gorapadd/delivery:cm
The push refers to repository [docker.io/gorapadd/delivery]
9b7c411bd183: Pushed 
ceaf9e1ebef5: Mounted from gorapadd/order 
9b9b7f3d56a0: Mounted from gorapadd/order 
f1b5933fe4b5: Mounted from gorapadd/order 
cm: digest: sha256:4b14d6e8d704aa338bf20c4be48ad632eabff99b70209b1ab5e8bedc925a316e size: 1159
root@labs--311032102:/home/project/istio-resiliency-part1/delivery# ls-lrt
bash: ls-lrt: command not found
root@labs--311032102:/home/project/istio-resiliency-part1/delivery# ls -lrt
total 32
-rw-r--r-- 1 root root 2739 May  9  2021 pom.xml
-rw-r--r-- 1 root root 2942 May  9  2021 cloudbuild.yaml
-rw-r--r-- 1 root root 4697 May  9  2021 azure-pipelines.yml
-rw-r--r-- 1 root root  205 May  9  2021 Dockerfile
drwxr-xr-x 2 root root 4096 Jun 23 04:29 kubernetes
drwxr-xr-x 3 root root 4096 Jun 23 04:29 src
drwxr-xr-x 6 root root 4096 Jun 23 08:54 target
root@labs--311032102:/home/project/istio-resiliency-part1/delivery# kubectl get cm order-cm -o yaml^C
root@labs--311032102:/home/project/istio-resiliency-part1/delivery# cd ku*
root@labs--311032102:/home/project/istio-resiliency-part1/delivery/kubernetes# ls
deployment.yml  service.yaml
root@labs--311032102:/home/project/istio-resiliency-part1/delivery/kubernetes# kubectl create -f deployment.yml
deployment.apps/delivery created
root@labs--311032102:/home/project/istio-resiliency-part1/delivery/kubernetes# kubectl create -f .
service/delivery created
Error from server (AlreadyExists): error when creating "deployment.yml": deployments.apps "delivery" already exists
root@labs--311032102:/home/project/istio-resiliency-part1/delivery/kubernetes# cd .
root@labs--311032102:/home/project/istio-resiliency-part1/delivery/kubernetes# ls
deployment.yml  service.yaml
root@labs--311032102:/home/project/istio-resiliency-part1/delivery/kubernetes# cd ..
root@labs--311032102:/home/project/istio-resiliency-part1/delivery# ls
Dockerfile  azure-pipelines.yml  cloudbuild.yaml  kubernetes  pom.xml  src  target
root@labs--311032102:/home/project/istio-resiliency-part1/delivery# cd ..
root@labs--311032102:/home/project/istio-resiliency-part1# ls
delivery  gateway  order  test.yaml  tracing.yaml
root@labs--311032102:/home/project/istio-resiliency-part1# cd order 
root@labs--311032102:/home/project/istio-resiliency-part1/order# ls
Dockerfile  azure-pipelines.yml  cloudbuild.yaml  kubernetes  oder_config.yaml  order-cm.yaml  order.iml  pom.xml  src  target
root@labs--311032102:/home/project/istio-resiliency-part1/order# cd ku*
root@labs--311032102:/home/project/istio-resiliency-part1/order/kubernetes# ls
deployment.yml  service.yaml
root@labs--311032102:/home/project/istio-resiliency-part1/order/kubernetes# kubectl create -f .
service/order created
Error from server (AlreadyExists): error when creating "deployment.yml": deployments.apps "order" already exists
root@labs--311032102:/home/project/istio-resiliency-part1/order/kubernetes# kubectl get pod
NAME                       READY   STATUS    RESTARTS   AGE
delivery-659ccb8c6-kqj74   1/1     Running   0          50s
my-kafka-0                 1/1     Running   1          2d3h
my-kafka-zookeeper-0       1/1     Running   1          2d3h
mysql-0                    1/1     Running   0          47m
order-6f656d876d-hgjr2     1/1     Running   5          14m
siege                      1/1     Running   0          3d1h
siege-55b7f7d5fc-7zwgd     1/1     Running   0          46m
root@labs--311032102:/home/project/istio-resiliency-part1/order/kubernetes# 
root@labs--311032102:/home/project/istio-resiliency-part1/order/kubernetes# 
root@labs--311032102:/home/project/istio-resiliency-part1/order/kubernetes# 
root@labs--311032102:/home/project/istio-resiliency-part1/order/kubernetes# kubectl run siege --image=zasmin/siege:1.0 
Error from server (AlreadyExists): pods "siege" already exists
root@labs--311032102:/home/project/istio-resiliency-part1/order/kubernetes# 
root@labs--311032102:/home/project/istio-resiliency-part1/order/kubernetes# 
root@labs--311032102:/home/project/istio-resiliency-part1/order/kubernetes# kubectl exec -it siege -- bash
root@siege:/# 
root@siege:/# 
root@siege:/# 
root@siege:/# 
root@siege:/# 
root@siege:/# exit
exit
root@labs--311032102:/home/project/istio-resiliency-part1/order/kubernetes# 
root@labs--311032102:/home/project/istio-resiliency-part1/order/kubernetes# kubectl get svc
NAME                          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
delivery                      ClusterIP   10.100.174.195   <none>        8080/TCP                     113s
kubernetes                    ClusterIP   10.100.0.1       <none>        443/TCP                      3d4h
my-kafka                      ClusterIP   10.100.147.203   <none>        9092/TCP                     2d3h
my-kafka-headless             ClusterIP   None             <none>        9092/TCP,9093/TCP            2d3h
my-kafka-zookeeper            ClusterIP   10.100.124.20    <none>        2181/TCP,2888/TCP,3888/TCP   2d3h
my-kafka-zookeeper-headless   ClusterIP   None             <none>        2181/TCP,2888/TCP,3888/TCP   2d3h
mysql                         ClusterIP   10.100.133.156   <none>        3306/TCP                     48m
mysql-headless                ClusterIP   None             <none>        3306/TCP                     48m
order                         ClusterIP   10.100.239.39    <none>        8080/TCP                     91s
root@labs--311032102:/home/project/istio-resiliency-part1/order/kubernetes# 
root@labs--311032102:/home/project/istio-resiliency-part1/order/kubernetes# 
root@labs--311032102:/home/project/istio-resiliency-part1/order/kubernetes# kubectl get deploy
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
delivery   1/1     1            1           2m8s
order      1/1     1            1           16m
siege      1/1     1            1           2d6h
root@labs--311032102:/home/project/istio-resiliency-part1/order/kubernetes# kubectl get pod
NAME                       READY   STATUS    RESTARTS   AGE
delivery-659ccb8c6-kqj74   1/1     Running   0          2m13s
my-kafka-0                 1/1     Running   1          2d3h
my-kafka-zookeeper-0       1/1     Running   1          2d3h
mysql-0                    1/1     Running   0          49m
order-6f656d876d-hgjr2     1/1     Running   5          16m
siege                      1/1     Running   0          3d1h
siege-55b7f7d5fc-7zwgd     1/1     Running   0          47m
root@labs--311032102:/home/project/istio-resiliency-part1/order/kubernetes# kubectl get rs
NAME                 DESIRED   CURRENT   READY   AGE
delivery-659ccb8c6   1         1         1       2m18s
order-6f656d876d     1         1         1       16m
siege-55b7f7d5fc     1         1         1       2d6h
root@labs--311032102:/home/project/istio-resiliency-part1/order/kubernetes# kubectl exec -it siege -- bash
root@siege:/# 





#####

root@labs--311032102:/home/project/istio-resiliency-part1/order/kubernetes# kubectl exec -it mysql-0 -- bash
I have no name!@mysql-0:/$ mysql -u root -p
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 697
Server version: 8.0.29 Source distribution

mysql> select * from order_table;^C
mysql> show tables;
+--------------------+
| Tables_in_orderdb  |
+--------------------+
| Order_table        |
| hibernate_sequence |
+--------------------+
2 rows in set (0.00 sec)

mysql> select * from Order_table :
    -> ^C
mysql> select * from Order_table ;
+----+-----------+------+-----------------+
| id | productId | qty  | status          |
+----+-----------+------+-----------------+
|  1 | 1         |    1 | DeliveryStarted |
+----+-----------+------+-----------------+
1 row in set (0.00 sec)



## 설정값들

url: "${DB_URL}"
"jdbc:mysql://localhost:3306/orderdb?serverTimezone=UTC&characterEncoding=UTF-8"

url: "${DB_URL}"                 jdbc:mysql://mysql.default:3306/orderdb?serverTimezone=UTC&characterEncoding=UTF-8
username: "${DB_USER}"           root
password: "${DB_PASSWORD}"       yiMK80ZKcV   

brokers: ${KAFKA_BROKER}   my-kafka.default:9092

url: http://delivery:8080


url: ${DELIVERY_SERVER}          delivery.default:8080

DB_URL: jdbc:mysql://mysql.default:3306/orderdb?serverTimezone=UTC&characterEncoding=UTF-8
DB_USER: root
DB_PASSWORD: yiMK80ZKcV
KAFKA_BROKER: my-kafka.default:9092
DELIVERY_SERVER: delivery.default:8080
